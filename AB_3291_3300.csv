AB,NO
"point-based indoor 3d object detection has received increasing attention with the large demand for augmented reality, autonomous driving, and robot technology in the industry. however, the detection precision suffers from inputs with semantic ambiguity, i.e., shape symmetries, occlusion, and texture missing, which would lead that different objects appearing similar from different viewpoints and then confusing the detection model. typical point-based detectors relieve this problem via learning proposal representations with both geometric and semantic information, while the entangled representation may cause a reduction in both semantic and spatial discrimination. in this paper, we focus on alleviating the confusion from entanglement and then enhancing the proposal representation by considering the proposal's semantics and the context in one scene. a semantic-context graph network (scgnet) is proposed, which mainly includes two modules: a category-aware proposal recoding module (capr) and a proposal context aggregation module (pcag). to produce semantically clear features from entanglement representation, the capr module learns a high-level semantic embedding for each category to extract discriminative semantic clues. in view of further enhancing the proposal representation and leveraging the semantic clues, the pcag module builds a graph to mine the most relevant context in the scene. with few bells and whistles, the scgnet achieves sota performance and obtains consistent gains when applying to different backbones (0.9% similar to 2.4% on scannet v2 and 1.6% similar to 2.2% on sun rgb-d for map@0.25). code is available at https://github.com/dsw-jlurgzn/scgnet.",AB_0330
"the aim of weakly supervised object co-localization is to locate different objects of the same superclass in a dataset. recent methods achieve impressive co-localization performance by multiple instance learning and self-supervised learning. however, these methods ignore the common part information shared by fine-grained objects and the influence of the complementary parts on the co-localization of the fine-grained objects. to solve these issues, we propose a complementary parts contrastive learning method for finegrained weakly supervised object co-localization. the proposed method follows such an assumption that fine-grained object parts with the same/different semantic meaning should have similar/dissimilar feature representations in the feature space. the proposed method tackles two critical issues in this task: i ) how to spread the model's attention and suppress the complex background noise, and ii) how to leverage the cross-category common parts information to mitigate the context co-occurrence problem. to address i ), we attempt to integrate local and context cues via three types of attention including self-supervised attention, channel, and spatial attention to spread the model's attention toward automatically identifying and localizing most discriminative parts of objects in the fine-grained images. to solve i i ), we propose a cross-category object complementarity part contrastive learning module to identify the extracted part regions with different semantic information by pulling the same part features closer and pushing different part features away, which can mitigate the confounding bias caused by the co-occurrence surroundings within specific classes. extensive qualitative and quantitative evaluations demonstrate the effectiveness of the proposed method on four fine-grained co-localization datasets: cub-200-2011, stanford cars, fgvc-aircraft, and stanford dogs. code and models are available at https://github.com/zhaofan/cpcl.",AB_0330
"organism detection plays a vital role in marine resource exploitation and marine economy. how to accurately locate the target organism object within the camouflaged and dark light oceanic scene has recently drawn great attention in the research community. existing learning-based works usually leverage local texture details within a neighboring area, with few methods explicitly exploring the usage of contextualized awareness for accurate object detection. from a novel perspective, we in this work present a bidirectional collaborative mentoring network (bcmnet) which fully explores both texture and context clues during the encoding and decoding stages, making the cross-paradigm interaction bidirectional and improving the scene understanding at all stages. specifically, we first extract texture and context features through a dual-branch encoder and attentively fuse them through our adjacent feature fusion (aff) block. then, we propose a structure-aware module (sam) and a detail-enhanced module (dem) to form our two-stage decoding pipeline. on the one hand, our sam leverages both local and global clues to preserve morphological integrity and generate an initial prediction of the target object. on the other hand, the dem explicitly explores long-range dependencies to refine the initially predicted object mask further. the combination of sam and dem enables better extracting, preserving, and enhancing the object morphology, making it easier to segment the target object from the camouflaged background with sharp contour. extensive experiments on three benchmark datasets show that our proposed bcmnet performs favorably over state-of-the-art models. the code will be made available at https://github.com/chasecjg/bcmnet.",AB_0330
"an an important type of 3d representation, the point cloud is widely used in many applications, such as autonomous driving, ar/vr, and intelligent robots, which require real-time interactions with humans. however, the sparsity of 3d point cloud data leads to severe computational inefficiency when being processed by 2d data processors, posing a huge challenge for hardware acceleration. in this brief, we aim at solving the inefficiency problem by algorithm-hardware co-optimization. firstly, a lightweight network, named lpn, is proposed for point cloud data classification, which is 30x smaller than pointnet and still has comparable accuracy. secondly, a reconfigurable computing core, named rcc, together with an adaptive dataflow, is developed to support different layers of the lpn. specifically, to accelerate memory-intensive layers, a partially-parallel computing scheme is introduced to minimize the on-chip memory requirements and dram accesses. finally, based on the above innovations, a low-latency accelerator is proposed to realize real-time computation for the point cloud, which is implemented on the xilinx kintex ultrascale kcu150 fpga board. experimental results show that it achieves 1.5x throughput improvement compared with the state-of-the-art works, and 35x speedup over intel xeon gold 6148 cpu, demonstrating the superiority of the proposed method. the code of lpn is available from https://github.com/snowsil/lpn-model-for-3d-classifification.",AB_0330
"multi-label image recognition aims to predict a set of labels that present in an image. the key to deal with such problem is to mine the associations between image contents and labels, and further obtain the correct assignments between images and their labels. in this paper, we treat each image as a bag of instances, and formulate the task of multi-label image recognition as an instance-label matching selection problem. to model such problem, we propose an innovative semantic-aware graph matching framework for multi-label image recognition (ml-sgm), in which graph matching mechanism is introduced owing to its good performance of excavating the instance and label relationship. the framework explicitly establishes category correlations and instance-label correspondences by modeling the relation among content-aware (instance) and semantic-aware (label) category representations, to facilitate multi-label image understanding and reduce the dependency of large amounts of training samples for each category. specifically, we first construct an instance spatial graph and a label semantic graph respectively and then incorporate them into a constructed assignment graph by connecting each instance to all labels. subsequently, the graph network block is adopted to aggregate and update all nodes and edges state on the assignment graph to form structured representations for each instance and label. our network finally derives a prediction score for each instance-label correspondence and optimizes such correspondence with a weighted cross-entropy loss. empirical results conducted on generic multi-label image recognition demonstrate the superiority of our proposed method. moreover, the proposed method also shows advantages in multi-label recognition with partial labels and multi-label few-shot learning, as well as outperforms current state-of-the-art methods with a clear margin. our code is available at https://github.com/yananwu0510/ml-sgm.",AB_0330
"in this paper, we present a dense hybrid proposal modulation (dhpm) method for lane detection. most existing methods perform sparse supervision on a subset of high-scoring proposals, while other proposals fail to obtain effective shape and location guidance, resulting in poor overall quality. to address this, we densely modulate all proposals to generate topologically and spatially high-quality lane predictions with discriminative representations. specifically, we first ensure that lane proposals are physically meaningful by applying single-lane shape and location constraints. benefitting from the proposed proposal-to-label matching algorithm, we assign each proposal a target ground truth lane to efficiently learn from spatial layout priors. to enhance the generalization and model the inter-proposal relations, we diversify the shape difference of proposals matching the same ground-truth lane. in addition to the shape and location constraints, we design a quality-aware classification loss to adaptively supervise each positive proposal so that the discriminative power can be further boosted. our dhpm achieves very competitive performances on four popular benchmark datasets. moreover, we consistently outperform the baseline model on most metrics without introducing new parameters and reducing inference speed. the codes of our method are available at https://github.com/wuyuej/dhpm.",AB_0330
"recent cnns (convolutional neural networks) have become more and more compact. the elegant structure design highly improves the performance of cnns. with the development of knowledge distillation technique, the performance of cnns gets further improved. however, existing knowledge distillation guided methods either rely on offline pretrained high-quality large teacher models or online heavy training burden. to solve the above problems, we propose a feature-sharing and weight-sharing based ensemble network (training framework) guided by knowledge distillation (ekd-fwsnet) to make baseline models stronger in terms of representation ability with less training computation and memory cost involved. specifically, motivated by getting rid of the dependence of offline pretrained teacher model, we design an end-to-end online training scheme to optimize ekd-fwsnet. motivated by decreasing the online training burden, we only introduce one auxiliary classmate branch to construct multiple forward branches, which will then be integrated as ensemble teacher to guide baseline model. compared to previous online ensemble training frameworks, ekd-fwsnet can provide diverse output predictions without relying on increasing auxiliary classmate branches. motivated by maximizing the optimization power of ekd-fwsnet, we exploit the representation potential of weight-sharing blocks and design efficient knowledge distillation mechanism in ekd-fwsnet. extensive comparison experiments and visualization analysis on benchmark datasets (cifar-10/100, tiny-imagenet, cub-200 and imagenet) show that self-learned ekd-fwsnet can boost the performance of baseline models by large margin, which has obvious superiority compared to previous related methods. extensive analysis also proves the interpretability of ekd-fwsnet. our code is available at https://github.com/cv516buaa/ekd-fwsnet.",AB_0330
"in this paper, we propose to compactly represent the nonlinear dynamics along the temporal trajectories for talking face video compression. by projecting the frames into a high dimensional space, the temporal trajectories of talking face frames, which are complex, non-linear and difficult to extrapolate, are implicitly modelled in an end-to-end inference framework based upon very compact feature representation. as such, the proposed framework is suitable for ultra-low bandwidth video communication and can guarantee the quality of the reconstructed video in such applications. the proposed compression scheme is also robust against large head-pose motions, due to the delicately designed dynamic reference refresh and temporal stabilization mechanisms. experimental results demonstrate that compared to the state-of-the-art video coding standard versatile video coding (vvc) as well as the latest generative compression schemes, our proposed scheme is superior in terms of both objective and subjective quality at the same bitrate. the project page can be found at https://github.com/berlin0610/cttr.",AB_0330
"the initial seed based on the convolutional neural network (cnn) for weakly supervised semantic segmentation always highlights the most discriminative regions but fails to identify the global target information. methods based on transformers have been proposed successively benefiting from the advantage of capturing long-range feature representations. however, we observe a flaw regardless of the gifts based on the transformer. given a class, the initial seeds generated based on the transformer may invade regions belonging to other classes. inspired by the mentioned issues, we devise a simple yet effective method with multi-estimations complementary patch (mecp) strategy and adaptive conflict module (acm), dubbed mecpformer. given an image, we manipulate it with the mecp strategy at different epochs, and the network mines and deeply fuses the semantic information at different levels. in addition, acm adaptively removes conflicting pixels and exploits the network self-training capability to mine potential target information. without bells and whistles, our mecpformer has reached new state-of-the-art 72.0% miou on the pascal voc 2012 and 42.4% on ms coco 2014 dataset. the code is available at https://github.com/chunmengliu1/mecpformer.",AB_0330
"underwater object detection suffers from low detection performance because the distance and wavelength dependent imaging process yield evident image quality degradations such as haze-like effects, low visibility, and color distortions. therefore, we commit to resolving the issue of underwater object detection with compounded environmental degradations. typical approaches attempt to develop sophisticated deep architecture to generate high-quality images or features. however, these methods are only work for limited ranges because imaging factors are either unstable, too sensitive, or compounded. unlike these approaches catering for high-quality images or features, this paper seeks transferable prior knowledge from detector-friendly images. the prior guides detectors removing degradations that interfere with detection. it is based on statistical observations that, the heavily degraded regions of detector-friendly (dfui) and underwater images have evident feature distribution gaps while the lightly degraded regions of them overlap each other. therefore, we propose a residual feature transference module (rftm) to learn a mapping between deep representations of the heavily degraded patches of dfui- and underwater-images, and make the mapping as a heavily degraded prior (hdp) for underwater detection. since the statistical properties are independent to image content, hdp can be learned without the supervision of semantic labels and plugged into popular cnn-based feature extraction networks to improve their performance on underwater object detection. without bells and whistles, evaluations on urpc2020 and uodd show that our methods outperform cnn-based detectors by a large margin. our method with higher speeds and less parameters still performs better than transformer-based detectors. our code and dfui dataset can be found in https://github.com/xiaodetection/learning-heavilydegraed-prior.",AB_0330
