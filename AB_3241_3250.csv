AB,NO
"personalized internet of things (iot) services prediction based on quality-of-service (qos) is an indispensable technique for selecting appropriate services for each user. however, existing collaborative prediction models do not take into account the user's authority to manage their own generated data. from the standpoint of users, the expectation is for models to eliminate the impact of their sensitive data to the greatest extent possible. meanwhile, iot service providers face the challenge of data contamination during service provision, which necessitates models to forget data quickly and accurately to restore performance. furthermore, existing qos prediction methods usually suffer from low model availability when handling unlearning requests by full retraining. this underscores the need to address security, availability, fidelity, privacy, and related issues, highlighting the urgency of unlearning. to solve the problem, we propose context-aware data driven eraser (cadderaser), a novel efficient machine unlearning framework for qos prediction tasks. firstly, we divide the training data into multiple shards to train submodels and obtain node embeddings by utilizing contextual information to derive graph embeddings. then these embeddings are employed in a balanced clustering partition, ensuring the preservation of the qos record between users and services. finally, we use a concatenate aggregation method and stacking & attention-based aggregation methods to synthesize information from sub-models more efficiently. experiments on large-scale datasets show that our cadderaser framework not only improves efficiency but also enhances the accuracy of qos prediction, achieving efficient unlearning and outperforms state-of-the-art unlearning approaches. source codes are available at https://github.com/zengyuxiang7/cadderaser.",AB_0325
"legendrenet is a novel graph neural network (gnns) model that addresses stability issues present in traditional gnn models such as chebnet, while also more effectively capturing higher-order dependencies within graphical data. compared to traditional gnns models such as gcn, legendrenet is better equipped to handle large-scale graphical data, demonstrating superior performance on such datasets. furthermore, legendre polynomials, which are a set of completely orthogonal polynomials, are capable of approximating any function to arbitrary precision within a bounded interval. as such, when applied to graph neural networks, legendre polynomials provide a more precise and stable means of fitting spectral filters to graphical data. this enables legendrenet to more accurately capture graphical features when dealing with complex graphical data, and to exhibit greater robustness in adversarial attack scenarios. compared to traditional gnns methods, legendrenet offers improved modeling and generalization capabilities, making it a more effective solution across various graphical data applications. our experiments have demonstrated that our model outperforms state-of-the-art methods on large-scale graphical datasets. the code for legendrenet is available at https://github.com/12chen20/legendrenet.",AB_0325
"deep neural networks have shown excellent performance in the field of pattern classification and are widely used. however, real-world data are often cannot be obtained at once, and the knowledge of old classes will be heavily forgotten when training new classes of data on the network, which is called catastrophic forgetting. therefore, the incremental learning method to solve this problem came into being. in this paper, we propose a class-incremental learning method based on a big data pre-trained model, which makes full use of the large amount of public knowledge in the pre-trained model's front network to reduce the forgetting problem of the network in subsequent classification tasks. on the basis of our previous incremental learning method based on pedcc, we discuss the effects of different pre-trained models, training strategy, training hyperparameters, etc. pedcc-loss is used to constrain the cosine distance between the latent feature and the pre-defined class center, and finally the joint prediction is determined by multiple network prediction results. the algorithm in this paper is verified on the cifar100, tiny imagenet, and facescrub datasets with and without partial retention of old samples, and achieves the best results compared to the previous typical class-incremental learning methods. the performance in coarse-grained datasets even exceeds the accuracy of non-incremental learning without pre-trained model. code is available in https://github.com/bybinwen/class-incremental-learning-based-on-big-dataset-pre-trained-models.",AB_0325
"colonoscopy is one of the most effective means of detecting intestinal polyps and colon cancer. with the development of machine vision, doctors often use automation to help diagnose intestinal polyps when using colonoscopy. the high rate of missed polyps during traditional intestinal polyp detection can lead to an increased recurrence rate and cancer of polyps. therefore, we propose a multi-scale fusion network with boundary judgment function. the network can accurately segment smaller polyps and improve the blurred boundaries of segmented images. the network achieves the purpose of deepening the boundary information by using the attention mechanism by judging the global information generated by the encoder at different levels and the prediction mask map obtained by multi-scale fusion. experimental results show that the network can accurately identify the boundaries of diminutive polyps and improve the overall accuracy of polyp segmentation. compared with the latest network models, we achieved first place in several metrics in the mainstream datasets(e.t., kvasir, cvc-clinicdb, cvc-300, and etis laribpolypdb). codes available: https://github.com/wdy1997/j-net.git",AB_0325
"as informatization 3.0 accelerates the pace of people's life and work, people's happiness index and physical and mental health have become the focus of attention and research in sociology, psychology and medicine. currently, neurological diseases represented by insomnia have become common chronic diseases. however, existing insomnia treatment methods mainly focus on drug therapy and eeg-based expert intervention, ignoring the individual variability of insomnia patients, the high cost of expert intervention, and the privacy of the user's treatment environment. therefore, aiming at the effect of white noise lite on sleep quality, this paper proposes a time-frequency domain correlation multimodal sleep enhancement framework based on reinforcement learning (rlsf), which is a closed-loop feedback sleep improvement framework that includes hardware and software. specifically, the individual sleep state is fed for learning through eeg sensors' input, and the agent is gradually trained to suit the sleep habit. this paper provides a reinforcement learning environment in which different agents can be deployed easily; then, we propose the deep net sleep improvement agent (dnsi agent) and the time and frequency-based lightweight sleep improvement agent (tflsi agent) for rlsf. finally, the substantial experiments compare dnsi and tflsi agent performance, and the results indicate that these two agents both have decision-making ability, and three volunteers' pittsburgh sleep quality index significantly reduces by 3-7 points within two months and the average time to sleep is reduced by 131.4 seconds. our code is publicly available at https://github.com/terryzag/rlsf. our self-made dataset is publicly available at https://github.com/terryzag/tgam-datasets-for-rlsf.",AB_0325
"named entity recognition (ner) poses challenges for both flat and nested tasks, which require different paradigms. to overcome this issue, we present gfner, a unified global feature-aware framework based on table filling, that can handle both types of tasks with low computational cost. while pretrained models have shown great promise in ner, they typically focus on local contextual information, disregarding global relationships that are crucial for accurate entity boundary extraction. to address this limitation, we introduce a global feature learning module that captures the inter-entity associations and significantly enhances entity recognition. experimental results on flat and nested ner datasets demonstrate that gfner outperforms previous state-of-the-art models. the code for gfner is available at https://github.com/cjymz886/gfner.",AB_0325
"neddylation, as a reversible post-translational modification (ptm), plays a role in various cellular processes. defects in neddylation are related to human diseases. detecting neddylation sites is nec-essary for revealing the mechanisms of protein neddylation. as identifying such sites through experimental methods is expensive and time-consuming, it is essential to develop in silico methods to predict neddylation sites. in this study, we constructed a few classifiers integrating various algorithms and encoding features. however, they performed poorly (auc approximate to 0.767), mainly due to the limited number (similar to 1000) of identified neddylation sites. the large number (>100,000) of other lysine ptm sites inspired us to employ a deep transfer learning (dtl) strategy for performance improvement. we constructed a predictor, dubbed dtl-neddsite, which adopted the dtl-based convolution neural network using the one-hot encoding approach. specifically, the massive number of lysine ptm sites were used to build the source model, followed by the fine-tuning of the target model using neddylation sites. dtl-neddsite compared favourably with the corresponding model without the dtl strategy in cross-validation and independent tests. for instance, the auc value increased to 0.818. contrary to a general dtl model that combines frozen and unfrozen layers, all the layers in dtl-neddsite were unfrozen to re-train. we expect the dtl strategy to be widely used in newly discovered modification types with limited known sites. furthermore, dtl-neddsite is freely accessible at https://github.com/xudeli123/dtl-neddsite.",AB_0325
"human age information is present in the face image and speech, but most age estimation methods focus on single-modal data only. although multi-modal approaches have achieved promising performance in other fields by leveraging complementarity between modalities, the development of age estimation has hardly been more inspired by them because of the problem of modality missing in age data. in this paper, we propose an embedding-regularized double branches fusion (erdbf) framework that can simultaneously handle single-modal and multi-modal age estimation. to address the modality missing, we design a double branches fusion network which consists of an embedding regularization module and an information interaction module. the former aims to enhance the representational capacity of single-modal features. the latter learns inter-modal complementary information. through their collaboration in the fusion process, the network can extract discriminative age representations, and even if a certain modality is missing, robust age estimation can be achieved using enhanced single-modal information. to our best knowledge, the proposed framework is the first deep learning-based multi-modal age estimation framework that can easily utilize the achievements made in the single-modal and have broader applications. experimental results show that our best method achieves state-of-the-art results on agevoxceleb. the code is available at https://github.com/daretowin/erdbf.",AB_0325
"compressed sensing magnetic resonance imaging (cs-mri) has made great progress in speeding up mri imaging. the existing non-local self-similarity (nss) prior based cs-mri models mainly take similar image patches as the processing objects, this patch-level non-local sparse representation method can not make full use of the self-similarity among pixels in the image, so it can not recover the weak edge information in the undersampled mri image well and there will still be some artifacts. in this paper, a pixel-level non-local method based compressed sensing undersampled mri image reconstruction method is introduced. first, zero filling is performed on the undersampled k-space data to obtain a full-size 2d signal, and ifft is performed to obtain a preliminary reconstructed mri image. block-matching and row-matching are successively performed on the reconstructed image in turn to obtain similar pixel groups, so as to establish a better sparse representation under the non-local self-similarity (nss) prior. the separable haar transform is performed on similar pixel groups, and the hard threshold of the transform coefficients and wiener filtering can effectively remove the artifacts introduced in the undersampled reconstructed mri images. the proposed pixel-level non-local iterative thinning model based on compressed sensing theory can ensure the removal of artifacts and better restore the details in the image. the qualitative and quantitative results under different undersampling modes and undersampling rates prove the advantages of the proposed method in subjective visual quality and objective evaluation (peak signal to noise ratio and structure similarity index). the performance of this method is not only superior to the existing traditional cs-mri methods, but also competitive with the existing deep neural network (dnn) based models. the code will be released at https://github.com/haohou-98/pncs.",AB_0325
"in this era of information overload, to better provide personalized content services to users, recommendation systems have greatly improved the efficiency of information distribution. graph convolution network(gcn), which is one of the representative works of graph structure aggregation processing, works by node convolution with the help of the laplacian matrix of the graph and weighted combination of neighbor node information according to the outgoing and incoming degrees of neighbor nodes to obtain the representation of the current node. however, the mainstream gcn models nowadays do not take into account data augmentation of metadata and the fact that each node plays different roles with different importance and weights, thus making the recommendation performance limited. to better solve the above problems, we propose the icagcn model, which can perform data augmentation and calculate node weights in modules, and is a convenient plug-and-play method. finally, extensive experimental results on four real-world datasets have shown the effectiveness and robustness of the proposed model. especially on the amazon-book dataset, our icagcn has improved by 6.32%, 42.29%, and 12.38% in recall@20, mrr@20, and ndcg@20, respectively, compared to other existing state-of-the-art models. we also provide source code and data to reproduce the experimental results available at https://github.com/personz1223/icagcn.git",AB_0325
