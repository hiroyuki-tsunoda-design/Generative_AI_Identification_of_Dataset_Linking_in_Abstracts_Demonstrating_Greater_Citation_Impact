AB,NO
"text2text question classification (tqc), as a particular application case of question classification (qc), is of great practical value. traditional qc methods usually label categories of questions using one or mul-tiple keywords provided by users. in contrast, in tqc, each question in natural language is automatically categorized into pre-designed standard question classes, which are coded in the form of short text. because of this unique characteristic, tqc relies on a specifically designed framework and should be trained and validated based on customized experimental datasets. previous tqc-related work mainly uti-lized textual similarity-matching methods. however, no effective pairwise learning paradigm has been proposed in tqc to model correlations between input text and classes; and the influence of distance met-rics and loss function in tqc has not been investigated. in this work, we propose a novel and comprehen-sive strategy, augmented dynamic multi-layer contrastive (admc), to resolve the challenge of tqc. our framework consists of (1) an optional data augmentation module, (2) one stage for dynamic negative sampling, and (3) one stage for precise matching. the comprehensive tqc framework with admc strat-egy in this work resolves data imbalance and explores distance metrics learning via multiple augmenta-tion options and dynamic negative sampling based on multi-layer contrastive learning. to compensate for the shortage of public datasets for this task, we collected two real-world datasets and adaptively expanded three existing public datasets, which will be available after data masking. the results show that our admc outperformed other baseline methods investigated in this paper. the codes are available at https://github.com/wjulyw/admc. (c) 2023 elsevier b.v. all rights reserved.",AB_0357
"recent years have witnessed remarkable progress of learning-based methods in single image dehazing. among them, dehazing methods trained on the synthetic images cannot adapt to real hazy ones due to the domain gap, and domain adaptation methods only concentrate on creating a mapping or extracting shared features regardless of representations of deep features. in this paper, we propose an innovative cross-domain dehazing architecture that integrates domain adaptation and disentangled representation. specifically, we first construct a shared encoder to map synthetic and real hazy images to a latent space and narrow their domain gap at the feature level. then we utilize a separator to separate the hazy image features into haze-free representations and haze ones. we introduce feature consistency loss and orthogonal loss to further guide the disentanglement process. and then, we utilize a decoder to produce clean images from haze-free features and introduce both supervised and unsupervised losses to guide the training process. moreover, we also propose to reconstruct the images from both domains by recombining the separated features, which guarantees information completeness. experiments demonstrate that our method is on par with state-of-the-art methods. codes are available at https://github.com/lixiao peng123456/dadrnet. (c) 2023 elsevier b.v. all rights reserved.",AB_0357
"deep clustering has become a popular technique in various fields due to its superior performance over conventional clustering. however, it can be challenging to classify data located near the decision bound-ary, and difficult or noisy samples can confuse or mislead the training process of deep neural networks, ultimately impacting clustering performance. to address this issue, we propose a novel self-paced deep clustering method that gradually increases the number of samples input into the network from easy to difficult. our approach involves attaching a loss prediction module to the convolutional neural network to judge the difficulty of samples. the module is robust as it relies on the input contents, not statistical estimates of uncertainty from outputs. moreover, this module selects the most informative data for the training model, thus enabling the network to converge stably and reach a good optimal solution. finally, we consider the reconstruction error, clustering loss, and loss prediction error to construct the loss of the model. experimental results on four image datasets show that our method outperforms state-of-the-art works. the main code can be found at https://github.com/clustering106/sdcll .(c) 2023 elsevier b.v. all rights reserved.",AB_0357
"learning-based local feature point extraction work has made great research progress in related computer vision tasks. however, it is difficult to balance the detection speed and detection efficiency of general deep neural networks, which are affected by illumination and viewpoint changes in outdoor environments. to this end, a lightweight image feature point efficient detection network jointly trained by detector-descriptor is proposed. the shared encoding of the feature map is carried out through the multi-branch structure, and a shuffle block model is designed to enhance the channel features. the enhanced feature is divided into three branches to further encode the feature map. one branch improves the network's ability to identify feature information through a lightweight attention mechanism, and the other two branches perform feature aggregation on the detector and the descriptor through skip connection layers to enrich image features. considering the problem of network operation efficiency, the multi-branch coding structure is coupled into a set of parameters corresponding to the plane forward propagation structure through the structure re-parameterization technology, which can reduce the complexity of the network and improve the inference speed of the network. in addition, the correct convergence of the network is guaranteed by constraints on the classification loss and position loss, and the sparsely sampled descriptors are used for training to reduce the computational cost. experimental results on the hpatches and titti datasets show that the proposed method reduces the parameters and gflops by 10.7% and 61.6%, respectively, compared with recent lightweight work, while maintaining comparable accuracy. moreover, the proposed method achieves a detection speed of 55fps on images with a resolution of 480x640. https://github.com/spiritashes/channel-enhancement.",AB_0357
"cross-domain few-shot learning (cdfsl) aims to classify new categories from new domains with few samples. it confronts a greater domain shift than few-shot learning (fsl). based on the transfer learn-ing framework, we propose a knowledge transduction method (kt) to alleviate domain shift and achieve few-shot recognition. first, a feature adaptation module based on feed-forward attention is constructed to learn domain-adapted features. the feature adaptation module weakens domain shift by transducing knowledge from an auxiliary dataset to the new dataset. second, a feature transduction module based on deep sparse representation is developed to gather class semantics from limited support images. the feature transduction module transduces knowledge from support images to query images for few-shot recognition. in addition, a stochastic image augmentation method is proposed for fsl to train a more generalized model through consistency representation learning. our method achieves competitive accu-racy on four cdfsl datasets and four fsl datasets compared to state-of-the-art methods. the source code is available at https://github.com/xdupfli/kt .(c) 2023 elsevier ltd. all rights reserved.",AB_0357
"domain generalization generalizes a prediction model trained on multiple source domains to an unseen target domain. the source and target domains are different but related, making cross domain model generalization challenging but possible. existing works assume that the domains are related by a feature transformation that makes the marginal distributions, the class-conditional distributions, or the posterior distributions similar among the domains, and learn this transformation via kernel mean matching or adversarial training. here, in a neural network context we relate the source and target domains via the network mapping, innovatively learn this mapping by matching multiple source joint distributions to their mixture distribution, and simultaneously learn a subsequent probabilistic classifier for target domain classification. to quantify the discrepancy among the source joint distributions, we exploit the kullback-leibler (kl) divergence, and show that in our case the kl divergence can be approximated via estimating a domain label posterior distribution. we model this discrete posterior distribution as multiple linear functions, and obtain their optimal parameters in an analytic manner. the resulting cost function is a combination of the cross-entropy loss and the estimated kl divergence, which is directly minimized via optimizing the network parameters. the experiments on several publicly available datasets demonstrate the effectiveness of our proposal. we release the source code at https://github.com/sentaochen/domain-generalization-by-distribution-estimation.",AB_0357
"recently the deep learning has shown its advantage in representation learning and clustering for time series data. despite the considerable progress, the existing deep time series cluster-ing approaches mostly seek to train the deep neural network by some instance reconstruction based or cluster distribution based objective, which, however, lack the ability to exploit the sample-wise (or augmentation-wise) contrastive information or even the higher-level (e.g., cluster-level) contrastiveness for learning discriminative and clustering-friendly representa-tions. in light of this, this paper presents a deep temporal contrastive clustering (dtcc) approach, which for the first time, to our knowledge, incorporates the contrastive learning paradigm into the deep time series clustering research. specifically, with two parallel views generated from the original time series and their augmentations, we utilize two identical auto-encoders to learn the corresponding representations, and in the meantime perform the cluster distribution learning by incorporating a k-means objective. further, two levels of con-trastive learning are simultaneously enforced to capture the instance-level and cluster-level contrastive information, respectively. with the reconstruction loss of the auto-encoder, the cluster distribution loss, and the two levels of contrastive losses jointly optimized, the net-work architecture is trained in a self-supervised manner and the clustering result can thereby be obtained. experiments on a variety of time series datasets demonstrate the superiority of our dtcc approach over the state-of-the-art. the code is available at https://github.com/ 07zy/dtcc.",AB_0357
"federated learning (fl) has recently made significant progress as a new machine learning paradigm for privacy protection. due to the high communication cost of traditional fl, one-shot federated learning is gaining popularity as a way to reduce communication cost between clients and the server. most of the existing one-shot fl methods are based on knowledge distillation; however, distillation based approach requires an extra training phase and depends on publicly available data sets or generated pseudo samples. in this work, we consider a novel and challenging cross-silo setting: performing a single round of parameter aggregation on the local models without server-side training. in this setting, we propose an effective algorithm for model aggregation via exploring common harmonized optima (ma-echo), which iteratively updates the parameters of all local models to bring them close to a common low-loss area on the loss surface, without harming performance on their own data sets at the same time. compared to the existing methods, ma-echo can work well even in extremely non-identical data distribution settings where the support categories of each local model have no overlapped labels with those of the others. we conduct extensive experiments on two popular image classification data sets to compare the proposed method with existing methods and demonstrate the effectiveness of ma-echo, which clearly outperforms the state-of-the-arts. the source code can be accessed in https://github.com/fudanvi/maecho.(c) 2023 elsevier ltd. all rights reserved.",AB_0357
"deep neural networks (dnns) are vulnerable to adversarial examples with small perturbations. adversarial defense thus has been an important means which improves the robustness of dnns by defending against adversarial examples. existing defense methods focus on some specific types of adversarial examples and may fail to defend well in real-world applications. in practice, we may face many types of attacks where the exact type of adversarial examples in real-world applications can be even unknown. in this paper, motivated by that adversarial examples are more likely to appear near the classification boundary and are vulnerable to some transformations, we study adversarial examples from a new perspective that whether we can defend against adversarial examples by pulling them back to the original clean distribution. we empirically verify the existence of defense affine transformations that restore adversarial examples. relying on this, we learn defense transformations to counterattack the adversarial examples by parameterizing the affine transformations and exploiting the boundary information of dnns. extensive experiments on both toy and real-world data sets demonstrate the effectiveness and generalization of our defense method. the code is avaliable at https://github.com/scutjinchengli/defensetransformer.",AB_0357
"accurately segmenting pancreas or pancreatic tumor from limited computed tomography (ct) scans plays an essential role in making a precise diagnosis and planning the surgical procedure for clinicians. al-though deep convolutional neural networks (dcnns) have greatly advanced in automatic organ segmen-tation, there are still many challenges in solving the pancreas segmentation problem with small region and complex background. many researchers have developed a coarse-to-fine scheme, which employ pre-diction from the coarse stage as a smaller input region for the fine stage. despite this scheme effec-tiveness, most existing approaches handle two stages individually, and fail to identify the reliability of coarse stage predictions. in this work, we present a novel coarse-to-fine framework based on spatial con-textual cues and active localization offset. the novelty lies in carefully designed two modules: spacial visual cues fusion (svcf) and active localization offset (alot). the svcf combines the correlations be-tween all pixels in an image to optimize the rough and uncertain pixel prediction at the coarse stage, while alot dynamically adjusts the localization as the coarse stage iteration. these two modules work together to optimize the coarse stage results and provide high-quality input for the fine stage, thereby achieving inspiring target segmentation. empirical results on nih pancreas segmentation and msd pan-creatic tumor segmentation dataset show that our framework yields state-of-the-art results. the code will make available at https://github.com/pinkghost0812/sanet .(c) 2023 elsevier b.v. all rights reserved.",AB_0357
