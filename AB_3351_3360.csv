AB,NO
"deep hashing techniques have received considerable attention and are widely used in the field of crossmodal retrieval. the training mode of deep cross-modal hashing models has also gradually converged. however, the commonly used training mode ignores noisy labels from different modality original datasets and can only generate fixed-length hash codes, resulting in poor robustness and flexibility of the training generated hash models. therefore, to effectively optimize the training mode, we propose a multilabel aware framework for bit-scalable deep cross-modal hashing (mafh) with the following contributions. first, we introduce a label network module to indirectly used the hash representation of multilabels to more accurately supervise the model training process across all modalities. second, we design a scalable coding strategy to freely manipulate the length of the hash code, which makes the trained model flexible to adapt to different scenarios. third, we propose a layered multilabel similarity algorithm to preserve more complete feature information, which avoids the loss of discriminative feature information during model training. the training mode of mafh can effectively enhance model robustness, flexibility and accuracy. we selected thirteen representative hash methods to compare with the proposed method, and experimental results on four public datasets show that the proposed method achieves good performance. https://github.com/x-28/mafh.git.(c) 2023 published by elsevier b.v.",AB_0336
"collaboratively leveraging limited pixel-level segmentation annotations and large-scale slide-level classification labels in hybrid supervision learning can significantly enhance model performance. however, the direct application of this approach within computational pathology presents challenges as end-to-end classification models grapple with processing high-resolution whole slide images (wsis). an alternative approach is to use patch-based models with pixel-level pseudo-labels, but these models can be susceptible to the cumulative effects of noisy labels, leading to convergence and drift problems during iterative training. to surmount these hurdles, we propose a hybrid supervision learning framework tailored for pathological image classification. our method employs coarse classification labels to optimize pixel-level pseudo-labels and incorporates a comprehensive strategy to diminish false negatives and positives throughout the segmentation process. this framework holistically integrates the supervised information derived from segmentation and classification procedures, and is applicable to the general classification of high-resolution images, thereby boosting both specificity and sensitivity. we assess our proposed method's effectiveness using one publicly accessible dataset and two proprietary datasets, collectively constituting over 10,000 pathological images of various disease types such as gastric, cervical, and breast cancer. our experimental results reveal a 100% sensitivity rate in slide-level classification tasks, simultaneously reducing the false-positive rate to a mere third of the state-of-the-art. in conclusion, this paper presents a potent instrument for the precise and efficient classification of high-resolution pathological images, with promising results showcased across a wide array of datasets and disease types. code is available at https://github.com/jarveelee/hybridsupervisionlearning_pathology.",AB_0336
"hypersphere guided embedding for masked face recognition has been proposed to address the problem encountered in the masked face recognition task, which arises due to non-biological information from occlusions. while some existing algorithms prefer to digesting the existence of masks by probing and covering, others aim to integrate face recognition and masked face recognition tasks into a unified solution domain. in this paper, we propose a framework to enable existing methods to accommodate multiple data distributions by orthogonal subspaces. specifically, we introduce constraints on multiple hypersphere manifolds via multi -center loss and employ a spatial split strategy to ensure the orthogonality of base vectors associated with different hypersphere manifolds, corresponding to distinct distribution. our method is extensively evaluated on publicly available datasets on face recognition, mask face recognition and occlusion, demonstrating promising performance. our code is available on an anonymous website: https://github.com/captainkai/he_mfr.",AB_0336
"image restoration under severe weather is a challenging task. most of the past works focused on removing rain and haze phenomena in images. however, snow is also an extremely common atmospheric phenomenon that will seriously affect the performance of high-level computer vision tasks, such as object detection and semantic segmentation. recently, some methods have been proposed for snow removing, and most methods deal with snow images directly as the optimization object. however, the distribution of snow location and shape is complex. therefore, failure to detect snowflakes/snow streak effectively will affect snow removing and limit the model performance. to solve these issues, we propose a snow mask guided adaptive residual network (smgarn). specifically, smgarn consists of three parts, mask-net, guidance-fusion network (gf-net), and reconstruct-net. firstly, we build a mask-net with self-pixel attention (sa) and cross-pixel attention (ca) to capture the features of snowflakes and accurately localized the location of the snow, thus predicting an accurate snow mask. secondly, the predicted snow mask is sent into the specially designed gf-net to adaptively guide the model to remove snow. finally, an efficient reconstruct-net is used to remove the veiling effect and correct the image to reconstruct the final snow-free image. furthermore, we propose a more refined dataset of real snow images, snowworld24, to provide faster evaluation of snow-free images. extensive experiments show that our smgarn numerically outperforms all existing snow removal methods, and the reconstructed images are clearer in visual contrast. all codes are available at https://github.com/mivrc/smgarn.",AB_0336
"compared with other biometrics, such as fingerprints and irises, eeg signals contain richer identity information due to their high subject dependence. in this study, we proposed an affective eeg-based person identification by using an affective interrelated temporal-spatial transformer (aitst) to overcome the influence of different emotional states in affective eeg-based person identification. the aitst contains an interrelated temporal- spatial attention that can preserve the interrelationships of temporal and spatial domains while extracting temporal and spatial features of the eeg signal. we evaluated the proposed aitst on the public emotional database deap. the experimental results show that our aitst can reach an average recognition accuracy of 98.94 & plusmn;0.04% in the low valence, low arousal state, 99.06 & plusmn;0.02% in low valence, high arousal state, 98.97 & plusmn;0.02% in high valence, low arousal state, and 99.21 & plusmn;0.03% in the high valence, high arousal state, significantly outperforming the other compared models and overcoming the influence of different emotional states. furthermore, we explored the contribution of different frequency bands, different emotional states and different features of eeg for eeg-based person identification. the results showed that gamma band of eeg, power spectrum density (psd), and high arousal state have the greatest contribution to affective eeg-based person identification. source code is available at https://github.com/jerrykingqaq/aitst.",AB_0336
"the aim of camouflaged object detection (cod) is to find objects that are hidden in their surrounding environment. due to the factors like low illumination, occlusion, small size and high similarity to the background, cod is recognized to be a very challenging task. in this paper, we propose a general cod framework, termed as mscaf-net, focusing on learning multi-scale context-aware features. to achieve this target, we first adopt the improved pyramid vision transformer (pvtv2) model as the backbone to extract global contextual information at multiple scales. an enhanced receptive field (erf) module is then designed to refine the features at each scale. further, a cross-scale feature fusion (csff) module is introduced to achieve sufficient interaction of multi-scale information, aiming to enrich the scale diversity of extracted features. in addition, inspired the mechanism of the human visual system, a dense interactive decoder (did) module is devised to output a rough localization map, which is used to modulate the fused features obtained in the csff module for more accurate detection. the effectiveness of our mscaf-net is validated on four benchmark datasets. the results show that the proposed method significantly outperforms state-of-the-art (sota) cod models by a large margin. besides, we also investigate the potential of our mscaf-net on some other vision tasks that are highly related to cod, such as polyp segmentation, covid-19 lung infection segmentation, transparent object detection and defect detection. experimental results demonstrate the high versatility of the proposed mscaf-net. the source code and results of our method are available at https://github.com/yuliu316316/mscaf-cod.",AB_0336
"hard-constrained text generation is an important task that requires generating fluent sentences to include several specific keywords. it has numerous real-world applications, such as advertisement generation, keyword-based summary generation, and query rewriting. although previous plug-and play approaches like enhanced beam search and stochastic search have been proven effective, they lack time efficiency and may reduce the quality of generated sentences. while end-to-end methods based on seq2seq models are superior in speed, they cannot guarantee that outputs satisfy all constraints. in this work, we propose a novel end-to-end method for lexically constrained text generation via incrementally predicting segments (ips) between every two adjacent lexical constraints using seq2seq models. our approach guarantees that all constrained keywords will be included in the generated sentence. the experimental results show that our method not only satisfies all lexical constraints but also achieves state-of-the-art performance. our code and data will be available at https://github.com/ blcuicall/ips.(c) 2023 elsevier b.v. all rights reserved.",AB_0336
"compared with spatial counterparts, temporal relationships between frames and their influences on video quality assessment (vqa) are still relatively under-studied in existing works. these relationships lead to two important types of effects for video quality. firstly, some meaningless temporal variations (such as shaking, flicker, and unsmooth scene transitions) cause temporal distortions that degrade quality of videos. secondly, the human visual system often has different attention to frames with different contents, resulting in their different importance to the overall video quality. based on prominent time-series modeling ability of transformers, we propose a novel and effective transformer-based vqa method to tackle these two issues. to better differentiate temporal variations and thus capture the temporal distortions, we design the spatial-temporal distortion extraction (stde) module that extracts multi-level spatial-temporal features with a video swin transformer tiny (swin-t) backbone and uses temporal difference layer to further capture these distortions. to tackle with temporal quality attention, we propose the encoder-decoder-like temporal content transformer (tct). we also introduce the temporal sampling on features to reduce the input length for the tct, so as to improve the learning effectiveness and efficiency of this module. consisting of the stde and the tct, the proposed temporal distortion-content transformers for video quality assessment (discovqa) reaches state-of-the-art performance on several vqa benchmarks without any extra pre-training datasets and up to 10% better generalization ability than existing methods. we also conduct extensive ablation experiments to prove the effectiveness of each part in our proposed model, and provide visualizations to prove that the proposed modules achieve our intention on modeling these temporal issues. our code is published at https://github.com/qualityassessment/discovqa.",AB_0336
"recently, some researchers have begun to adopt the transformer to combine or replace the widely used resnet as their new backbone network. as the transformer captures the long-range relations between pixels well using the self-attention scheme, which complements the issues caused by the limited receptive field of cnn. although their trackers work well in regular scenarios, they simply flatten the 2d features into a sequence to better match the transformer. we believe these operations ignore the spatial prior of the target object, which may lead to sub-optimal results only. in addition, many works demonstrate that self-attention is actually a low-pass filter, which is independent of input features or keys/queries. that is to say, it may suppress the high-frequency component of the input features and preserve or even amplify the low-frequency information. to handle these issues, in this paper, we propose a unified spatial-frequency transformer that models the gaussian spatial prior and high-frequency emphasis attention (gpha) simultaneously. to be specific, gaussian spatial prior is generated using dual multi-layer perceptrons (mlps) and injected into the similarity matrix produced by multiplying query and key features in self-attention. the output will be fed into a softmax layer and then decomposed into two components, i.e., the direct and high-frequency signal. the low- and high-pass branches are rescaled and combined to achieve all-pass, therefore, the high-frequency features will be protected well in stacked self-attention layers. we further integrate the spatial-frequency transformer into the siamese tracking framework and propose a novel tracking algorithm termed sftranst. the cross-scale fusion based swintransformer is adopted as the backbone, and also a multi-head cross-attention module is used to boost the interaction between search and template features. the output will be fed into the tracking head for target localization. extensive experiments on short-term and long-term tracking benchmarks all demonstrate the effectiveness of our proposed framework. source code will be released at https://github.com/tchuanm/sftranst.git.",AB_0336
"pretrained vision-language models (vlms) such as clip have shown impressive generalization capability in downstream vision tasks with appropriate text prompts. instead of designing prompts manually, context optimization (coop) has been recently proposed to learn continuous prompts using task-specific training data. despite the performance improvements on downstream tasks, several studies have reported that coop suffers from the overfitting issue in two aspects: (i) the test accuracy on base classes first improves and then worsens during training; (ii) the test accuracy on novel classes keeps decreasing. however, none of the existing studies can understand and mitigate such overfitting problems. in this study, we first explore the cause of overfitting by analyzing the gradient flow. comparative experiments reveal that coop favors generalizable and spurious features in the early and later training stages, respectively, leading to the non-overfitting and overfitting phenomena. given those observations, we propose subspace prompt tuning (sub pt) to project the gradients in back-propagation onto the low-rank subspace spanned by the early-stage gradient flow eigenvectors during the entire training process and successfully eliminate the overfitting problem. in addition, we equip coop with a novel feature learner (nfl) to enhance the generalization ability of the learned prompts onto novel categories beyond the training set, needless of image training data. extensive experiments on 11 classification datasets demonstrate that sub pt+nfl consistently boost the performance of coop and outperform the state-of-the-art cocoop approach. experiments on more challenging vision downstream tasks, including open-vocabulary object detection and zero-shot semantic segmentation, also verify the effectiveness of the proposed method. codes can be found at https://tinyurl.com/mpe64f89.",AB_0336
