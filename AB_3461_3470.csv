AB,NO
"feature matching for multimodal images is an important task in image processing. however, most methods perform image feature detection, description, and matching sequentially, resulting in a large loss, low matching accuracy, and slow performance. to tackle these challenges, we propose a detector-free method called femip for feature matching of multimodal images. we design coarse matching and fine regression modules to implement accurate multimodal image feature matches in a coarse-to-fine manner. furthermore, we add a novel data augmentation method enabling femip to achieve feature matching faster and more accurately. the coarse-to-fine module automatically generates pixel-level labels on the original image, enabling femip to perform pixel-level matching on data with only image-level labels. in addition, we use the principle of reinforcement learning to design a policy gradient method to improve the solution to the problem of discreteness in matching. extensive experiments show that femip has good generalization and achieves excellent matching performances. the code will be released at: https://github.com/liaoyun0x0/femip.",AB_0347
"to effectively solve the problems of human occlusion and motion blur in pose estimation algorithms, this paper proposes a bottom-up method for multi-person pose estimation based on human anchor joints and perception-enhancement networks. first, in terms of the detection task label, we divide human joint points into two groups-upper body and lower body-and then select two geometric anchor joint points from the two groups as joint point matching clues. then, the other joint points of each group are represented by offset embedding of the joint point matching clues. furthermore, two directional anchor joints that are rich in human orientation information are added to constitute a set of human anchor joints and form a new network detection target. second, we design a perception-enhancement network based on the attention mechanism and feature fusion strategy, which can help the network effectively learn the unique features of each half-body and the inherent consistent features of the whole body. the proposed network has a stronger detection task modelling ability. in the test phase, based on the greedy strategy, the postprocessing algorithm is carried out to obtain the pose estimation results of multiple people by the final joint extraction and matching. the experimental results on the mpii dataset and crowdpose dataset demonstrate the effectiveness of the proposed method. the code is open source and available online (https://github.com/ozone-oo/perception_ enhancement_network.git).",AB_0347
"this paper shows that time series forecasting transformer (tsft) suffers from severe over-fitting problem caused by improper initialization method of unknown decoder inputs, especially when handling non-stationary time series. based on this observation, we propose gbt, a novel two-stage transformer framework with good beginning. it decouples the prediction process of tsft into two stages, including auto-regression stage and self-regression stage to tackle the problem of different statistical properties between input and prediction sequences. prediction results of auto-regression stage serve as a 'good beginning', i.e., a better initialization for inputs of self-regression stage. we also propose the error score modification module to further enhance the forecasting capability of the selfregression stage in gbt. extensive experiments on seven benchmark datasets demonstrate that gbt outperforms sota tsfts (fedformer, pyraformer, etsformer, etc.) and many other forecasting models (scinet, n-hits, etc.) with only canonical attention and convolution while owning less time and space complexity. it is also general enough to couple with these models to strengthen their forecasting capability. the source code is available at: https://github.com/origamisl/gbt & copy; 2023 the author(s). published by elsevier ltd. this is an open access article under the cc by-nc-nd license ().",AB_0347
"data-free knowledge distillation (dfkd) is an effective manner to solve model compression and trans-mission restrictions while retaining privacy protection, which has attracted extensive attention in recent years. currently, the majority of existing methods utilize a generator to synthesize images to support the distillation. although the current methods have achieved great success, there are still many issues to be explored. firstly, the outstanding performance of supervised learning in deep learning drives us to explore a pseudo-supervised paradigm on dfkd. secondly, current synthesized methods cannot distin-guish the distributions of different categories of samples, thus producing ambiguous samples that may lead to an incorrect evaluation by the teacher. besides, current methods cannot optimize the category -wise diversity samples, which will hinder the student model learning from diverse samples and further achieving better performance. in this paper, to address the above limitations, we propose a novel learning paradigm, i.e., conditional pseudo-supervised contrast for data-free knowledge distillation (cpsc-dfkd). the primary innovations of cpsc-dfkd are: (1) introducing a conditional generative adversarial network to synthesize category-specific diverse images for pseudo-supervised learning, (2) improving the mod-ules of the generator to distinguish the distributions of different categories, and (3) proposing pseudo -supervised contrastive learning based on teacher and student views to enhance diversity. comprehensive experiments on three commonly-used datasets validate the performance lift of both the student and gen-erator brought by cpsc-dfkd. the code is available at https://github.com/roryshao/cpsc-dfkd.git & copy; 2023 elsevier ltd. all rights reserved.",AB_0347
"graph similarity estimation is a challenging task due to the complex graph structure. though important and well-studied, three key aspects are yet to be fully handled in a unified framework: (i) how to exploit the node embedding by leveraging both local spatial neighborhood information and the global context, (ii) how to effectively learn richer cross graph interactions from a pairwise node perspective and (iii) how to map the similarity matrix into a similarity score by exploiting the inherent structure in the similarity matrix. to solve these issues, we explore multiple attention mechanisms for graph similarity learning in this work. more specifically, we propose a unified graph similarity learning framework involving (i) a hybrid of graph convolution and graph self-attention for node embedding learning, (ii) a cross graph co-attention (gca) module for graph interaction modeling, (iii) similarity-wise self-attention (ssa) module for graph similarity matrix alignment and (iv) graph similarity matrix learning for predicting the similarity scores. extensive experimental results on three challenging benchmarks including linux, aids, and imdbmulti demonstrate that the proposed na-gsl performs favorably against state-of-the-art graph similarity estimation methods. the code is available at https://github.com/alberttan404/na-gsl. & copy; 2023 elsevier b.v. all rights reserved.",AB_0347
"the importance of hierarchical image organization has been witnessed by a wide spectrum of applications in computer vision and graphics. different from image segmentation with the spatial whole-part consideration, this work designs a modern framework for disassembling an image into a family of derived signals from a scale-space perspective. specifically, we first formulate the goal of the hierarchical image organization problem. then, by concerning desired properties, such as peeling hierarchy and structure preservation, we convert the original complex problem into a series of two-component separation sub-problems, significantly reducing the complexity. the proposed framework is flexible to be trained on both paired and unpaired data. a compact recurrent network, namely hierarchical image peeling net, is customized to efficiently and effectively fulfill the task, which is about 3.5mb in size, and can handle 1080p images in more than 60 fps per recurrence on a gtx 2080ti gpu, making it attractive for practical use. both theoretical findings and experimental results are provided to demonstrate the efficacy of the proposed framework, reveal its superiority over other state-ofthe-art alternatives, and show its potential to various applicable scenarios. the codes have been made publicly available at https://github.com/forawardstar/hipe.",AB_0347
"multiple kernel k-means clustering (mkkc) is proposed to efficiently incorporate multiple base kernels to generate an optimal kernel. however, many existing mkkc methods all involve two stages: learning a clustering indicator matrix and performing clustering on it. this cannot ensure the ultimate clustering results are optimal because the optimal values of two steps are not equivalent to those of the original problem. to address this issue, in this paper, we propose a novel method named multiple kernel k-means clustering with block diagonal property (mkkc-bd). it is the first time to find the relationship between an indicator matrix and laplacian matrix of the graph theory and get a block diagonal (bd) representation of the indicator matrix. by imposing the bd constraint on the indicator matrix, the bd property of the indicator matrix is ensured. further, the explicit clustering results are generated directly from the unified framework integrating the three processes of learning an optimal kernel, an indicator matrix and clustering results, which shows the clustering task is executed just by one step. in addition, a simple kernel weight strategy is used in this framework to obtain the optimal kernel, where the value of each kernel weight directly reveals the relationship of each base kernel and the optimal kernel. finally, by extensive experiments on ten data sets and comparison of clustering results with eight state-of-the-art multiple kernel clustering methods, it is concluded that mkkc-bd is effective. our code is available at https://github.com/mathchen-git/mkkc-bd.",AB_0347
"along with the development of modern smart cities, human-centric video analysis has been encountering the challenge of analyzing diverse and complex events in real scenes. a complex event relates to dense crowds, anomalous individuals, or collective behaviors. however, limited by the scale and coverage of existing video datasets, few human analysis approaches have reported their performances on such complex events. to this end, we presenta new large-scale dataset with comprehensive annotations, named human-in-events or human-centric video analysis in complex events (hieve), for the understanding of human motions, poses, and actions in a variety of realistic events, especially in crowd and complex events. it contains a record number of poses (> 1 m), the largest number of action instances (> 56k) under complex events, as well as one of the largest numbers of trajectories lasting for longer time (with an average trajectory length of >480 frames). based on its diverse annotation, we present two simple baselines for action recognition and pose estimation, respectively. they leverage cross-label information during training to enhance the feature learning in corresponding visual tasks. experiments show that they could boost the performance of existing action recognition and pose estimation pipelines. more importantly, they prove the widely ranged annotations in hieve can improve various video tasks. furthermore, we conduct extensive experiments to benchmark recent video analysis approaches together with our baseline methods, demonstrating hieve is a challenging dataset for human-centric video analysis. we expect that the dataset will advance the development of cutting-edge techniques in human-centric analysis and the understanding of complex events. the dataset is available at http://humaninevents.org.",AB_0347
"deep learning-based approaches have recently achieved considerable results in poisson denoising under low-light conditions. however, most existing methods mainly focus on the network architecture design, which lacks physical interpretability and thus unsuitable for blind denoising in real environments with unknown levels of noises. to address this issue, we propose a variational bayesian deep network for blind poisson denoising (vbdnet). we mainly consider an approximate posterior form for the noise variance in a variational bayesian framework and utilize a neural network to parameterize the variance of poisson noise. for network design, vbdnet is divided into two sub-networks. the noise estimation sub-network is responsible for the bayesian inference. this network improves the blind denoising ability of the subse-quent denoising sub-network by learning poisson noise characteristics under different noise levels in the training process. a network of u-net structures implements the denoising sub-network for noise removal. by combining the advantage of bayesian inference (noise estimation sub-network) and deep learning (de -noising sub-network), vbdnet outperforms other state-of-the-art methods on both synthetic and natural data. the code and details are available at https://github.com/hlimg/vbdnet .& copy; 2023 elsevier ltd. all rights reserved.",AB_0347
"recent studies have made remarkable progress on 3d human motion prediction by describing motion with kinematic knowledge. however, kinematics only considers the 3d positions or rotations of human skeletons, failing to reveal the physical characteristics of human motion. motion dynamics reflects the forces between joints, explicitly encoding the skeleton topology, whereas rarely exploited in motion pre-diction. in this paper, we propose the kinematic and dynamic coupled transformer (kd-former), which incorporates dynamics with kinematics, to learn powerful features for high-fidelity motion prediction. specifically, we first formulate a reduced-order dynamic model of human body to calculate the forces of all joints. then we construct a non-autoregressive encoder-decoder framework based on the transformer structure. the encoder involves a kinematic encoder and a dynamic encoder, which are respectively re-sponsible for extracting the kinematic and dynamic features for given history sequences via a spatial transformer and a temporal transformer. future query sequences are decoded in parallel in the decoder by leveraging the encoded kinematic and dynamic information of history sequences. experiments on hu-man3.6m and cmu mocap benchmarks verify the effectiveness and superiority of our method. code will be available at: https://github.com/wslh852/kd-former.git . & copy; 2023 published by elsevier ltd.",AB_0347
