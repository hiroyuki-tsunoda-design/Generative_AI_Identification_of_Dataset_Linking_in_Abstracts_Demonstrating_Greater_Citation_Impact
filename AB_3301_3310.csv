AB,NO
"snow removal aims to locate snow areas and recover clean images without repairing traces. unlike the regularity and semitransparency of rain, snow with various patterns and degradations seriously occludes the background. as a result, the state-of-the-art snow removal methods usually retains a large parameter size. in this paper, we propose a lightweight but high-efficient snow removal network called laplace mask query transformer (lmqformer). firstly, we present a laplace-vqvae to generate a coarse mask as prior knowledge of snow. instead of using the mask in dataset, we aim at reducing both the information entropy of snow and the computational cost of recovery. secondly, we design a mask query transformer (mqformer) to remove snow with the coarse mask, where we use two parallel encoders and a hybrid decoder to learn extensive snow features under lightweight requirements. thirdly, we develop a duplicated mask query attention (dmqa) that converts the coarse mask into a specific number of queries, which constraint the attention areas of mqformer with reduced parameters. experimental results in popular datasets have demonstrated the efficiency of our proposed model, which achieves the state-of-the-art snow removal quality with significantly reduced parameters and the lowest running time. codes and models are available at https://github.com/stephenlinn/lmqformer.",AB_0331
"detection transformer (detr) for object detection reaches competitive performance compared with faster r-cnn via a transformer encoder-decoder architecture. however, trained with scratch transformers, detr needs large-scale training data and an extreme long training schedule even on coco dataset. inspired by the great success of pre-training transformers in natural language processing, we propose a novel pretext task named random query patch detection in unsupervised pre-training detr (up-detr). specifically, we randomly crop patches from the given image and then feed them as queries to the decoder. the model is pre-trained to detect these query patches from the input image. during the pre-training, we address two critical issues: multi-task learning and multi-query localization. (1) to trade off classification and localization preferences in the pretext task, we find that freezing the cnn backbone is the prerequisite for the success of pre-training transformers. (2) to perform multi-query localization, we develop up-detr with multi-query patch detection with attention mask. besides, up-detr also provides a unified perspective for fine-tuning object detection and one-shot detection tasks. in our experiments, up-detr significantly boosts the performance of detr with faster convergence and higher average precision on object detection, one-shot detection and panoptic segmentation. code and pre-training models: https://github.com/dddzg/up-detr.",AB_0331
"deepfake technique can synthesize realistic images, audios, and videos, facilitating the thriving of entertainment, education, healthcare, and other industries. however, its abuse may pose potential threats to personal privacy, social stability, and even national security. therefore, the development of deepfake detection methods is attracting more and more attention. existing works mainly focus on the detection of common videos for entertainment purposes. in contrast, fake videos maliciously synthesized for person of interest (poi, i.e., who is in an authoritative position and has broadly public influences) are much more harmful to society because of celebrity endorsement. however, there is no particular benchmark for driving related research in the community. motivated by this observation, we present the first large-scale benchmark dataset, named fakepoi, to enable the research on fake poi detection. it contains numerous fake videos of important people from all walks of life, e.g., police chiefs, city mayors, famous artists, and well-known internet bloggers. in summary, our fakepoi includes 11092 synthesized videos where only a few clips rather than the entire are fake. previous fake detection algorithms deteriorate heavily or even fail on our fakepoi due to two main challenges. on the one hand, the rich diversity of our fake videos makes it pretty difficult to find universally applicable patterns for detection. on the other hand, the high credibility contributed by the presence of real frames easily confuses a common detector. to tackle these challenges, we present an amplifier framework, highlighting the feature gap between real and generated video frames. specifically, we present a quadruplet loss to narrow the distance of all real pois and meanwhile push away each real and fake poi in embedding space. we implement our framework and conduct extensive experiments on the proposed benchmark. the quantitative results demonstrate that our approach outperforms existing methods significantly, setting a strong baseline on fakepoi. the qualitative analysis also shows its superiority. we will release our dataset and code at https://github.com/cslltian/deepfake-detection to encourage future research on this valuable area.",AB_0331
"in recent years, graph-based deep learning algorithms have attracted widespread attention in the field of consumer electronics. still, most of the current graph neural networks are based on supervised learning or semi-supervised learning, which often relies on the true labels of the given samples as auxiliary information. to solve this problem, we propose a deep self-supervised attention convolution autoencoder graph clustering (dsagc) model and use it for social networks clustering. we divide the proposed model into two parts: a pretext task and a downstream task. in the pretext task, we obtain pseudo-labels of samples by graph attention autoencoder and clustering, then adopt the proposed reliable sample selection mechanism to gain a high confidential sample. these samples and corresponding pseudo-labels are selected as input of downstream task for helping the downstream task to finish training. the model obtains the predictive labels of the sample from the downstream task. this model does not depend on the true label of the sample for training and uses the pseudo-labels produced by unsupervised learning to improve the clustering performance. we apply the proposed model to three commonly used public social network datasets to test its performance. compared with the presented unsupervised clustering algorithms and other deep graph neural network algorithms, various metrics have shown that the proposed model consistently outperforms the state-of-the-art. the proposed method can be used for consumer behavior analysis. we release the source code at https://github.com/hulu88/dsagc.",AB_0331
"mainstream multi-object tracking methods exploit appearance information and/or motion information to achieve interframe association. however, dealing with similar appearance and occlusion is a challenge for appearance information, while motion information is limited by linear assumptions and is prone to failure in nonlinear motion patterns. in this work, we disregard appearance clues and propose a pure motion tracker to address the above issues. it dexterously utilizes transformer to estimate complex motion and achieves high-performance tracking with low computing resources. furthermore, contrastive learning is introduced to optimize feature representation for robust association. specifically, we first exploit the long-range modeling capability of transformer to mine intention information in temporal motion and decision information in spatial interaction and introduce prior detection to constrain the range of motion estimation. then, we introduce contrastive learning as an auxiliary task to extract reliable motion features to compute affinity and introduce bidirectional matching to improve the affinity computation distribution. in addition, given that both tasks are dedicated to narrowing the embedding distance between the motion features of the tracked object and the detection features, we design a joint-motion-and-association framework to unify the above two tasks in one framework for optimization. the experimental results achieved with three benchmark datasets, mot17, mot20 and dancetrack, verify the effectiveness of our proposed method. compared with state-of-the-art methods, the proposed stdformer sets a new state-of-the-art on dancetrack and achieves competitive performance on mot17 and mot20. this demonstrates the advantage of our method in handling associations under similar appearance, occlusion or nonlinear motion. at the same time, the significant advantages of the proposed method over transformer-based and contrastive learning-based methods suggest a new direction for the application of transformer and contrastive learning in mot. in addition, to verify the generalization of stdformer in unmanned aerial vehicle (uav) videos, we also evaluate stdformer on visdrone2019. the results show that stdformer achieves state-of-the-art performance on visdrone2019, which proves that it can handle small-scale object associations in uav videos well. the code is available at https://github.com/xiaotong-zhu/stdformer.",AB_0331
"davis camera, streaming two complementary sensing modalities of asynchronous events and frames, has gradually been used to address major object detection challenges (e.g., fast motion blur and low-light). however, how to effectively leverage rich temporal cues and fuse two heterogeneous visual streams remains a challenging endeavor. to address this challenge, we propose a novel streaming object detector with transformer, namely sod-former, which first integrates events and frames to continuously detect objects in an asynchronous manner. technically, we first build a large-scale multimodal neuromorphic object detection dataset (i.e., pku-davis-sod) over 1080.1 k manual labels. then, we design a spatiotemporal transformer architecture to detect objects via an end-to-end sequence prediction problem, where the novel temporal transformer module leverages rich temporal cues from two visual streams to improve the detection performance. finally, an asynchronous attention-based fusion module is proposed to integrate two heterogeneous sensing modalities and take complementary advantages from each end, which can be queried at any time to locate objects and break through the limited output frequency from synchronized frame-based fusion strategies. the results show that the proposed sodformer outperforms four state-of-the-art methods and our eight baselines by a significant margin. we also show that our unifying framework works well even in cases where the conventional frame-based camera fails, e.g., high-speed motion and low-light conditions. our dataset and code can be available at https://github.com/dianzl/sodformer.",AB_0331
"vision transformers have recently attained state-of-the-art results in visual recognition tasks. their success is largely attributed to the self-attention component, which models the global dependencies among the image patches (tokens) and aggregates them into higher-level features. however, self-attention brings significant training difficulties to vits. many recent works thus develop various new self-attention components to alleviate this issue. in this article, instead of developing complicated self-attention mechanism, we aim to explore simple approaches to fully release the potential of the vanilla self-attention. we first study the token selection behavior of self-attention and find that it suffers from a low diversity due to attention over-smoothing, which severely limits its effectiveness in learning discriminative token features. we then develop simple approaches to enhance selectivity and diversity for self-attention in token selection. the resulted token selector module can server as a drop-in module for various vit backbones and consistently boost their performance. significantly, they enable vits to achieve 84.6% top-1 classification accuracy on imagenet with only 25m parameters. when scaled up to 81m parameters, the result can be further improved to 86.1%. in addition, we also present comprehensive experiments to demonstrate the token selector can be applied to a variety of transformer-based models to boost their performance for image classification, semantic segmentation and nlp tasks. code is available at https://github.com/zhoudaquan/dvit_repo.",AB_0331
"image harmonization, aiming to make composite images look more realistic, is an important and challenging task. the composite, synthesized by combining foreground from one image with background from another image, inevitably suffers from the issue of inharmonious appearance caused by distinct imaging conditions, i.e., lights. current solutions mainly adopt an encoder-decoder architecture with convolutional neural network (cnn) to capture the context of composite images, trying to understand what it should look like in the foreground referring to surrounding background. in this work, we seek to solve image harmonization with transformer, by leveraging its powerful ability of modeling long-range context dependencies, for adjusting foreground light to make it compatible with background light while keeping structure and semantics unchanged. we present the design of our two vision transformer frameworks and corresponding methods, as well as comprehensive experiments and empirical study, demonstrating the power of transformer and investigating the transformer for vision. our methods achieve state-of-the-art performance on the image harmonization as well as four additional vision and graphics tasks, i.e., image enhancement, image inpainting, white-balance editing, and portrait relighting, indicating the superiority of our work. code, models, more results and details can be found at the project website http://ouc.ai/project/harmonytransformer.",AB_0331
"recently, the vision transformer has achieved great success by pushing the state-of-the-art of various vision tasks. one of the most challenging problems in the vision transformer is that the large sequence length of image tokens leads to high computational cost (quadratic complexity). a popular solution to this problem is to use a single pooling operation to reduce the sequence length. this paper considers how to improve existing vision transformers, where the pooled feature extracted by a single pooling operation seems less powerful. to this end, we note that pyramid pooling has been demonstrated to be effective in various vision tasks owing to its powerful ability in context abstraction. however, pyramid pooling has not been explored in backbone network design. to bridge this gap, we propose to adapt pyramid pooling to multi-head self-attention (mhsa) in the vision transformer, simultaneously reducing the sequence length and capturing powerful contextual features. plugged with our pooling-based mhsa, we build a universal vision transformer backbone, dubbed pyramid pooling transformer (p2t). extensive experiments demonstrate that, when applied p2t as the backbone network, it shows substantial superiority in various vision tasks such as image classification, semantic segmentation, object detection, and instance segmentation, compared to previous cnn- and transformer-based networks. the code will be released at https://github.com/yuhuan-wu/p2t.",AB_0331
"many swarm intelligence techniques are facing rigorous challenges since they cannot exploit useful information well during the evolutionary procedure. to remedy this issue, this paper raises a reinforced jaya algorithm (qljaya) that employs the q-learning and gradient search scheme. in qljaya, to balance convergence and diversity, a modified search formula and gradient search scheme are adaptively selected to generate solutions under the control of q-learning. in addition, to strengthen the rotational invariance of jaya, the covariance matrix learning strategy is adopted to construct an eigen coordinate system for each solution. experimental simulations on cec2017 and cec2019 test suites and application to identify parameters of photovoltaic systems suggest that qljaya can exhibit a better or at least competitive overall performance compared to several typical jaya variants and other well-known metaheuristics. the source code of qljaya is publicly available at https://github.com/denglingyun123/qljaya.",AB_0331
