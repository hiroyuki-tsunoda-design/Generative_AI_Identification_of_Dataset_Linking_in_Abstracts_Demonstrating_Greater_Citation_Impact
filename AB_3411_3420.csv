AB,NO
"efficient communication is significant for federated learning and dnn model deployment. however, transferring hundreds of millions of dnn parameters over networks with limited bandwidth results in long communication delays or even data losses. to alleviate or even remove the communication bottleneck, efficient methods for parameter compression can be applied. inspired by video encoding, which exploits inter-frame similarity for compression, we investigate the strong temporal correlations of parameter updates in two near epochs of the dnn model and introduce a model parameter residual encoding framework. by transmitting encoded residual between model parameters in two near epochs, the receiver can reconstruct new model parameters and finish the updates with less communication cost. furthermore, with respect to our framework, we develop lossless and lossy model parameter compression methods and demonstrate them on popular classification and detection networks. the results show that the lossless method can compress the data size of the parameters to less than 90%, and the lossy method can shrink the parameter size to less than 50% with a fair low loss. our source code is released at https://github.com/zhouliguo/dnn_param_encode. & copy; 2023 elsevier b.v. all rights reserved.",AB_0342
"we attempt to reduce the computational costs in vision transformers (vits), which increase quadratically in the token number. we present a novel training paradigm that trains only one vit model at a time, but is capable of providing improved image recognition performance with various computational costs. here, the trained vit model, termed super vision transformer (supervit), is empowered with the versatile ability to solve incoming patches of multiple sizes as well as preserve informative tokens with multiple keeping rates (the ratio of keeping tokens) to achieve good hardware efficiency for inference, given that the available hardware resources often change from time to time. experimental results on imagenet demonstrate that our supervit can considerably reduce the computational costs of vit models with even performance increase. for example, we reduce 2 x flops of deit-s while increasing the top-1 accuracy by 0.2% and 0.7% for 1.5 x reduction. also, our supervit significantly outperforms existing studies on efficient vision transformers. for example, when consuming the same amount of flops, our supervit surpasses the recent state-of-the-art evit by 1.1% when using deit-s as their backbones. the project of this work is made publicly available at https://github.com/lmbxmu/supervit.",AB_0342
"model filter pruning has shown efficiency in compressing deep convolutional neural networks by removing unimportant filters without sacrificing the performance. however, most existing criteria are empirical, and overlook the relationship between channel saliencies and the non-linear activation functions within the networks. to address these problems, we propose a novel channel pruning method coined gradient flow based saliency (gfbs). instead of relying on the magnitudes of the entire feature maps, gfbs evaluates the channel saliencies from the gradient flow perspective and only requires the information in normalization and activation layers. concretely, we first integrate the effects of normalization and relu activation layers into convolutional layers based on taylor expansion. then, through backpropagation, the derived channel saliency of each layer is indicated by of the first-order taylor polynomial of the scaling parameter and the signed shifting parameter in the normalization layers. to validate the efficiency and generalization ability of gfbs, we conduct extensive experiments on various tasks, including image classification (cifar, imagenet), image denoising, object detection, and 3d object classification. gfbs could feasibly cooperate with the baseline networks and compress them with only negligible performance drop. moreover, we extended our method to pruning scratch networks and gfbs is capable to identify subnetworks with comparable performance with the baseline model at an early training stage. our code has been released at https://github.com/cuhk-aim-group/ gfbs.",AB_0342
"deep learning has achieved remarkable success in recent years; however, deep learning methods face significant challenges on long-tailed datasets, which are prevalent in real-world scenarios. in a long-tailed dataset, there are many more samples in the head classes than in the tail classes, and this class imbalance makes it difficult to learn a good feature representation for both head and tail classes simultaneously, particularly when using a single-stage method. although the existing two-stage methods can alleviate the problem of single-stage methods not performing well on the tail classes by classifier retraining in the second stage, this does not resolve the problem of insufficient learning of head and tail features. thus, in this paper, we propose a two-stage feature fusion network (ffn). the proposed ffn addresses this issue using one network for the head classes and another network for the tail classes, each of which is trained with a different loss function. this allows the feature learning module to effectively distinguish between the head and tail classes in the embedding space. the classifier learning module fuses the features obtained from the feature learning module, and the classifier is fine-tuned to classify the input images. different from traditional two-stage methods, the proposed utilizes different loss functions for the head and tail classes; thus, the classifier can achieve balanced results between the head and tail classes. we conduct extensive experiments on three benchmark datasets comparing the proposed ffn with six state-of-the-art methods including two baseline methods, the experimental results demonstrate that the ffn achieves significant improvement on all three benchmark datasets. the code is publicly available at https://github.com/zxsong999/feature-fusion-network.pytorch.",AB_0342
"because thermal infrared (tir) images are not affected by light and foggy environments, which are widely used in various night traffic scenarios. especially, thermal infrared images also play an important role in autonomous vehicles. however, low contrast and lack of chromaticity have always been their problems. image colorization is a vital technique to improve the quality of tir images, which is beneficial to human interpretation and downstream tasks. despite thermal infrared image colorization methods have been rapidly improved, the detail blurriness and color distortion in colorized images remain under-addressed. mostly because these methods cannot effectively extract the ambiguous feature information of tir images. hence, we propose a large kernel (lk) u-net and attention_u-net-transformer (vit-based) based generative adversarial network. an lk_u-net is designed to extract the feature of tir images. then, a branch structure composed of attention_u-net and vit-based can provide the network with semantic information from different perspectives to decode features. in addition, a composite loss function is employed to ensure the network generates a high-quality colorized image. the proposed method is evaluated on kaist and irvi datasets. experimental results demonstrate the superiority of the proposed lkat-gan over other methods for the task of thermal infrared image colorization. the code is available at https://github.com/jinxinhuo/lkat-gan.",AB_0342
"deep neural networks (dnns) are susceptible to adversarial samples that are carefully crafted to mislead the dnns with imperceptible perturbations. to test the robustness of dnns, attack methods based on adversarial samples have gained popularity due to their practicality and effectiveness in achieving encouraging attack results. however, most of these methods do not consider the more realistic multi-task attacks. the main challenge of multi-task attacks is that different tasks have different objective functions, making it difficult to find an optimal optimization goal to generate adversarial samples. to address this issue, we propose a new multi-task adversarial attack paradigm called multi-task adversarial attacks (mtaa) that uses a relationship preserving representation to learn adversarial patterns. unlike previous methods, our attack method does not rely on a task-specific loss function or an attack agent model. instead, we design a relationship-preserving module that projects samples into a low-dimensional embedding space while preserving their intrinsic geometric structure for adversarial pattern reasoning. this module effectively removes redundant information from high-dimensional features, providing an effective latent space for adversarial pattern reasoning. to learn adversarial representation in the latent space, we introduce a novel adversarial mechanism. our attack method can deceive different networks on multiple tasks since it is independent of task-specific loss functions and the attack agent. extensive experimental results show that our attack approach outperforms state-of-theart universal and transferable attack strategies on multi-task attacks. the codes for mtaa are available at https://github.com/antachen/mtaa.",AB_0342
"recently, cnn-based post-processing has shown great potential in synthesized view quality enhancement (svqe). however, due to the limited receptive field of convolution, it is ineffective in explicitly modeling long-range dependencies, which are critical to eliminate the distortion induced by depth image based rendering (dibr) in synthesized views. although transformers exhibit tremendous success at learning global contextual information, it is weak at extracting local texture information. to take full advantages of the cnn and transformer, we present a novel u-shaped hybrid transformer with asymmetric flow division to collaboratively capture global-local information for svqe, termed as afd-former. specifically, the afd-former utilizes the transformer-cnn block (tcb) as encoder and decoder, in which several dynamic hybrid attention blocks (dhabs) are designed to simultaneously model long-range interactions and retain texture details. then, considering that the deeper layers of the u-shaped network play more roles in capturing global information while shallow layers more in extracting local information, an asymmetric flow division unit (afdu) is embedded into each dhab to assign different contributions of global-local contextual information to the transformer and cnn branches across different layers. finally, a dynamic learnable modulator is incorporated into two branches to help model effectively feature representation learning. that can be viewed as the dynamic process of adjusting the weight for each channel of the input feature based on contextual cues. extensive experiments demonstrate that the proposed afd-former can significantly enhance perceptual quality of synthesized views with similar svqe speed compared with the related state-of-the-art svqe methods. the source code will be available at https://github.com/house-yuyu/afd-former.",AB_0342
"most anchor-free methods perform object detection using dense recommendation, which assumes that one point can simultaneously conduct accurate category prediction and regression estimation. however, due to different task drivers, valid features for classification and regression may locate at distinct areas in the training phase. this problem is called feature misalignment. to solve it, we propose a new feature alignment method based on anchor-free object detector. firstly, a global receptive field adaptor (g-rfa) is designed by incorporating the feature pyramid networks (fpn) with the global attention mechanism, and forward features are further fine-tuned with a deformable-subnet (de-subnet) to remove the influence of redundant contextual information. then, a new feature filter strategy with a misalignment score is proposed to guide the network to focus on sampling points with aligned features. in addition, we establish mutually independent multi-layer quality distributions to model the priori information of an object on different fpn levels. equipped with our method, the classification and regression features are aligned, and the generated foreground weight map converges to the centers of classification and regression heatmaps. experimental results show that without bells and whistles, our method achieves 49.3% ap on ms coco test-dev under the default 2x training schedule, outperforming related methods. besides, experiments on pascal voc demonstrate the generalization ability of our method. code is available at https://github.com/gfengg/featurealign.",AB_0342
"semantic communications can reduce the resource consumption by transmitting task-related semantic information extracted from source messages. however, when the source messages are utilized for various tasks, e.g., wireless sensing data for localization and activities detection, semantic communication technique is difficult to be implemented because of the increased processing complexity. in this paper, we propose the inverse semantic communications as a new paradigm. instead of extracting semantic information from messages, we aim to encode the task-related source messages into a hyper-source message for data transmission or storage. following this paradigm, we design an inverse semantic-aware wireless sensing framework with three algorithms for data sampling, reconfigurable intelligent surface (ris)-aided encoding, and self-supervised decoding, respectively. specifically, on the one hand, we propose a novel ris hardware design for encoding several signal spectrums into one metaspectrum. to select the task-related signal spectrums for achieving efficient encoding, a semantic hash sampling method is introduced. on the other hand, we propose a self-supervised learning method for decoding the metaspectrums to obtain the original signal spectrums. using the sensing data collected from real-world, we show that our framework can reduce the data volume by 95% compared to that before encoding, without affecting the accomplishment of sensing tasks. moreover, compared with the typically used uniform sampling scheme, the proposed semantic hash sampling scheme can achieve 67% lower mean squared error in recovering the sensing parameters. in addition, experiment results demonstrate that the amplitude response matrix of the ris enables the encryption of the sensing data. the code for this paper is available at https://github.com/hongyangdu/semsensing.",AB_0342
"in this paper, a novel and effective image quality assessment (iqa) algorithm based on frequency disparity for high dynamic range (hdr) images is proposed, termed as local-global frequency feature-based model (lgfm). motivated by the assumption that the human visual system (hvs) is highly adapted for extracting structural information and partial frequencies when perceiving the visual scene, the gabor and the butterworth filters are applied to the luminance component of the hdr image to extract the local and global frequency features, respectively. the similarity measurement and feature pooling strategy are sequentially performed on the frequency features to obtain the predicted single quality score. the experiments evaluated on four widely used benchmarks demonstrate that the proposed lgfm can provide a higher consistency with the subjective perception compared with the state-of-the-art hdr iqa methods. our code is available at: https://github.com/eezkni/lgfm.",AB_0342
