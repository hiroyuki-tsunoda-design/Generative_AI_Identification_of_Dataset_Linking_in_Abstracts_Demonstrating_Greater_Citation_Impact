AB,NO
"multi-object tracking (mot) has been steadily studied for video understanding in computer vision. however, existing mot frameworks usually employ straightforward appearance or motion models and may struggle in dynamic environments with similar appearance and complex motion. in this paper, we present a robust mot framework with local appearance and stable motion models to overcome these two hindrances. the framework incorporates object and local part detectors, a feature extractor, a keypoint extractor, and a data association method. for the data association, we utilize five types of similarity metrics and a cascaded matching strategy. the local appearance model is suggested to be used additionally with global appearance features of full bounding boxes to obtain discriminative features even for objects with a similar appearance. at the same time, the stable motion model considers the core of the body as the central point of the object and subdivides the body using a novel 12-tuple kalman state vector to analyze complex motion. as a result, our new tracker achieves state-of-the-art performance on the dancetrack test set, surpassing all other listed tracking systems in terms of both detection and tracking quality metrics, obtaining 61.3 hota, 82.3 deta, 45.8 assa, and 91.7 mota. the source code is available at https://github.com/jubi-hwang/robust-mot-with-local-appearance-and-stable-motion-models.",AB_0122
"semantic segmentation of airborne laser scanning (als) point clouds using deep learning is a hot research in remote sensing and photogrammetry. a current trend is to aggregate contextual features from different scales for boosting network generalization and diversity discrimination capabilities. one main challenge is how to achieve effective fusion with multi-scale information. in this letter, we propose a multilevel context feature fusion network (mcfn) for semantic segmentation of als point cloud based on an encoder-decoder structure. more specifically, we design the squeeze-expansion shared multilayer perceptron (se-mlp) module following kernel point convolution (kpconv) in the encoding stage, which can extend the receptive field of kpconv. to aggregate low-level features and highlevel representations, we establish channel self-attention between skip connections. in the decoding stage, we develop a crosslayer attention fusion (caf) module to generate additional discriminative channel features by fusing multiscale features at different upsampling layers. experiments on the isprs and lasdu datasets demonstrate the superiority of the proposed method. code: https://github.com/sc-shendazt/mcfn.",AB_0122
"the ultralow delay tracking system, which enables seamless actuation in visual feedback applications, draws increasing attention in the fields of robotics and factory automation (fa). tracking accuracy and rotational robustness are critical in robotics and fa applications. in the real-time scene, the change (e.g., object moving) continues in the real space when tracking processing is performed in the virtual (computing) space. however, existing research focuses primarily on the image-processing accuracy of still images, with few works paying attention to tracking errors introduced by the change that occurs during the processing period. however, making the algorithm robust to rotation comes with a complex model, preventing the whole system from reaching an ultralow delay. this research aims to develop a 1-ms rotation-robust lucas kanade (lk)-based tracking algorithm and processing architecture, focusing on minimizing the error between the virtual space's output and the true state in real space. to achieve the above target, this article proposes: 1) temporal prediction-based temporal iterative tracking (tit), which compensates for temporal change with temporal prediction, significantly reducing real-time processing error and 2) temporal prediction-based parallel motion estimation, which breaks the data dependency of translation and rotation estimations by temporal prediction, allowing translation and rotation estimation to be performed parallelly with low delay. the proposed methods are implemented as a practical system by integrating a high-speed camera and a field programmable gate array (fpga). algorithm evaluation shows that the proposed method outperforms the other five related methods in terms of real-time tracking performance. hardware evaluation shows that the designed rotation-robust lk-based tracking system supports sensing and processing a 1000-fps sequence with a delay of less than 1 ms/frame while costing resources less than 30%. a video demonstration is available at https://wcms.waseda.jp/em/6422257c9c402.",AB_0122
"deep convolutional neural networks (cnns) have improved remote sensing image analysis, but their high computational demands may limit their deployment on low-end devices with limited resources, such as intelligent satellites and unmanned aerial vehicles. considering the computation complexity, we propose a guided hybrid quantization with one-to-one self-teaching (ghost) framework. more concretely, we first design a structure called guided quantization self-distillation (gqsd), an innovative idea for realizing a lightweight model through the synergy of quantization and distillation. the training process of the quantization model is guided by its full-precision model, which is time-saving and cost-saving without preparing a huge pretrained model in advance. second, we put forward a hybrid quantization (hq) module that automatically acquires the optimal bit-width by imposing a threshold constraint on the distribution distance between the center point and samples in the weight search space, aiming to retain more shallow detail information that is advantageous for small object detection. third, to improve information transformation, we propose a one-to-one self-teaching (ost) module to give the student network the ability to self-judgment. a switch control machine (scm) builds a bridge between the student and teacher networks in the same location to help the teacher reduce wrong guidance and impart vital knowledge about objects without vast background information to the student. this distillation method allows a model to learn from itself and gain substantial improvement without any additional supervision. extensive experiments on a multimodal dataset (vedai) and single-modality datasets (dota, nwpu, and dior) show that object detection based on ghost outperforms the existing detectors. the tiny parameters (< 9.7 mb) and bit-operations (bops) (< 2158 g) compared with any remote sensing-based, lightweight, or distillation-based algorithms demonstrate the superiority in the lightweight design domain. our code and model will be released at https://github.com/icey-zhang/ghost.",AB_0122
"semantic segmentation of high-resolution aerial images is a challenging task on account of complex scene variations and large-scale differences. however, these two issues are inadequately addressed in general semantic segmentation methods. in this article, we propose a multiscale prototype contrast network (mpcnet) to improve the adaptive capability for different scenes and scales. specifically, a novel multiscale prototype transformer decoder (mptd) is designed to extract dynamic scene-specific prototypes as pixel classifiers by fusing information from feature maps and learnable class tokens. to exploit cross-scene context information and accommodate the large-scale difference in the aerial image, we build a multiscale prototype memory queue to store these multiscale prototypes during training. upon the multiscale prototype memory queue, a novel multiscale prototype contrastive loss is proposed to increase object feature discriminability across multiple scales, which brings better consistency of intermediate features and boosts the convergence of the network. extensive experimental results on three publicly available datasets demonstrate the effectiveness and efficiency of our mpcnet over other state-of-the-art methods. the code is available at https://github.com/qixiong-wang/mmsegmentation-mpcnet.",AB_0122
"storytelling is a remarkable human skill that plays a significant role in learning and experiencing everyday life. developing narratives is central to human mental health development, simultaneously encapsulating broad details such as psychology, morality and common sense. contemporary deep-learning algorithms require similar skills to be able to tell a story from a visual perspective. however, most algorithms function at a superficial or factual level, aligning descriptive text with images in a one-to-one manner without considering the temporal relation. stories are more expressive in style, language and content, involving imaginary concepts not explicit in the images. an ideal deep learning system should learn and develop cohesive, meaningful, and causal stories. unfortunately, most existing storytelling methods are trained and evaluated on a single dataset, i.e., the visual storytelling (vist) dataset. multiple datasets are essential to test the generalization ability of algorithms. we bridge the gap and present a new dataset for expressive and coherent story creation. we present the sequential storytelling image dataset (ssid, https://ieee-dataport.org/documents/sequential-storytelling-image-dataset-ssid) consisting of open-source video frames accompanied by story-like annotations. we provide four annotations (stories) for each set of five images. the image sets are collected manually from publicly available videos in three domains: documentaries, lifestyle, and movies, and then annotated manually using amazon mechanical turk. we perform a detailed analysis and benchmarking of the current vist dataset and our new ssid dataset and show that both datasets exhibit high variance within their multiple ground truth stories corresponding to the same image set. moreover, our dataset achieves lower mean average scores across all metrics, meaning that the ground truth stories of our dataset are more diverse. finally, we train and evaluate existing state-of-the-art rhetorical storytelling methods on both datasets and show that our dataset is more challenging and requires sophisticated techniques to accurately detect a significant variety of events.",AB_0122
"semantic communications have shown promising advancements by optimizing source and channel coding jointly. however, the dynamics of these systems remain understudied, limiting research and performance gains. inspired by the robustness of vision transformers (vits) in handling image nuisances, we propose a vit-based model for semantic communications. our approach achieves a peak signal-to-noise ratio (psnr) gain of +0.5 db over convolutional neural network variants. we introduce novel measures, average cosine similarity and fourier analysis, to analyze the inner workings of semantic communications and optimize the system's performance. we also validate our approach through a real wireless channel prototype using software-defined radio (sdr). to the best of our knowledge, this is the first investigation of the fundamental workings of a semantic communications system, accompanied by the pioneering hardware implementation. to facilitate reproducibility and encourage further research, we provide open-source code, including neural network implementations and labview codes for sdr-based wireless transmission systems (source codes available at https://bit.ly/semvit).",AB_0122
"recently, with the expansion of the video platform market, research has been actively conducted on temporal action localization (tal) for detecting actions in atypical videos. most learning methods for tal include full and weak supervision (weak supervision with only action classes) approaches. full supervision requires considerable time for labeling and weak supervision exhibits low localization performance owing to the lack of informative annotations. to solve this problem, point-level weak supervision using single-point timestamps within the temporal interval of action instances has been proposed, which demonstrates superior performance to weakly-supervised methods using only action classes of action instances. in this study, we proposed an improved point-level supervision mechanism that provides point-level annotations for each action and background instance. in addition, a widely used multiple instance learning (mil)-based framework was used to verify the proposed method, and pseudo-labels were used for action instance boundary learning. also, the background point loss was designed to leverage the added point-level annotations. the datasets used in the experiment were thumos14, gtea, beoid, and activitynet1.2, and improved results were obtained compared to existing point-level supervision. the code is available from https://github.com/sang9390/an-improved-point-level-supervision-method-for-tal.",AB_0122
"prediction of accurate wind speed is necessary for a variety of applications such as energy production, agriculture, climate modeling, and weather forecasting. various satellites orbiting the earth measure the wind speed, which is particularly useful as they provide measurements of wind speed over large areas and in remote locations that might be difficult to measure using other methods. however, satellite-based wind speed measurements have relatively low spatial resolution compared to other methods, such as ground-based radar. in this research, we develop windsr and a lightweight tiny-windsr to improve the resolution of satellite wind speed data by four times from the nasa's geos-5 nature run dataset. windsr has srresnet-based architecture consisting of several residual-in-residual dense blocks to compute features from low spatial resolution (28 km) wind speed for upscaling. we train windsr with more than 20,000 pairs of low-resolution (28 km) and corresponding high-resolution (7 km) wind speed data and evaluate its performance on the validation set consisting of 2,102 wind speed images. experimental results show that windsr outperforms classical upsampling algorithms, such as bicubic interpolation and lanczos interpolation by 17.89% and general-purpose super-resolution gans such as bsrgan and swinir by up to 11.35% on the rmse metric. the dataset developed in this research is publicly available at: https://github.com/sekilab/windsr_dataset.",AB_0122
"the speed of tracking-by-detection (tbd) greatly depends on the number of running a detector because the detection is the most expensive operation in tbd. in many practical cases, multi-object tracking (mot) can be, however, achieved based tracking-by-motion (tbm) only. this is a possible solution without much loss of mot accuracy when the variations of object cardinality and motions are not much within consecutive frames. therefore, the mot problem can be transformed to find the best tbd and tbm mechanism. to achieve it, we propose a novel decision coordinator for mot (decode-mot) which can determine the best tbd/tbm mechanism according to scene and tracking contexts. in specific, our decode-mot learns tracking and scene contextual similarities between frames. because the contextual similarities can vary significantly according to the used trackers and tracking scenes, we learn the decode-mot via self-supervision. the evaluation results on mot challenge datasets prove that our method can boost the tracking speed greatly while keeping the state-of-the-art mot accuracy. our code will be available at https://github.com/reussite-cv/decode-mot.",AB_0122
