AB,NO
"state-of-the-art semantic segmentation methods capture the relationship between pixels to facilitate contextual information exchange. advanced methods utilize fixed pathways for context exchange, lacking the flexibility to harness the most relevant context for each pixel. in this paper, we present configurable context pathways (ccps), a novel model for establishing pathways for augmenting contextual information. in contrast to previous pathway models, ccps are learned, leveraging configurable regions to form information flows between pairs of pixels. we propose tagnet to adaptively configure the regions, which span over the entire image space, driven by the relationships between the remote pixels. subsequently, the information flows along the pathways are updated gradually by the information provided by sequences of configurable regions, forming more powerful contextual information. we extensively evaluate the traveling, adaption, and gathering (tag) stages of our network on the public benchmarks, demonstrating that all of the stages successfully improve the segmentation accuracy and help to surpass the state-of-the-art results. the code package is available at: https://github.com/dilincv/tagnet.",AB_0374
"reflection removal has been discussed for more than decades. this paper aims to provide the analysis for different reflection properties and factors that influence image formation, an up-to-date taxonomy for existing methods, a benchmark dataset, and the unified benchmarking evaluations for state-of-the-art (especially learning-based) methods. specifically, this paper presents a single-image reflection removal plus dataset sir2+ with the new consideration for in-the-wild scenarios and glass with diverse color and unplanar shapes. we further perform quantitative and visual quality comparisons for state-of-the-art single-image reflection removal algorithms. open problems for improving reflection removal algorithms are discussed at the end. our dataset and follow-up update can be found at https://reflectionremoval.github.io/sir2data/.",AB_0374
"thanks to the advent of deep neural networks, recent years have witnessed rapid progress in person re-identification (re-id). deep-learning-based methods dominate the leadership of large-scale benchmarks, some of which even surpass the human-level performance. despite their impressive performance under the single-domain setup, current fully-supervised re-id models degrade significantly when transplanted to an unseen domain. according to the characteristics of the re-id task, such degradation is mainly attributed to the dramatic variation within the target domain and the severe shift between the source and target domain, which we call dual discrepancy in this paper. to achieve a model that generalizes well to the target domain, it is desirable to take such dual discrepancy into account. in terms of the former issue, a prevailing solution is to enforce consistency between nearest-neighbors in the embedding space. however, we find that the search of neighbors is highly biased in our case due to the discrepancy across cameras. for this reason, we equip the vanilla neighborhood invariance approach with a camera-aware learning scheme. as for the latter issue, we propose a novel cross-domain mixup scheme. it works in conjunction with virtual prototypes which are employed to handle the disjoint label space between the two domains. in this way, we can realize the smooth transfer by introducing the interpolation between the two domains as a transition state. extensive experiments on four public benchmarks demonstrate the superiority of our method. without any auxiliary models and offline clustering procedure, it achieves competitive performance against existing state-of-the-art methods. the code is available at https://github.com/luckydc/generalizing-reid-improved.",AB_0374
"data augmentation is a critical technique in object detection, especially the augmentations targeting at scale invariance training (scale-aware augmentation). however, there has been little systematic investigation of how to design scale-aware data augmentation for object detection. we propose scale-aware autoaug to learn data augmentation policies for object detection. we define a new scale-aware search space, where both image- and instance-level augmentations are designed for maintaining scale robust feature learning. upon this search space, we propose a new search metric, termed pareto scale balance, to facilitate efficient augmentation policy search. in experiments, scale-aware autoaug yields significant and consistent improvement on various object detectors (e.g., retinanet, faster r-cnn, mask r-cnn, and fcos), even compared with strong multi-scale training baselines. our searched augmentation policies are generalized well to other datasets and instance-level tasks beyond object detection, e.g., instance segmentation. the search cost is much less than previous automated augmentation approaches for object detection, i.e., 8 gpus across 2.5 days versus. 800 tpu-days. in addition, meaningful patterns can be summarized from our searched policies, which intuitively provide valuable knowledge for hand-crafted data augmentation design. based on the searched scale-aware augmentation policies, we further introduce a dynamic training paradigm to adaptively determine specific augmentation policy usage during training. the dynamic paradigm consists of an heuristic manner for image-level augmentations and a differentiable copy-paste-based method for instance-level augmentations. the dynamic paradigm achieves further performance improvements to scale-aware autoaug without any additional burden on the long tailed lvis benchmarks. we also demonstrate its ability to prevent over-fitting for large models, e.g., the swin transformer large model. code and models are available at https://github.com/dvlab-research/sa-autoaug.",AB_0374
"deep product quantization networks (dpqns) have been successfully used in image retrieval tasks, due to their powerful feature extraction ability and high efficiency of encoding high-dimensional visual features. recent studies show that deep neural networks (dnns) are vulnerable to input with small and maliciously designed perturbations (a.k.a., adversarial examples) for classification. however, little effort has been devoted to investigating how adversarial examples affect dpqns, which raises the potential safety hazard when deploying dpqns in a commercial search engine. to this end, we propose an adversarial example generation framework by generating adversarial query images for dpqn-based retrieval systems. unlike the adversarial generation for the classic image classification task that heavily relies on ground-truth labels, we alternatively perturb the probability distribution of centroids assignments for a clean query, then we can induce effective non-targeted attacks on dpqns in white-box and black-box settings. moreover, we further extend the non-targeted attack to a targeted attack by a novel sample space averaging scheme (s(2)as), whose theoretical guarantee is also obtained. extensive experiments show that our methods can create adversarial examples to successfully mislead the target dpqns. besides, we found that our methods both significantly degrade the retrieval performance under a wide variety of experimental settings. the source code is available at https://github.com/kira0096/pqag.",AB_0374
"we propose to restore old photos that suffer from severe degradation through a deep learning approach. unlike conventional restoration tasks that can be solved through supervised learning, the degradation in real photos is complex and the domain gap between synthetic images and real old photos makes the network fail to generalize. therefore, we propose a novel triplet domain translation network by leveraging real photos along with massive synthetic image pairs. specifically, we train two variational autoencoders (vaes) to respectively transform old photos and clean photos into two latent spaces. and the translation between these two latent spaces is learned with synthetic paired data. this translation generalizes well to real photos because the domain gap is closed in the compact latent space. besides, to address multiple degradations mixed in one old photo, we design a global branch with a partial nonlocal block targeting the structured defects, such as scratches and dust spots, and a local branch targeting the unstructured defects, such as noises and blurriness. we also extend the global branch with a more memory-efficient scheme, named multi-scale patch-based attention to processing high-resolution photos. two branches are fused in the latent space, leading to improved capability to restore old photos from multiple defects. furthermore, we apply another face refinement network to recover fine details of faces in the old photos, thus ultimately generating photos with enhanced perceptual quality. with comprehensive experiments, the proposed pipeline demonstrates superior performance over state-of-the-art methods as well as existing commercial tools in terms of visual quality for old photos restoration. both code and models could be found at https://github.com/microsoft/bringing-old-photos-back-to-life.",AB_0374
"modeling statistics of image priors is useful for image super-resolution, but little attention has been paid from the massive works of deep learning-based methods. in this work, we propose a bayesian image restoration framework, where natural image statistics are modeled with the combination of smoothness and sparsity priors. concretely, first we consider an ideal image as the sum of a smoothness component and a sparsity residual, and model real image degradation including blurring, downscaling, and noise corruption. then, we develop a variational bayesian approach to infer their posteriors. finally, we implement the variational approach for single image super-resolution (sisr) using deep neural networks, and propose an unsupervised training strategy. the experiments on three image restoration tasks, i.e., ideal sisr, realistic sisr, and real-world sisr, demonstrate that our method has superior model generalizability against varying noise levels and degradation kernels and is effective in unsupervised sisr. the code and resulting models are released via https://zmiclab.github.io/projects.html.",AB_0374
"this paper proposes attribute-decomposed gan (adgan) and its enhanced version (adgan++) for controllable image synthesis, which can produce realistic images with desired attributes provided in various source inputs. the core ideas of the proposed adgan and adgan++ are both to embed component attributes into the latent space as independent codes and thus achieve flexible and continuous control of attributes via mixing and interpolation operations in explicit style representations. the major difference between them is that adgan processes all component attributes simultaneously while adgan++ utilizes a serial encoding strategy. more specifically, adgan consists of two encoding pathways with style block connections and is capable of decomposing the original hard mapping into multiple more accessible subtasks. in the source pathway, component layouts are extracted via a semantic parser and the segmented components are fed into a shared global texture encoder to obtain decomposed latent codes. this strategy allows for the synthesis of more realistic output images and the automatic separation of un-annotated component attributes. although the original adgan works in a delicate and efficient manner, intrinsically it fails to handle the semantic image synthesizing task when the number of attribute categories is huge. to address this problem, adgan++ employs the serial encoding of different component attributes to synthesize each part of the target real-world image, and adopts several residual blocks with segmentation guided instance normalization to assemble the synthesized component images and refine the original synthesis result. the two-stage adgan++ is designed to alleviate the massive computational costs required when synthesizing real-world images with numerous attributes while maintaining the disentanglement of different attributes to enable flexible control of arbitrary component attributes of the synthesized images. experimental results demonstrate the proposed methods' superiority over the state of the art in pose transfer, face style transfer, and semantic image synthesis, as well as their effectiveness in the task of component attribute transfer. our code and data are publicly available at https://github.com/menyifang/adgan.",AB_0374
"one essential problem in skeleton-based action recognition is how to extract discriminative features over all skeleton joints. however, the complexity of the recent state-of-the-art (sota) models for this task tends to be exceedingly sophisticated and over-parameterized. the low efficiency in model training and inference has increased the validation costs of model architectures in large-scale datasets. to address the above issue, recent advanced separable convolutional layers are embedded into an early fused multiple input branches (mib) network, constructing an efficient graph convolutional network (gcn) baseline for skeleton-based action recognition. in addition, based on such the baseline, we design a compound scaling strategy to expand the model's width and depth synchronously, and eventually obtain a family of efficient gcn baselines with high accuracies and small amounts of trainable parameters, termed efficientgcn-bx, where x denotes the scaling coefficient. on two large-scale datasets, i.e., ntu rgb+d 60 and 120, the proposed efficientgcn-b4 baseline outperforms other sota methods, e.g., achieving 92.1% accuracy on the cross-subject benchmark of ntu 60 dataset, while being 5.82x smaller and 5.85x faster than ms-g3d, which is one of the sota methods. the source code in pytorch version and the pretrained models are available at https://github.com/yfsong0709/efficientgcnv1.",AB_0374
"the difficulties in both data acquisition and annotation substantially restrict the sample sizes of training datasets for 3d medical imaging applications. therefore, it is non-trivial to build well-performing 3d con-volutional neural networks from scratch. previous efforts on 3d pre-training have frequently relied on self-supervised approaches, which use either predictive or contrastive learning on unlabeled data to build invariant 3d representations. however, because of the unavailability of large-scale supervision informa-tion, obtaining semantically invariant and discriminative representations from these learning frame-works remains problematic. in this paper, we revisit an innovative yet simple fully-supervised 3d network pre-training framework to take advantage of semantic supervision from large-scale 2d natural image datasets. with a redesigned 3d network architecture, reformulated natural images are used to address the problem of data scarcity and develop powerful 3d representations. comprehensive experi-ments on five benchmark datasets demonstrate that the proposed pre-trained models can effectively accelerate convergence while also improving accuracy for a variety of 3d medical imaging tasks such as classification, segmentation, and detection. in addition, as compared to training from scratch, it can save up to 60% of annotation efforts. on the nih deeplesion dataset, it also achieves state-of-the-art detection performance, outperforming earlier self-supervised and fully-supervised pre-training approaches, as well as methods that do training from scratch. to facilitate further development of 3d medical models, our code and pre-trained model weights are publicly available at https://github.com/u rmagicsmine/cspr. (c) 2023 elsevier b.v. all rights reserved.",AB_0374
