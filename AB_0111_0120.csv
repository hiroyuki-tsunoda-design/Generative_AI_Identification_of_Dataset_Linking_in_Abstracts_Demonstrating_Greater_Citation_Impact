AB,NO
"noninvasive fetal electrocardiogram (ni-fecg) plays a significant role in fetal diagnosis. however, it is challenging to estimate fecg signals from the abdominal ecg due to the following issues: (1) the fecg signals are always masked by the maternal ecg (mecg) signals; (2) the fecg waveform is often corrupted by the noise. to solve such problems, a canonical-structured graph sparse attention network is proposed for fetal ecg estimation, where the canonical spatial graph sparse attention module is designed to estimate fecg signals masked by the mecg signals by learning its waveform features, and the canonical channel graph sparse attention is devised to discriminate the characteristic waveforms from noise by capturing the fecg signal details. experiments conducted on the two databases demonstrate the proposed csgas-net outperforms the state-of-the-art deep learning methods. the project is available at https://github.com/langdecc511/csgsa-net.",AB_0012
"the visual quality of digitized whole slide images (wsis) for kidney tissue not only affects the diagnosis and subsequent treatment, but also has a decisive impact on the accuracy of multiclass segmentation, classification and object detection during computer intelligent analysis. currently, pathologists usually assess image quality through eye screening, which greatly relies on the pathologist experience and brings about subjectivity and non-repeatability issues. in this paper, we develop a no-reference image quality assessment framework including a fused cnn classification module, a quality score conversion module and a comprehensive quality prediction module, which automatically classifies wsis of kidney tissue into four quality levels: excellent, good, average, and poor, and calculates a rough quality score. the original image and the regions of interest are combined and fused to comprehensively evaluate the quality of a wsi through multiple factors instead of a simple deep learning network. extensive experiments conducted on our in-house dataset confirm that our proposed framework obtains satisfactory results with an accuracy of 90.05%, surpassing the performance of the typical image quality assessment models, and achieves the level of junior pathologist. therefore, our proposed method can be embedded into a computer assisted diagnosis system to help pathologists in analysis of histopathological images and judgment of reliability of the images. the source code and trained models will be available at https://github. com/kidneypathology/wsiqa.",AB_0012
"multi-modal remote sensing image (mrsi) has nonlinear radiation distortion (nrd) and significant contrast differences to which image gradient features are usually sensitive. although image phase features are more robust against nrd, they might not be much helpful in resolving the problems of directional inversion or phase extreme value mutations that are common in the phase feature calculation. to address these issues, a new mrsi matching method-histogram of the orientation of weighted phase (howp)-is proposed in this paper. this method distinguishes itself from other methods in three aspects: (1) a feature aggregation strategy is used to optimize feature points by extracting the corner and blob features separately; (2) a novel weighted phase orientation model is established to replace the traditional image gradient orientation features; and (3) a regularization-based log-polar descriptor is constructed to generate robust feature description vectors. to eval-uate the performance of the proposed method, we selected 50 sets of typical mrsis with translation, scale, and rotation differences for comparison with the other four state-of-the-art methods. the results show that our method is more resistant to radiometric distortion and the contrasting differences in mrsis. it also performs better in tackling the problems of direction reversal and phase extreme value mutation, as evidenced by more, the number of correct matches (ncm). since the method has improved the average ncm by 1.6-4.5 times, the average success rate by 35.5%, and the average rate of correct matches by 11.1% with an average root of mean-squared error of 1.93 pixels. moreover, we have put forward an extended version of the howp method (simplified-howp) when there is no image rotation, which manifests in an average 0.75 times improvement in ncm of simplified-howp performance over that of the howp method. the executable code and test data are linked in https://skyearth.org/publication/project/howp/.",AB_0012
"in this paper, we propose a general deep learning based framework, named sat-mvsf, to perform threedimensional (3d) reconstruction of the earth's surface from multi-view optical satellite images. the framework is a complete processing pipeline, including pre-processing, a multi-view stereo (mvs) network for satellite imagery (sat-mvsnet), and post-processing. the pre-processing handles the geometric and radiometric configuration of the multi-view images and their cropping. the cropped multi-view patches are then fed into satmvsnet, which includes deep feature extraction, rational polynomial camera (rpc) warping, pyramid cost volume construction, regularization, and regression, to obtain the height maps. the error matches are then filtered out and a digital surface model (dsm) is generated in the post-processing. considering the complexity and diversity of real-world scenes, we also introduce a self-refinement strategy that does not require any groundtruth labels to enhance the performance and robustness of the sat-mvsf framework. we comprehensively compare the proposed framework with popular commercial software and open-source methods, to demonstrate the potential of the proposed deep learning framework. on the whu-tlc dataset, where the images are captured with a three-line camera (tlc), the proposed framework outperforms all the other solutions in terms of reconstruction fineness, and also outperforms most of the other methods in terms of efficiency. on the challenging mvs3d dataset, where the images are captured by the worldview-3 satellite at different times and seasons, the proposed framework also exceeds the existing methods when using the model pretrained on aerial images and the introduced self-refinement strategy, demonstrating a high generalization ability. we also note that the lack of training samples hinders research in this field, and the availability of more high-quality open-source training data will greatly accelerate the research into deep learning based mvs satellite image reconstruction. the code will be available at https://gpcv.whu.edu.cn/data.",AB_0012
"understanding prognosis and mortality is critical for evaluating the treatment plan of patients. advances in digital pathology and deep learning techniques have made it practical to perform survival analysis in whole slide images (wsis). current methods are usually based on a multi-stage framework which includes patch sampling, feature extraction and prediction. however, the random patch sampling strategy is highly unstable and prone to sampling non-roi. feature extraction typically relies on hand-crafted features or convolutional neural networks (cnns) pre-trained on imagenet, while the artificial error or domain gaps may affect the survival prediction performance. besides, the limited information representation of local sampling patches will create a bottleneck limitation on the effectiveness of prediction. to address the above challenges, we propose a novel patch sampling strategy based on image information entropy and construct a multi-scale feature fusion network (msfn) based on self-supervised feature extractor. specifically, we adopt image information entropy as a criterion to select representative sampling patches, thereby avoiding the noise interference caused by random to blank regions. meanwhile, we pretrain the feature extractor utilizing self -supervised learning mechanism to improve the efficiency of feature extraction. furthermore, a global-local feature fusion prediction network based on the attention mechanism is constructed to improve the survival prediction effect of wsis with comprehensive multi-scale information representation. the proposed method is validated by adequate experiments and achieves competitive results on both of the most popular wsis survival analysis datasets, tcga-gbm and tcga-lusc. code and trained models are made available at: https://github.com/mercuriiio/msfn.",AB_0012
"brain tissue segmentation is of great value in diagnosing brain disorders. three-dimensional (3d) and two-dimensional (2d) segmentation methods for brain magnetic resonance imaging (mri) suffer from high time complexity and low segmentation accuracy, respectively. to address these two issues, we propose a context -assisted full attention network (can) for brain mri segmentation by integrating 2d and 3d data of mri. different from the fully symmetric structure u-net, the can takes the current 2d slice, its 3d contextual skull slices and 3d contextual brain slices as the input, which are further encoded by the densenet and decoded by our constructed full attention network. we have validated the effectiveness of the can on our collected dataset pwml and two public datasets dhcp2017 and malc2012. our code is available at https://github.com/nwuai/can.",AB_0012
"hyperspectral image (hsi) provides rich spectral-spatial information and the light detection and ranging (lidar) data reflect the elevation information, which can be jointly exploited for better land-cover clas-sification. however, due to different imaging mechanisms, hsi and lidar data always present significant image difference, current pixel-wise feature fusion classification methods relying on concatenation or weighted fusion are not effective. to achieve accurate classification result, it is important to extract and fuse similar high-order semantic information and complementary discriminative information contained in multimodal data. in this paper, we propose a novel coupled adversarial learning based classification (calc) method for fusion classification of hsi and lidar data. in specific, a coupled adversarial feature learning (cafl) sub-network is first trained, to effectively learn the high-order semantic features from hsi and lidar data in an unsupervised manner. on one hand, the proposed cafl sub-network establishes an adversarial game between dual generators and discriminators, so that the learnt features can preserve detail information in hsi and lidar data, respectively. on the other hand, by designing weight-sharing and linear fusion structure in the dual generators, we can simultaneously extract similar high-order semantic information and modal-specific complementary information. meanwhile, a supervised multi-level feature fusion classification (mffc) sub-network is trained, to further improve the classification performance via adaptive probability fusion strategy. in brief, the low-level, mid-level and high-level features learnt by the cafl sub-network lead to multiple class estimation probabilities, which are then adaptively combined to generate a final accurate classification result. both the cafl and mffc sub-networks are collaboratively trained by optimizing a designed joint loss function, which consists of unsupervised adversarial loss and supervised classification loss. overall, by optimizing the joint loss function, the proposed calc network is pushed to learn highly discriminative fusion features from multimodal data, leading to higher classification accuracies. extensive experiments on three well-known hsi and lidar data sets demonstrate the superior classification performance by the proposed calc method than several state-of-the-art methods. the source code of the proposed method will be made publicly available at https://github.com/ding-kexin/calc.",AB_0012
"automatic segmentation of pelvic functional bone marrow (fbm) in computed tomography (ct) plays a critical role in the accurate diagnosis of pelvic malignancies and treatment planning for patients. pelvic bone marrow near the mid-axis of the body, i.e., the fbm, is the most hematopoietic. here we propose an attention-guided, combined multi-scale, transformer reasoning-based network (acmtr) for accurate segmentation of pelvic fbm for intensity-modulated radiation therapy. the presented convolution-attention fusion encoder overcomes the limited receptive field of convolution as well as the class imbalance issue in pelvic fbm segmentation. we also propose a multi-scale transformer global reasoning bottleneck module for obtaining multi-scale, global attention features for extraction of small edge tissue of the fbm. we introduce a deformable attention mechanism at the decoding path to focus on key features instead of redundant information with a minimal increase in computational complexity. the pelvic fbm ct images used in this study were from a self-constructed clinical database of 92 cases and showed two main areas of fbm: the most intense fbm area (i.e., most hematopoietic area) and the second most intense fbm area. the proposed acmtr was compared with the popular deep learning models u-net, vgg-net, attention-unet, transunet, cotr, nnunet, and nnformer. for the most intense and second most intense fbm areas, the proposed acmtr achieved dice scores of 0.799 and 0.772 and haus-dorff distance of 6.061 and 4.791, respectively, on test data. the implementation code is available at the following github link: https://github.com/walynlee/acmtr.",AB_0012
"background and objective: fundus fluorescein angiography (ffa) is widely used in clinical ophthalmic diagnosis and treatment with the requirement of adverse fluorescent dyes injection. recently, many deep convolutional neural network(cnn)-based methods have been proposed to estimate ffa from color fundus (cf) images to eliminate the use of adverse fluorescent dyes. however, the robustness of these methods is affected by pathological changes.method: in this work, we present a cnn-based approach, lesion-aware generative adversarial networks (la-gan), to enhance the visual effect of lesion characteristics in the generated ffa images. first, we lead the generator notice lesion information by joint learning with lesion region segmentation. a new hierarchical correlation multi-task framework for high-resolution images is designed. second, to enhance the visual contrast between normal regions and lesion regions, a newly designed region-level adversarial loss is used rather than the image-level adversarial loss. the code is publicly available at: https://github.com/nicetomeetu21/la-gan.results: the effectiveness of la-net has been verified in data with branch retinal vein occlusion. the proposed model reported as measures of generation performance a mean structural similarity (ssim) of 0.536, mean learned perceptual image patch similarity (lpips) 0.312, outperforming other ffa generation and general image generation methods. further, due to the proposed multi-task learning framework, the lesion-region segmentation performance was further reported as the mean dice increased from 0.714 to 0.797 and the mean accuracy increased from 0.873 to 0.905, outperforming general single-task image segmentation methods.conclusions: the results show that the visual effect of lesion characteristics can be improved by employing the region-level adversarial loss and the hierarchical correlation multi-task framework respectively. based on the results of comparison with the state-of-the-art methods, la-gan is not only effective for cf-to-ffa translation, but also effective for lesion-region segmentation. thus, it may be used for various image translation and lesion segmentation tasks in future research. (c) 2022 elsevier b.v. all rights reserved.",AB_0012
"coronavirus disease (covid-19) has caused a worldwide pandemic, putting millions of people's health and lives in jeopardy. detecting infected patients early on chest computed tomography (ct) is critical in combating covid-19. harnessing uncertainty-aware consensus-assisted multiple instance learning (uc-mil), we propose to diagnose covid-19 using a new bilateral adaptive graph-based (ba-gcn) model that can use both 2d and 3d discriminative information in 3d ct volumes with arbitrary number of slices. given the importance of lung segmentation for this task, we have created the largest manual annotation dataset so far with 7,768 slices from covid-19 patients, and have used it to train a 2d segmentation model to segment the lungs from individual slices and mask the lungs as the regions of interest for the subsequent analyses. we then used the uc-mil model to estimate the uncertainty of each prediction and the consensus between multiple predictions on each ct slice to automatically select a fixed number of ct slices with reliable predictions for the subsequent model reasoning. finally, we adaptively constructed a ba-gcn with vertices from different granularity levels (2d and 3d) to aggregate multi-level features for the final diagnosis with the benefits of the graph convolution network's superiority to tackle cross-granularity relationships. experimental results on three largest covid-19 ct datasets demonstrated that our model can produce reliable and accurate covid-19 predictions using ct volumes with any number of slices, which outperforms existing approaches in terms of learning and generalisation ability. to promote reproducible research, we have made the datasets, including the manual annotations and cleaned ct dataset, as well as the implementation code, available at https://doi.org/10.5281/zenodo.6361963.",AB_0012
