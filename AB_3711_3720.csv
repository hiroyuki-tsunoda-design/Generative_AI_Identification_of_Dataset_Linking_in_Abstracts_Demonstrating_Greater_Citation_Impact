AB,NO
"we propose a new task called sentimental visual captioning that generates captions with the inherent sentiment reflected by the input image or video. compared with the stylized visual captioning task that requires a predefined style independent of the image or video, our new task automatically analyzes the inherent sentiment tendency from the visual content. with this in mind, we propose a multimodal transformer model namely senti-transformer for sentimental visual captioning, which integrates both content and sentiment information from multiple modalities and incorporates prior sentimental knowledge to generate sentimental sentence. specifically, we extract prior knowledge from sentimental corpus to obtain sentimental textual information and design a multi-head transformer encoder to encode multimodal features. then we decompose the attention layer in the middle of transformer decoder to focus on important features of each modality, and the attended features are integrated through an intra-and inter-modality fusion mechanism for generating sentimental sentences. to effectively train the proposed model using the external sentimental corpus as well as the paired images or videos and factual sentences in existing captioning datasets, we propose a two-stage training strategy that first learns to incorporate sentimental elements into the sentences via a regularization term and then learns to generate fluent and relevant sentences with the inherent sentimental styles via reinforcement learning with a sentimental reward. extensive experiments on both image and video datasets demonstrate the effectiveness and superiority of our senti-transformer on sentimental visual captioning. source code is available at https:// github.com/ezeli/insenticap_ext.",AB_0372
"taking into account the limitations of optical imaging, image acquisition equipment is usually designed to make a trade-off between spatial information and spectral information. hyperspectral image(hsi) can finely identify and classify imaging objects owing to its rich spectral information, while multi -spectral image(msi) can provide fine geometric features because of its sufficient spatial information. hence, fusing hsi and msi to achieve information complementarity has become a prevalent manner, which increases the reliability and accuracy of the information obtained. however, unlike traditional optical multi-focus image fusion and pan-sharpening of msi, existing hsi and msi fusion methods still face with problems in achieving cross-modality information interaction and lack effective utilization of spatial location information. to solve the above problems and to achieve more effective information integration between hsi and msi, this paper proposes a novel multi-hierarchical cross transformer for hyperspectral and multispectral image fusion (mct-net). the proposed mct-net consists of two components: (1) a multi-hierarchical cross-modality interacting module (mcim), which first extracts the deep multi-scale features of hsi and msi, and then performs cross-modality information interaction between them at identical scales by applying a multi-hierarchical cross transformer (mct), to reconstruct the spectral information lacking in msi and the spatial information lacking in hsi; (2) a feature aggregation reconstruction module (farm) which combines features from mcim, uses strip convolution to further restore edge features, and reconstructs the fusion results through cascaded upsampling. we conduct comparative experiments on five mainstream hsi datasets to prove the effectiveness and superiority of the proposed method, including the pavia center, pavia university, urban, botswana, and washington dc mall. for instance, on the washington dc mall dataset, compared with the state-of-the-art(sota) method in the comparison algorithms, our method improves psnr by 18.52% and reduces rmse, ergas and sam by 56.63%, 56.90% and 58.58%, respectively. the source code for mct-net can be downloaded from https://github.com/wxy11-27/mct-net. (c) 2023 elsevier b.v. all rights reserved.",AB_0372
"monocular 3d object detection aims to localize objects in 3d space from a single image. this is a difficult problem due to the lack of accurate depth measurements. many methods predict depths upfront using a pre-trained vision-based depth estimator to assist in 3d object detection. however, the methods show limited improved performances because of the depth inaccuracy and the neglect of depth confidence. in this paper, we propose a new end-to-end 3d object detection framework by combining a monocular camera with a cheap 4-beam lidar. the minimal lidar signal is leveraged as an additional input to pre-dict high-quality and dense depth maps from monocular images. meanwhile, several 3d proposals are generated by a keypoint-based detector. the key challenge is encoding the depth confidence to capture the depth estimation uncertainty. therefore, we propose probabilistic instance shape reconstruction to exploit instance shape information for box refinement. our method employs a fully differentiable end -to-end framework, making it simple and efficient. the experimental results on the kitti dataset demon-strate that the proposed method achieves state-of-the-art performance, thus validating the effectiveness of the sparse lidar data and the probabilistic instance shape reconstruction. code is available at https:// github.com/xjtuwh/sparselidar_fusion.(c) 2023 published by elsevier b.v.",AB_0372
"deep learning techniques have achieved impressive progress in the task of person re-identification. how-ever, how to generalize a learned model from the source domain to the target domain remains a long-standing challenge. inspired by the fact that the enrichment of data diversity and the utilization of mis-cellaneous semantic features can lead to better generalization ability, we design a model that integrates a novel data augmentation method with a multi-label assignment strategy to achieve semantic features decoupling in the source domain. the pre-trained model is employed to extract several kinds of semantic features from the target dataset, and each kind of semantic features is regarded as a specific domain. we then cluster features of each domain and exploit the connection between different clustering results to perform self-distillation for generating more reliable pseudo labels. finally, the obtained pseudo la-bels are used to fine-tune the pre-trained model to achieve model transfer from the source domain to the target one. extensive experiments demonstrate that our approach outperforms some state-of-the-art methods by a clear margin and even surpass some supervised methods. source code is available at: https://www.github.com/flychen321/mdjl .(c) 2023 elsevier ltd. all rights reserved.",AB_0372
"aspect based sentiment analysis(absa) aims to identify aspect terms in online reviews and predict their corresponding sentiment polarity. sentiment analysis poses a challenging fine-grained task. two typical subtasks are involved: aspect term extraction (ate) and aspect polarity classification (apc). these two subtasks are usually trained discretely, which ignores the connection between ate and apc. concretely, we can relate ate to apc through aspects and train them concurrently. we mainly use the ate task as an auxiliary task, allowing the apc to focus more on relevant aspects to facilitate aspect polarity classification. in addition, previous studies have shown that utilizing dependency syntax information with a graph neural network (gnn) also contributes to the performance of the apc task. however, most studies directly input sentence dependency relations into graph neural networks without considering the influence of aspects, which do not emphasize the important dependency relationships. to address these issues, we propose a multitask learning model combining apc and ate tasks that can extract aspect terms as well as classify aspect polarity simultaneously. moreover, we exploit multihead attention(mha) to associate dependency sequences with aspect extraction, which not only combines both ate and apc tasks but also stresses the significant dependency relations, enabling the model to focus more on words closely related to aspects. according to our experiments on three benchmark datasets, we demonstrate that the connection between ate and apc can be better established by our model, which enhances aspect polarity classification performance significantly. the source code has been released on github https://github.com/winder-source/mtabsa. (c) 2023 elsevier b.v. all rights reserved.",AB_0372
"neural architecture search (nas) has demonstrated state-of-the-art performance on various computer vision tasks. despite the superior performance achieved, the efficiency and generality of existing methods are highly valued due to their high computational complexity and low generality. in this paper, we propose an efficient and unified nas framework termed ddpnas via dynamic distribution pruning, facilitating a theoretical bound on accuracy and efficiency. in particular, we first sample architectures from a joint categorical distribution. then the search space is dynamically pruned and its distribution is updated every few epochs. with the proposed efficient network generation method, we directly obtain the optimal neural architectures on given constraints, which is practical for on-device models across diverse search spaces and constraints. the architectures searched by our method achieve remarkable top-1 accuracies, 97.56 and 77.2 on cifar-10 and imagenet (mobile settings), respectively, with the fastest search process, i.e., only 1.8 gpu hours on a tesla v100. codes for searching and network generation are available at: https://openi.pcl.ac.cn/pcl_automl/xnas.",AB_0372
"although existing machine reading comprehension models are making rapid progress on many datasets, they are far from robust. in this article, we propose an understanding-oriented machine reading comprehension model to address three kinds of robustness issues, which are over-sensitivity, over-stability, and generalization. specifically, we first use a natural language inference module to help the model understand the accurate semantic meanings of input questions to address the issues of over-sensitivity and over-stability. then, in the machine reading comprehension module, we propose a memory-guided multi-head attention method that can further well understand the semantic meanings of input questions and passages. third, we propose a multi-language learning mechanism to address the issue of generalization. finally, these modules are integrated with a multi-task learning-based method. we evaluate our model on three benchmark datasets that are designed to measure models' robustness, including dureader (robust) and two squad-related datasets. extensive experiments show that our model can well address the mentioned three kinds of robustness issues. and it achieves much better results than the compared state-of-the-art models on all these datasets under different evaluation metrics, even under some extreme and unfair evaluations. the source code of our work is available at https://github.com/neukg/robustmrc.",AB_0372
"a common challenge posed to robust semantic segmentation is the expensive data annotation cost. existing semi-supervised solutions show great potential for solving this problem. their key idea is constructing consistency regularization with unsupervised data augmentation from unlabeled data for model training. the perturbations for unlabeled data enable the consistency training loss, which benefits semi-supervised semantic segmentation. however, these perturbations destroy image context and introduce unnatural boundaries, which is harmful for semantic segmentation. besides, the widely adopted semi-supervised learning framework, i.e. mean-teacher, suffers performance limitation since the student model finally converges to the teacher model. in this paper, first of all, we propose a context friendly differentiable geometric warping to conduct unsupervised data augmentation; secondly, a novel adversarial dual-student framework is proposed to improve the mean-teacher from the following two aspects: (1) dual student models are learned independently except for a stabilization constraint to encourage exploiting model diversities; (2) adversarial training scheme is applied to both students and the discriminators are resorted to distinguish reliable pseudo-label of unlabeled data for self-training. effectiveness is validated via extensive experiments on pascal voc2012 and cityscapes. our solution significantly improves the performance and state-of-the-art results are achieved on both datasets. remarkably, compared with fully supervision, our solution achieves comparable miou of 73.4% using only 12.5% annotated data on pascal voc2012. our codes and models are available at https://github.com/cao-cong/ads-semiseg.",AB_0372
"multi-scale architectures and attention modules have shown effectiveness in many deep learning-based image de-raining methods. however, manually designing and integrating these two components into a neural network requires a bulk of labor and extensive expertise. in this article, a high-performance multi-scale attentive neural architecture search (manas) framework is technically developed for image de-raining. the proposed method formulates a new multi-scale attention search space with multiple flexible modules that are favorite to the image de-raining task. under the search space, multi-scale attentive cells are built, which are further used to construct a powerful image de-raining network. the internal multi-scale attentive architecture of the de-raining network is searched automatically through a gradient-based search algorithm, which avoids the daunting procedure of the manual design to some extent. moreover, in order to obtain a robust image de-raining model, a practical and effective multi-to- one training strategy is also presented to allow the de-raining network to get sufficient background information from multiple rainy images with the same background scene, and meanwhile, multiple loss functions including external loss, internal loss, architecture regularization loss, and model complexity loss are jointly optimized to achieve robust de-raining performance and controllable model complexity. extensive experimental results on both synthetic and realistic rainy images, as well as the down-stream vision applications (i.e., objection detection and segmentation) consistently demonstrate the superiority of our proposed method. the code is publicly available at https://github.com/lcai-gz/manas.",AB_0372
"although the current generative adversarial networks (gan)-generated face forensic detectors based on deep neural networks (dnns) have achieved considerable performance, they are vulnerable to adversarial attacks. in this paper, an effective local perturbation generation method is proposed to expose the vulnerability of state-of-the-art forensic detectors. the main idea is to mine the fake faces' areas of common concern in multiple-detectors' decision-making, then generate local anti-forensic perturbations by gans in these areas to enhance the visual quality and transferability of anti-forensic faces. meanwhile, in order to improve the anti-forensic effect, a double- mask (soft mask and hard mask) strategy and a three-part loss (the gan training loss, the adversarial loss consisting of ensemble classification loss and ensemble feature loss, and the regularization loss) are designed for the training of the generator. experiments conducted on fake faces generated by stylegan demonstrate the proposed method's advantage over the state-of-the-art methods in terms of anti-forensic success rate, imperceptibility, and transferability. the source code is available at https://github.com/imagecbj/a-local-perturbation-generation-method-for-gan-generated-face-anti-forensics.",AB_0372
