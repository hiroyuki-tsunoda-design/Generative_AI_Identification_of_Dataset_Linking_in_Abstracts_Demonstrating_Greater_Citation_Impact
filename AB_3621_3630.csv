AB,NO
"global average pooling (gap) plays an important role in traditional channel attention. however, there is the disadvantage of insufficient information to use the result of gap as the channel scalar. at the same time, the existing spatial attention models focus on the areas of interest using average pooling or convolutional networks, but there is a loss of feature information and neglect of the structural feature. in this paper, dual wavelet attention is proposed, which can effectively alleviate the aforementioned problems and enhance the representation ability of cnns. firstly, the equivalence between the sum of the low-frequency subband coefficients of 2d dwt (haar) and gap is proved. on this basis, the statistical characteristics of low-frequency and high-frequency subbands are effectively combined to obtain the channel scalars, which can better measure the importance of each channel. in addition, 2d dwt can effectively capture the approximate and detailed structural features. thus, wavelet spatial attention is proposed, which can effectively focus on the key spatial structural features. different from traditional spatial attention, it can better curve the structural and spatial attention for different channels. the experiments are verified on four natural image data sets and three remote sensing scene classification data sets, which shows the effectiveness and versatility of the proposed methods. the code of this paper will be available at https://github.com/yutinyang/dwan.",AB_0363
"recent researches have made a great progress in domain adaptive object detectors. these detectors aim to learn explicit domain-invariant features by adversarially mitigating domain divergence and simultaneously optimizing source risks. however, an inherent problem is that they ignore the informative knowledge implied in domain-specific features, which is recognized as implicit domain-invariant feature. this is mainly caused by the multimode structure underlying target distribution, characterized by various scales and categories of objects in target images. to solve that, we propose the implicit domain-invariant faster r-cnn (idf) by using non-adversarial domain discriminator, dual attention mechanism and selective feature perception. this idea is implemented on the faster r-cnn backbone, but with an improved architecture of two branches, i.e. domain-invariant branch and domain-specific branch. the former can clearly learn explicit domain adaptive features w.r.t. easy samples, while the latter aims to learn implicit domain-invariant features w.r.t. hard samples. experiments on numerous benchmark datasets, including the cityscapes, foggy cityscapes, kitti and sim10k, show the superiority of our idf over other state-of-the-art domain adaptive object detectors. the demo code is released in https://github.com/sea123321/idf.",AB_0363
"although having achieved the promising results on shape and color recovery through self-supervision, the multi-layer perceptrons-based methods usually suffer from heavy computational cost on learning the deep implicit surface representation. since rendering each pixel requires a forward network inference, it is very computationally intensive to synthesize a whole image. to tackle these challenges, we propose an effective coarse-to-fine approach to recover the textured mesh from multi-views in this paper. specifically, a differentiable poisson solver is employed to represent the object's shape, which is able to produce topology-agnostic and watertight surfaces. to account for depth information, we optimize the shape geometry by minimizing the differences between the rendered mesh and the predicted depth from multi-view stereo. in contrast to the implicit neural representation on shape and color, we introduce a physically-based inverse rendering scheme to jointly estimate the environment lighting and object's reflectance, which is able to render the high resolution image at real-time. the texture of reconstructed mesh is interpolated from a learnable dense texture grid. we have conducted the extensive experiments on several multi-view stereo datasets, whose promising results demonstrate the efficacy of our proposed approach. the code is available at https://github.com/l1346792580123/diff.",AB_0363
"superpixel generation is increasingly an important area for computer vision tasks. while superpixels with highly regular shapes are preferred to make the subsequent processing easier, the accuracy of the superpixel boundaries is also necessary. previous methods usually depend on a distance function considering both spatial and color coherency regularization on the whole image, which however is hard to balance between shape regularity and boundary adherence, especially when the desired number of superpixels is small. in addition, non-adaptive parameters and insufficient contour information also affect the performance of segmentation. to mitigate these problems, we propose a robust divide-and-conquer superpixel segmentation method, of which the core idea is that we apply a new contour information extraction and a pixel clustering to separate the input image into flat and non-flat regions, where the former targets shape regularity and the latter emphasizes boundary adherence, followed by an efficient hierarchical merging to clean up tiny and dangling superpixels. our algorithm requires no additional parameter tuning except the desired number of superpixels since our internal parameters are self-adaptive to the image contents. experimental results demonstrate that for public benchmark datasets, our algorithm consistently generates more regular superpixels with stronger boundary adherence than state-of-the-art methods while maintaining a competitive efficiency. the source code is available at https://github.com/yunyangxu/hqsgrd.",AB_0363
"recent years person re-identification (reid) has been developed rapidly due to its broad practical applications. most existing benchmarks assume that the same person wears the same clothes across captured images, while, in real-world scenarios, person may change his/her clothes frequently. thus the clothes-changing person reid (cc-reid) problem is introduced and several related benchmarks are established. cc-reid is a very difficult task as the main visual characteristics of a human body, clothes, are different between query and gallery, and clothes-irrelevant features are relatively weak. to promote the research and applications of person reid in clothes-changing scenarios, in this paper, we introduce a new task called clothes template based clothes-changing person reid (ctcc-reid), where the query image is enhanced by a clothes template which shares similar visual patterns with the clothes of the target person image in the gallery. so, reid methods are encouraged to jointly consider the original query image and the given clothes template for retrieval in the proposed ctcc-reid setting. to facilitate research works on ctcc-reid, we construct a novel large-scale reid dataset named clothes changing person set plus (cocas+), which contains both realistic and synthetic clothes-changing person images with manually collected clothes templates. furthermore, we propose a novel dual-attention biometric-clothes transfusion network (dualbct-net) for ctcc-reid, which can effectively learn to extract biometric features from the original query person image and clothes features from the given clothes template and then fuse them through a dual-attention fusion module. extensive experimental results show that the proposed ctcc-reid setting and cocas+ dataset can help greatly push the performance of clothes-changing reid toward practical applications, and synthetic data is impressively effective for ctcc-reid. what's more, the proposed dualbct-net shows significant improvements over state-of-the-art methods on the ctcc-reid task. cocas+ and code of dualbct-net will be released in https://github.com/chenhaobin/cocas-plus.",AB_0363
"bottom-up human pose estimation decouples computational complexity from the number of people but requires additional operations to match the detected keypoints to each human instance. existing approaches treat all keypoints equally while ignoring the relationships among keypoints, which in turn limit the performance ceilings. in this work, we propose a hierarchical associative encoding and decoding framework for bottom-up human pose estimation by introducing additional prior knowledge. specifically, in addition to keypoint-level and instance-level associations, we further divide keypoints into groups and explore group-level associations. this way, prior knowledge is incorporated to determine the keypoint groups for better associative encoding. to deal with complex poses, we introduce a focal pulling loss to focus more on the hard-to-associate keypoints. moreover, instead of using a pre-defined order for keypoint grouping, we propose a progressive associative decoding method to dynamically determine the order of keypoints for grouping, which helps reduce isolated keypoints. experimental results on the ms-coco, crowdpose and mpii datasets show superior performance of our proposed associative encoding and decoding algorithms. more importantly, we prove, through validation, that hierarchical associative encoding and decoding can be used as a plug-n-play module for performance improvement regardless of backbone architecture. our source code and pretrained models are available at https://github.com/ducongju/hae.",AB_0363
"real-time scene comprehension is the basis for automatic electric power inspection. however, existing rgb-based scene comprehension methods may achieve unsatisfied performance when dealing with complex scenarios, insufficient illumination or occluded appearances. to solve this problem, by cooperating visual and thermal images, the dual-space graph-based interaction network (dsgbinet) is proposed to achieve all-day time semantic segmentation of power equipment in high-voltage power transmission line and electric transformer substation scenes. specifically, modality-specific features are first extracted via two separate backbone networks with the same architecture. multi-modality high-level features are first fused via long-range relationship in coordinate space. then, multi-modality features from regular grids are further clustered and assigned to vertices in feature space. cross-graph and inner-graph regional relations are utilized for reasoning and enhancement, which could exploit the mutual benefits and extract rich contextual information in a semantic view. furthermore, to overcome the huge scale difference and the inherent characteristic of thermal images, the idea of multi-task learning is integrated into the decoding process. the edge detection and semantic segmentation are achieved collaboratively, which could segment the different power equipment more accurately and completely. the comparative and ablation experiments on the proposed two rgb-t semantic segmentation datasets evaluate the effectiveness and robustness of the proposed network compared with existing state-of-the-art methods. the extended experiments on the public datasets further demonstrate the superiority of the proposed method. our dataset and code will be released at: https://github.com/hhujiang/dsgbinet.",AB_0363
"visual counting, a task that aims to estimate the number of objects from an image/video, is an open-set problem by nature as the number of population can vary in [0, +8) in theory. however, collected data are limited in reality, which means that only a closed set is observed. existing methods typically model this task through regression, while they are prone to suffer from unseen scenes with counts out of the scope of the closed set. in fact, counting has an interesting and exclusive property- spatially decomposable. a dense region can always be divided until sub-region counts are within the previously observed closed set. we therefore introduce the idea of spatial divide-and-conquer (s-dc) that transforms open-set counting into a closed set problem. this idea is implemented by a novel supervised spatial divide-and-conquer network (ss-dcnet). it can learn from a closed set but generalize to open-set scenarios via s-dc. we provide mathematical analyses and a controlled experiment on synthetic data, demonstrating why closed-set modeling works well. experiments show that ss-dcnet achieves state-of-the-art performance in crowd counting, vehicle counting and plant counting. ss-dcnet also demonstrates superior transferablity under the cross-dataset setting. code and models are available at: https://git.io/ss-dcnet.",AB_0363
"depth and ego-motion estimations are essential for the localization and navigation of autonomous robots and autonomous driving. recent studies make it possible to learn the per-pixel depth and ego-motion from the unlabeled monocular video. in this paper, a novel unsupervised training framework is proposed with 3d hierarchical refinement and augmentation using explicit 3d geometry. in this framework, the depth and pose estimations are hierarchically and mutually coupled to refine the estimated pose layer by layer. the intermediate view image is proposed and synthesized by warping the pixels in an image with the estimated depth and coarse pose. then, the residual pose transformation can be estimated from the new view image and the image of the adjacent frame to refine the coarse pose. the iterative refinement is implemented in a differentiable manner in this paper, making the whole framework optimized uniformly. meanwhile, a new image augmentation method is proposed for the pose estimation by synthesizing a new view image, which creatively augments the pose in 3d space but gets a new augmented 2d image. the experiments on dkitti demonstrate that our depth estimation achieves state-of-the-art performance and even surpasses recent approaches that utilize other auxiliary tasks. our visual odometry outperforms all recent unsupervised monocular learning-based methods and achieves competitive performance to the geometry-based method, orb-slam2 with back-end optimization. the source codes will be released soon at: https://github.com/irmvlab/hranet.",AB_0363
"the curvature regularization method is well-known for its good geometric interpretability and strong priors in the continuity of edges, which has been applied to various image processing tasks. however, due to the non-convex, non-smooth, and highly non-linear intrinsic limitations, most existing algorithms lack a convergence guarantee. this paper proposes an efficient yet accurate scalar auxiliary variable (sav) scheme for solving both mean curvature and gaussian curvature minimization problems. the sav-based algorithms are shown unconditionally energy diminishing, fast convergent, and very easy to be implemented for different image applications. numerical experiments on noise removal, image deblurring, and single image super-resolution are presented on both gray and color image datasets to demonstrate the robustness and efficiency of our method. source codes are made publicly available at https://github.com/duanlab123/sav-curvature.",AB_0363
