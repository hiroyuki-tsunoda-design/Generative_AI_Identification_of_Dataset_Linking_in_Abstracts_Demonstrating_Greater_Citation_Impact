AB,NO
"nlrscape is a webserver that curates a collection of over 80 000 plant protein sequences identified in uniprotkb to contain nod-like receptor signatures, and hosts in addition a number of tools aimed at the exploration of the complex sequence landscape of this class of plant proteins. each entry gathers sequence information, domain and motif annotations from multiple third-party sources but also in-house advanced annotations aimed at addressing caveats of the existing broad-based annotations. nlrscape provides a top-down perspective of the nlr sequence landscape but also services for assisting a bottom-up approach starting from a given input sequence. sequences are clustered by their domain organization layout, global homology and taxonomic spread-in order to allow analysis of how particular traits of an nlr family are scattered within the plant kingdom. tools are provided for users to locate their own protein of interest in the overall nlr landscape, generate custom clusters centered around it and perform a large number of sequence and structural analyses using included interactive online instruments. amongst these, we mention: taxonomy distribution plots, homology cluster graphs, identity matrices and interactive msa synchronizing secondary structure and motif predictions. nlrscape can be found at: https:// nlrscape.biochim.ro/. .",AB_0061
"multi-label text classification (mltc) is the task that assigns each document to the most relevant subset of class labels. previous works usually ignored the correlation and semantics of labels resulting in information loss. to deal with this problem, we propose a new model that explores label dependencies and semantics by using graph convolutional networks (gcn). particularly, we introduce an efficient correlation matrix to model label correlation based on occurrence and co-occurrence probabilities. to enrich the semantic information of labels, we design a method to use external information from wikipedia for label embeddings. correlated label information learned from gcn is combined with fine-grained document representation generated from another sub-net for classification. experimental results on three benchmark datasets show that our model outweighs prior state-of-the-art methods. ablation studies also show several aspects of the proposed model. our code is available at https://github.com/chiennv2000/lr-gcn.",AB_0061
"upon the discovery of adversarial attacks, robust models have become obligatory for deep learning-based systems. adversarial training with first-order attacks has been one of the most effective defenses against adversarial perturbations to this day. the majority of the adversarial training approaches focus on iteratively perturbing each pixel with the gradient of the loss function with respect to the input image. however, the adversarial training with gradient-based attacks lacks diversity and does not generalize well to natural images and various attacks. this study presents a robust training algorithm where the adversarial perturbations are automatically synthesized from a random vector using a generator network. the classifier is trained with cross-entropy loss regularized with the optimal transport distance between the representations of the natural and synthesized adversarial samples. unlike prevailing generative defenses, the proposed one-step attack generation framework synthesizes diverse perturbations without utilizing the gradient of the classifier's loss. the main contributions of the proposed robust training framework are: i) preserving the state-of-the-art generalization performance of the deep model, ii) not requiring an iterative or recursive scheme, and iii) providing robustness that is comparable with the state-of-the-art in literature. experimental results show that the proposed approach attains comparable robustness with various gradient-based and generative robust training techniques on cifar10, cifar100, svhn, and tiny imagenet datasets. in addition, compared to the baselines, the proposed robust training framework generalizes well to the natural samples. code and trained models are available here https:// github.com/allab-boun/robustness-via-synthesis.git.",AB_0061
"with the increase in the world population and the rapid developments in technology, the energy demands on modern electricity grids are also rising. in order to meet these demands, power systems are increasingly using renewable energy resources (ress) in addition to traditional fossil fuel-powered generation units and thus, the structures being implemented in electricity grids are more complex. consequently, the planning and operation of modern power systems presents important problems, one of which is that of the optimal power flow (opf). with the integration of ress, which are usually intermittent in nature, the opf becomes a more difficult problem to solve. in this study, the opf problem was designed under different operating cases, considering thermal, wind, solar, small-hydro, and tidal energy systems and load demand uncertainties. the adaptive fitness-distance balance selection-based stochastic fractal search (afdb-sfs) algorithm was proposed to solve this designed opf problem. the results for the proposed approach from the experimental studies were statistically evaluated and compared with the results obtained from competitive optimization algorithms in the literature. the comparison demonstrated that the proposed afdb-sfs algorithm was able to outperform the other algorithms in finding the optimal solution, and convergence speed to the optimal solution. according to the experimental study results, the proposed afdb-sfs algorithm was able to optimize cost by 5.7362%, 0.0954%, 7.6244%, 0.17871%, 2.4307%, 0.12585%, 2.01729%, 1.7408%, 1.95317%, 3.5486%, 2.2007%, and 1.5203% better than the ao, gbo, gpc, hgs, hho, run, tso, lshade, lshade-epsin, lshade-cnepsin, lshade-spacma, and madde optimization algorithms in the proposed opf problem. the source codes of the afdb-sfs algorithm (proposed method) can be accessed at this link: https://ch.mathworks.com/matlabcentral/ fileexchange/118485-afdb-sfs.",AB_0061
"neural machine translation systems trained on low-resource languages produce sub-optimal results due to the scarcity of large parallel datasets. to alleviate this problem, parallel corpora can be mined from the web. two key tasks in a parallel corpus mining pipeline are web document alignment and sentence alignment. effective approaches for these tasks obtained vector representations of the documents (or sentences) belonging to the two languages and determine the alignment between the documents (or sentences) based on a semantic similarity scoring mechanism. recently, document or sentence representations obtained from pre-trained multilingual language models (pmlms) such as laser, xlm-r and labse have significantly improved the benchmark scores in diverse natural language processing tasks. in this study, we carry out an empirical analysis of the effectiveness of these pmlms of the document and sentence alignment tasks in the context of the low-resource language pairs sinhala-english, tamil-english and sinhala-tamil. further, we introduce a weighting mechanism based on small-scale bilingual lexicons to improve the semantic similarity measurement between sentences and documents. our results show that both document and sentence alignment can be further improved using our weighting mechanism. we have also compiled a gold-standard evaluation benchmark dataset for document alignment and sentence alignment tasks for the considered language pairs. this dataset (https://github.com/kdissa/comparable-corpus) and the source code (https://github.com/nipcuom/parallel_corpus_mining) are publicly released.",AB_0061
"we present a simulation tool, tethys (i.e., two-dimensional emitter of thz, hydrodynamic simulation), developed with the aim of studying hydrodynamic models for electronic transport in graphene, being particularly tailored for the simulation of the configuration of top-gated graphene field-effect transistors (gfet) near room temperature. we present the model governing the system and briefly discuss its validity and physical parameters; then we proceed to discuss the algorithms and numerical methods used in its implementation, namely, two-step richtmyer and weighted forward-time centred-space schemes, which address the hyperbolic and parabolic parts of the model, respectively. an abridged usage guide and description of the output are also provided, as well as two illustrative simulation cases: the dyakonov- shur instability and the boundary layer of pulsatile electric flow. the simulation program here brought forward is uncomplicated and lightweight, yet robust and accurate, having a wide variety of applicable scenarios and configurations to explore.program summaryprogram title: tethyscpc library link to program files: https://doi .org /10 .17632 /s28zwgpnpw.1developer's repository link: github .com /pcosme /tethys -graphene -hydrodynamic -simulationlicensing provisions: mitprogramming language: c++nature of problem: the main objective of this code is to simulate and analyse the electronic conduction on gated (mono-layer) graphene resorting to a hydrodynamic model [1]. note that, given the dirac-like dispersion relation of such material, the mass of the carriers is ill-defined; and, in fact, the mass of the fluid element varies with the number density, which sets this system apart from other 2d electron fluids [2]. the model assumes enough doping, so that only one kind of carriers, electrons or holes, participates in the dynamics of the system. moreover, the simulation is limited to the fully degenerate limit.thus, these assumptions correspond to the situation of a regular gfet and this application can be used to explore atypical phenomena arising from the effects of uniform and static magnetic field and hall viscosity, and even spatially non-uniform gate capacitance.furthermore, this software was designed with the intent of examining unstable nonlinear modes that may lead to the emission of radiation in the thz range [3].solution method: the fluid equations (continuity, momentum and energy transport) are solved with a split-operator technique, with a second-order finite-volume lax-wendroff method [4] for the hyperbolic part and a forward-time centred-space of second-order stencil to tackle the dissipation and heat diffusion. that is, the fields of number density, velocity/momentum density and temperature are calculated over a rectangular grid and stored at a hdf5 file for further processing.then, the obtained field profiles are used to compute the relevant macroscopic electronic quantities, such as drain-to-source current or dissipated power, and, particularly, the electric dipole moment of the charge distribution, which can subsequently be used to characterize the emitted far-field radiation. this is mostly done by numerically integrating the data in space, with the necessary time derivatives obtained by gaussian convolution.",AB_0061
"memes have become a de-facto media device in online communication. unfortunately, memes are also used for trolling, which intends to demean, harass, or bully targeted individuals. as a result of which, the targeted individual could fall prey to opinion manipulation. trolling via image with text (iwt) memes which we refer to as 'troll memes', are difficult to identify due to the multimodal (image + text) nature of such memes. however, the research into the identification and classification of troll memes with opinion manipulation remains unexplored. to bridge this research gap, we introduce a three-level taxonomy that studies the effect of trolling in domain-specific opinion manipulation. on the first level, we classify the meme as troll or not troll. on the second level, we classify if the meme intends opinion manipulation. on the third level, if the opinion manipulation is present, then we classify the domain (political, product, other) of the opinion manipulation. to support the class definitions proposed in the taxonomy, we enhanced an existing dataset (memotion) by annotating the data with our defined classes. this results in a dataset of 8,881 iwt memes in the english language (trollswithopinion dataset) which we make available as open-source at github(https://github.com/sharduls007/trollopinionmemes). we perform experiments on all three levels and present the classification report of the results using machine learning and state-of-the-art deep learning techniques. the classification report highlights the complex nature of the task since the models perform well on the first two levels. however, we see a degradation of the evaluation results on the third level of the taxonomy.",AB_0061
"in the present work, we present normative data for a set of 39 original clipart-style images that can be used as material in studies involving judgements of proportion. the original images are drawings that depict different day-to-day scenarios (e.g., lighted windows in a building; books on a shelf) and each has seven variants of different proportions (from 20% to 80%) belonging to different categories (discrete vs continuous; social vs non-social; natural vs artificial; stimuli physical dimensions; number of referents). normative data for these images are presented in an interactive database (available at https://judgment-images-and-norms.shinyapps.io/estimates_interactive/), corresponding to the means of proportion estimates (in percentage form), the perceived ease of making such estimates, the perceived level of familiarity and liking for each image, and the relationships between these variables. in the paper, we analyse the data at an individual level, addressing how the latter judgements are related to the proportion estimates, how those estimates are related to objective proportions, and how these relationships are moderated by image category. the analyses presented in this paper aim to aid readers in selecting images that enable them to better address specific influences on proportional estimates or to control for those influences in their studies.",AB_0061
"recent research in disaster informatics demonstrates a practical and important use case of artificial intelligence to save human lives and suffering during natural disasters based on social media contents (text and images). while notable progress has been made using texts, research on exploiting the images remains relatively under-explored. to advance image-based approaches, we propose medic (https://crisisnlp.qcri.org/meclic/index.html), which is the largest social media image classification dataset for humanitarian response consisting of 71,198 images to address four different tasks in a multi-task learning setup. this is the first dataset of its kind: social media images, disaster response, and multi-task learning research. an important property of this dataset is its high potential to facilitate research on multi-task learning, which recently receives much interest from the machine learning community and has shown remarkable results in terms of memory, inference speed, performance, and generalization capability. therefore, the proposed dataset is an important resource for advancing image-based disaster management and multi-task machine learning research. we experiment with different deep learning architectures and report promising results, which are above the majority baselines for all tasks. along with the dataset, we also release all relevant scripts (https://github.com/firojalam/medic).",AB_0061
"zero-shot learning enables the recognition of classes not seen during training through the use of semantic information comprising a visual description of the class either in textual or attribute form. despite the advances in the performance of zero-shot learning methods, most of the works do not explicitly exploit the correlation between the visual attributes of the image and their corresponding semantic attributes for learning discriminative visual features. in this paper, we introduce an attention-based strategy for deriving features from the image regions regarding the most prominent attributes of the image class. in particular, we train a convolutional neural network (cnn) for image attribute prediction and use a gradient-weighted method for deriving the attention activation maps of the most salient image attributes. these maps are then incorporated into the feature extraction process of zero-shot learning (zsl) approaches for improving the discriminability of the features produced through the implicit inclusion of semantic information. for experimental validation, the performance of state-of-the-art zsl methods was determined using features with and without the proposed attention model. surprisingly, we discover that the proposed strategy degrades the performance of zsl methods in classical zsl datasets (awa2), but it can significantly improve performance when using face datasets. our experiments show that these results are a consequence of the interpretability of the dataset attributes, suggesting that existing zsl datasets attributes are, in most cases, difficult to be identifiable in the image. source code is available at https://github.com/cristianopatricio/sgam.",AB_0061
