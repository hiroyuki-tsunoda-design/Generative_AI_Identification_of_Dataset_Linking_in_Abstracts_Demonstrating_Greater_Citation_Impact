AB,NO
"deep cross-modal hashing has achieved excellent retrieval performance with the powerful representation capability of deep neural networks. regrettably, current methods are inevitably vulnerable to adversarial attacks, especially well-designed subtle perturbations that can easily fool deep cross-modal hashing models into returning irrelevant or the attacker's specified results. although adversarial attacks have attracted increasing attention, there are few studies on specialized attacks against deep cross-modal hashing. to solve these issues, we propose a targeted adversarial attack method against deep cross-modal hashing retrieval in this paper. to the best of our knowledge, this is the first work in this research field. concretely, we first build a progressive fusion module to extract fine-grained target semantics through a progressive attention mechanism. meanwhile, we design a semantic adaptation network to generate the target prototype code and reconstruct the category label, thus realizing the semantic interaction between the target semantics and the implicit semantics of the attacked model. to bridge modality gaps and preserve local example details, a semantic translator seamlessly translates the target semantics and then embeds them into benign examples in collaboration with a u-net framework. moreover, we construct a discriminator for adversarial training, which enhances the visual realism and category discrimination of adversarial examples, thus improving their targeted attack performance. extensive experiments on widely tested cross-modal retrieval datasets demonstrate the superiority of our proposed method. also, transferable attacks show that our generated adversarial examples have well generalization capability on targeted attacks. the source codes and datasets are available at https://github.com/tswang0116/ta-dch.",AB_0334
"differently exposed low dynamic range (ldr) images are often captured sequentially using a smart phone or a digital camera with movements. optical flow thus plays an important role in ghost removal for high dynamic range (hdr) imaging. the optical flow estimation is based on the theory of photometric consistency, which assumes that the corresponding pixels between two images have the same intensity. however, the assumption is no longer valid for the differently exposed ldr images since a pixel's intensity changes significantly inter images. to address the problem, an unsupervised optical flow estimation framework, is presented in this study. intensity mapping functions (imfs) are first adopted to alleviate the intensity changes between the ldr images. then a novel imf-based unsupervised learning objective is proposed to circumvent the need for ground truth optical flows when training the deep network. experimental results and ablation studies on publicly available datasets show that our framework outperforms the state-of-the-art unsupervised optical flow methods, demonstrating the effectiveness of the imf and the learning objective. our code is available at https://github.com/liuziyang123/ldrflow.",AB_0334
"point cloud registration is a fundamental problem in 3d computer vision. outdoor lidar point clouds are typically large-scale and complexly distributed, which makes the registration challenging. in this paper, we propose an efficient hierarchical network named hregnet for large-scale outdoor lidar point cloud registration. instead of using all points in the point clouds, hregnet performs registration on hierarchically extracted keypoints and descriptors. the overall framework combines the reliable features in deeper layer and the precise position information in shallower layers to achieve robust and precise registration. we present a correspondence network to generate correct and accurate keypoints correspondences. moreover, bilateral consensus and neighborhood consensus are introduced for keypoints matching, and novel similarity features are designed to incorporate them into the correspondence network, which significantly improves the registration performance. in addition, we design a consistency propagation strategy to effectively incorporate spatial consistency into the registration pipeline. the whole network is also highly efficient since only a small number of keypoints are used for registration. extensive experiments are conducted on three large-scale outdoor lidar point cloud datasets to demonstrate the high accuracy and efficiency of the proposed hregnet. the source code of the proposed hregnet is available at https://github.com/ispc-lab/hregnet2.",AB_0334
"panoramic depth estimation has become a hot topic in 3d reconstruction techniques with its omnidirectional spatial field of view. however, panoramic rgb-d datasets are difficult to obtain due to the lack of panoramic rgb-d cameras, thus limiting the practicality of supervised panoramic depth estimation. self-supervised learning based on rgb stereo image pairs has the potential to overcome this limitation due to its low dependence on datasets. in this work, we propose the spdet, an edge-aware self-supervised panoramic depth estimation network that combines the transformer with a spherical geometry feature. specifically, we first introduce the panoramic geometry feature to construct our panoramic transformer and reconstruct high-quality depth maps. furthermore, we introduce the pre-filtered depth-image-based rendering method to synthesize the novel view image for self-supervision. meanwhile, we design an edge-aware loss function to improve the self-supervised depth estimation for panorama images. finally, we demonstrate the effectiveness of our spdet with a series of comparison and ablation experiments while achieving the state-of-the-art self-supervised monocular panoramic depth estimation. our code and models are available at https://github.com/zcq15/spdet.",AB_0334
"pre-training on large-scale datasets has played an increasingly significant role in computer vision and natural language processing recently. however, as there exist numerous application scenarios that have distinctive demands such as certain latency constraints and specialized data distributions, it is prohibitively expensive to take advantage of large-scale pre-training for per-task requirements. we focus on two fundamental perception tasks (object detection and semantic segmentation) and present a complete and flexible system named gaia-universe(gaia), which could automatically and efficiently give birth to customized solutions according to heterogeneous downstream needs through data union and super-net training. gaia is capable of providing powerful pre-trained weights and searching models that conform to downstream demands such as hardware constraints, computation constraints, specified data domains, and telling relevant data for practitioners who have very few datapoints on their tasks. with gaia, we achieve promising results on coco, objects365, open images, bdd100 k, and uodb which is a collection of datasets including kitti, voc, widerface, dota, clipart, comic, and more. taking coco as an example, gaia is able to efficiently produce models covering a wide range of latency from 16 ms to 53 ms, and yields ap from 38.2 to 46.5 without whistles and bells. gaia is released at https://github.com/gaia-vision.",AB_0334
"weakly supervised object detection (wsod) is of increasing importance in the community of computer vision as its extensive applications and low manual cost. most of the advanced wsod approaches build upon an indefinite and quality-agnostic framework, leading to unstable and incomplete object detectors. this paper attributes these issues to the process of inconsistent learning for object variations and the unawareness of localization quality and constructs a novel end-to-end invariant and equivariant network (ienet). it is implemented with a flexible multi-branch online refinement, to be naturally more comprehensive-perceptive against various objects. specifically, ienet first performs label propagation from the predicted instances to their transformed ones in a progressive manner, achieving affine-invariant learning. meanwhile, ienet also naturally utilizes rotation-equivariant learning as a pretext task and derives an instance-level rotation-equivariant branch to be aware of the localization quality. with affine-invariance learning and rotation-equivariant learning, ienet urges consistent and holistic feature learning for wsod without additional annotations. on the challenging datasets of both natural scenes and aerial scenes, we substantially boost wsod to new state-of-the-art performance. the codes have been released at: https://github.com/xiaoxfeng/ienet.",AB_0334
"this paper explores the problem of reconstructing high-resolution light field (lf) images from hybrid lenses, including a high-resolution camera surrounded by multiple low-resolution cameras. the performance of existing methods is still limited, as they produce either blurry results on plain textured areas or distortions around depth discontinuous boundaries. to tackle this challenge, we propose a novel end-to-end learning-based approach, which can comprehensively utilize the specific characteristics of the input from two complementary and parallel perspectives. specifically, one module regresses a spatially consistent intermediate estimation by learning a deep multidimensional and cross-domain feature representation, while the other module warps another intermediate estimation, which maintains the high-frequency textures, by propagating the information of the high-resolution view. we finally leverage the advantages of the two intermediate estimations adaptively via the learned confidence maps, leading to the final high-resolution lf image with satisfactory results on both plain textured areas and depth discontinuous boundaries. besides, to promote the effectiveness of our method trained with simulated hybrid data on real hybrid data captured by a hybrid lf imaging system, we carefully design the network architecture and the training strategy. extensive experiments on both real and simulated hybrid data demonstrate the significant superiority of our approach over state-of-the-art ones. to the best of our knowledge, this is the first end-to-end deep learning method for lf reconstruction from a real hybrid input. we believe our framework could potentially decrease the cost of high-resolution lf data acquisition and benefit lf data storage and transmission. the code will be publicly available at https://github.com/jingjin25/lfhybridsr-fusion.",AB_0334
"modern deep neural networks can easily overfit to biased training data containing corrupted labels or class imbalance. sample re-weighting methods are popularly used to alleviate this data bias issue. most current methods, however, require to manually pre-specify the weighting schemes relying on the characteristics of the investigated problem and training data. this makes them fairly hard to be generally applied in practical scenarios, due to their significant complexities and inter-class variations of data bias. to address this issue, we propose a meta-model capable of adaptively learning an explicit weighting scheme directly from data. specifically, by seeing each training class as a separate learning task, our method aims to extract an explicit weighting function with sample loss and task/class feature as input, and sample weight as output, expecting to impose adaptively varying weighting schemes to different sample classes based on their own intrinsic bias characteristics. extensive experiments substantiate the capability of our method on achieving proper weighting schemes in various data bias cases, like class imbalance, feature-independent and dependent label noises, and more complicated bias scenarios beyond conventional cases. besides, the task-transferability of the learned weighting scheme is also substantiated, by readily deploying the weighting function learned on relatively smaller-scale cifar-10 dataset on much larger-scale full webvision dataset. the general availability of our method for multiple robust deep learning issues, including partial-label learning, semi-supervised learning and selective classification, has also been validated. code for reproducing our experiments is available at https://github.com/xjtushujun/cmw-net.",AB_0334
"we present pymaf-x, a regression-based approach to recovering a parametric full-body model from a single image. this task is very challenging since minor parametric deviation may lead to noticeable misalignment between the estimated mesh and the input image. moreover, when integrating part-specific estimations into the full-body model, existing solutions tend to either degrade the alignment or produce unnatural wrist poses. to address these issues, we propose a pyramidal mesh alignment feedback (pymaf) loop in our regression network for well-aligned human mesh recovery and extend it as pymaf-x for the recovery of expressive full-body models. the core idea of pymaf is to leverage a feature pyramid and rectify the predicted parameters explicitly based on the mesh-image alignment status. specifically, given the currently predicted parameters, mesh-aligned evidence will be extracted from finer-resolution features accordingly and fed back for parameter rectification. to enhance the alignment perception, an auxiliary dense supervision is employed to provide mesh-image correspondence guidance while spatial alignment attention is introduced to enable the awareness of the global contexts for our network. when extending pymaf for full-body mesh recovery, an adaptive integration strategy is proposed in pymaf-x to produce natural wrist poses while maintaining the well-aligned performance of the part-specific estimations. the efficacy of our approach is validated on several benchmark datasets for body, hand, face, and full-body mesh recovery, where pymaf and pymaf-x effectively improve the mesh-image alignment and achieve new the project page with code and video results can be found at https://www.liuyebin.com/pymaf-x.",AB_0334
"guided depth super-resolution aims at using a low-resolution depth map and an associated high-resolution rgb image to recover a high-resolution depth map. however, restoring precise and sharp edges near depth discontinuities and fine structures is still challenging for state-of-the-art methods. to alleviate this issue, we propose a novel multi-stage depth super-resolution network, which progressively reconstructs hr depth maps from explicit and implicit high-frequency information. we introduce an efficient transformer to obtain explicit high-frequency information. the shape bias and global context of the transformer allow our model to focus on high-frequency details between objects, i.e., depth discontinuities, rather than texture within objects. furthermore, we project the input color images into the frequency domain for additional implicit high-frequency cues extraction. finally, to incorporate the structural details, we develop a fusion strategy that combines depth features and high-frequency information in the multi-stage-scale framework. exhaustive experiments on the main benchmarks show that our approach establishes a new state-of-the-art. code will be publicly available at https://github.com/wudiqx106/dsr-ei.",AB_0334
