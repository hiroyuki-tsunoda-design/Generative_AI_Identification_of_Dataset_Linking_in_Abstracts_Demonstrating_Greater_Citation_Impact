AB,NO
"the fusion of infrared and visible images aims to generate a composite image that can simultaneously contain the thermal radiation information of an infrared image and the plentiful texture details of a visible image to detect targets under various weather conditions with a high spatial resolution of scenes. previous deep fusion models were generally based on convolutional operations, resulting in a limited ability to represent long-range context information. in this paper, we propose a novel end-to-end model for infrared and visible image fusion via a dual attention transformer termed datfuse. to accurately examine the significant areas of the source images, a dual attention residual module (darm) is designed for important feature extraction. to further model long-range dependencies, a transformer module (trm) is devised for global complementary information preservation. moreover, a loss function that consists of three terms, namely, pixel loss, gradient loss, and structural loss, is designed to train the proposed model in an unsupervised manner. this can avoid manually designing complicated activity-level measurement and fusion strategies in traditional image fusion methods. extensive experiments on public datasets reveal that our datfuse outperforms other representative state-of-the-art approaches in both qualitative and quantitative assessments. the proposed model is also extended to address other infrared and visible image fusion tasks without fine-tuning, and the promising results demonstrate that it has good generalization ability. the source code is available at https://github.com/tthinking/datfuse.",AB_0349
"one effective way to address unsupervised person re-identification is to use a clustering-based contrastive learning approach. existing state-of-the-art methods adopt clustering algorithms (e.g., dbscan) and camera id information to divide all person images into several camera-aware proxies. then, for each person image, the extracted feature representation is pulled closer to the centroids of its pseudo-positive proxies (the proxies that share the same pseudo-identity label with this image) and pushed away from the centroids of other pseudo-negative proxies (the proxies that share the different pseudo-identity label with this image). however, the quality of the proxy centroid is significantly affected by the proxy impurity issue and thus deteriorates the learned feature representations. on the premise that we cannot introduce superior supervision signals by thoroughly solving the proxy impurity issue, for a person image, identifying its plausible proxies: the pseudo-negative proxies which potentially include its wrongly-clustered instances (the instances with the same ground-truth identity with this image), and further fixing the resulted incorrect supervision signals become an urgent and challenging problem. this paper proposes a simple yet effective approach to address this problem. with a given image, our method can effectively locate its plausible proxies. then we introduce credibility to measure how much we should treat the centroid of each mined plausible proxy as a positive supervision signal rather than entirely negative. extensive experiments on three widely-used person re-id datasets validate the effectiveness of our proposed approach. codes will be available at: https://github.com/dingyuan-zheng/ppcl.",AB_0349
"human activity understanding is of widespread interest in artificial intelligence and spans diverse applications like health care and behavior analysis. although there have been advances with deep learning, it remains challenging. the object recognition-like solutions usually try to map pixels to semantics directly, but activity patterns are much different from object patterns, thus hindering another success. in this article, we propose a novel paradigm to reformulate this task in two-stage: first mapping pixels to an intermediate space spanned by atomic activity primitives, then programming detected primitives with interpretable logic rules to infer semantics. to afford a representative primitive space, we build a knowledge base including 26+ m primitive labels and logic rules from human priors or automatic discovering. our framework, human activity knowledge engine (hake), exhibits superior generalization ability and performance upon canonical methods on challenging benchmarks. code and data are available at http://hake-mvig.cn/.",AB_0349
"registration is a basic yet crucial task in point cloud processing. in correspondence-based point cloud registration, matching correspondences by point feature techniques may lead to an extremely high outlier (false correspondence) ratio. current outlier removal methods still suffer from low efficiency, accuracy, and recall rate. we use an intuitive method to describe the 6-dof (degree of freedom) curtailment process in point cloud registration and propose an outlier removal strategy based on the reliability of the correspondence graph. the method constructs the corresponding graph according to the given correspondences and designs the concept of the reliability degree of the graph node for optimal candidate selection and the reliability degree of the graph edge to obtain the global maximum consensus set. the presented method achieves fast and accurate outliers removal along with gradual aligning parameters estimation. extensive experiments on simulations and challenging real-world datasets demonstrate that the proposed method can still perform effective point cloud registration even the correspondence outlier ratio is over 99%, and the efficiency is better than the state-of-the-art. code is available at https://github.com/wpc-whu/gror.",AB_0349
"although synthetic aperture imaging (sai) can achieve the seeing-through effect by blurring out off-focus foreground occlusions while recovering in-focus occluded scenes from multi-view images, its performance is often deteriorated by dense occlusions and extreme lighting conditions. to address the problem, this paper presents an event-based sai (e-sai) method by relying on the asynchronous events with extremely low latency and high dynamic range acquired by an event camera. specifically, the collected events are first refocused by a refocus-net module to align in-focus events while scattering out off-focus ones. following that, a hybrid network composed of spiking neural networks (snns) and convolutional neural networks (cnns) is proposed to encode the spatio-temporal information from the refocused events and reconstruct a visual image of the occluded targets. extensive experiments demonstrate that our proposed e-sai method can achieve remarkable performance in dealing with very dense occlusions and extreme lighting conditions and produce high-quality images from pure events. codes and datasets are available at https://dvs-whu.cn/projects/esai/.",AB_0349
"co-salient object detection (co-sod) aims at discovering the common objects in a group of relevant images. mining a co-representation is essential for locating co-salient objects. unfortunately, the current co-sod method does not pay enough attention that the information not related to the co-salient object is included in the co-representation. such irrelevant information in the co-representation interferes with its locating of co-salient objects. in this paper, we propose a co-representation purification (corp) method aiming at searching noise-free co-representation. we search a few pixel-wise embeddings probably belonging to co-salient regions. these embeddings constitute our co-representation and guide our prediction. for obtaining purer co-representation, we use the prediction to iteratively reduce irrelevant embeddings in our co-representation. experiments on three datasets demonstrate that our corp achieves state-of-the-art performances on the benchmark datasets. our source code is available at https://github.com/zzy816/corp.",AB_0349
"supervised deep learning has achieved tremendous success in many computer vision tasks, which however is prone to overfit noisy labels. to mitigate the undesirable influence of noisy labels, robust loss functions offer a feasible approach to achieve noise-tolerant learning. in this work, we systematically study the problem of noise-tolerant learning with respect to both classification and regression. specifically, we propose a new class of loss function, namely asymmetric loss functions (alfs), which are tailored to satisfy the bayes-optimal condition and thus are robust to noisy labels. for classification, we investigate general theoretical properties of alfs on categorical noisy labels, and introduce the asymmetry ratio to measure the asymmetry of a loss function. we extend several commonly-used loss functions, and establish the necessary and sufficient conditions to make them asymmetric and thus noise-tolerant. for regression, we extend the concept of noise-tolerant learning for image restoration with continuous noisy labels. we theoretically prove that l(p) loss (p > 0) is noise-tolerant for targets with the additive white gaussian noise. for targets with general noise, we introduce two losses as surrogates of l(0) loss that seeks the mode when clean pixels keep dominant. experimental results demonstrate that alfs can achieve better or comparative performance compared with the state-of-the-arts. the source code of our method is available at: https://github.com/hitcszx/alfs.",AB_0349
"attention-based neural networks, such as transformers, have become ubiquitous in numerous applications, including computer vision, natural language processing, and time-series analysis. in all kinds of attention networks, the attention maps are crucial as they encode semantic dependencies between input tokens. however, most existing attention networks perform modeling or reasoning based on representations, wherein the attention maps of different layers are learned separately without explicit interactions. in this paper, we propose a novel and generic evolving attention mechanism, which directly models the evolution of inter-token relationships through a chain of residual convolutional modules. the major motivations are twofold. on the one hand, the attention maps in different layers share transferable knowledge, thus adding a residual connection can facilitate the information flow of inter-token relationships across layers. on the other hand, there is naturally an evolutionary trend among attention maps at different abstraction levels, so it is beneficial to exploit a dedicated convolution-based module to capture this process. equipped with the proposed mechanism, the convolution-enhanced evolving attention networks achieve superior performance in various applications, including time-series representation, natural language understanding, machine translation, and image classification. especially on time-series representation tasks, evolving attention-enhanced dilated convolutional (ea-dc-) transformer outperforms state-of-the-art models significantly, achieving an average of 17% improvement compared to the best sota. to the best of our knowledge, this is the firstwork that explicitly models the layer-wise evolution of attention maps. our implementation is available at https://github.com/pkuyym/evolvingattention.",AB_0349
"reference-based super-resolution (ref-sr) has recently emerged as a promising paradigm to enhance a low-resolution (lr) input image or video by introducing an additional high-resolution (hr) reference image. existing ref-sr methods mostly rely on implicit correspondence matching to borrow hr textures from reference images to compensate for the information loss in input images. however, performing local transfer is difficult because of two gaps between input and reference images: the transformation gap (e.g., scale and rotation) and the resolution gap (e.g., hr and lr). to tackle these challenges, we propose c-2-matching in this work, which performs explicit robust matching crossing transformation and resolution. 1) to bridge the transformation gap, we propose a contrastive correspondence network, which learns transformation-robust correspondences using augmented views of the input image. 2) to address the resolution gap, we adopt teacher-student correlation distillation, which distills knowledge from the easier hr-hr matching to guide the more ambiguous lr-hr matching. 3) finally, we design a dynamic aggregation module to address the potential misalignment issue between input images and reference images. in addition, to faithfully evaluate the performance of reference-based image super-resolution (ref image sr) under a realistic setting, we contribute the webly-referenced sr (wr-sr) dataset, mimicking the practical usage scenario. we also extend c-2-matching to reference-based video super-resolution (ref vsr) task, where an image taken in a similar scene serves as the hr reference image. extensive experiments demonstrate that our proposed c-2-matching significantly outperforms state of the arts by up to 0.7 db on the standard cufed5 benchmark and also boosts the performance of video super-resolution by incorporating the c-2-matching component into video sr pipelines. notably, c-2-matching also shows great generalizability on wr-sr dataset as well as robustness across large scale and rotation transformations. codes and datasets are available at https://github.com/yumingj/c-2-matching.",AB_0349
"recent advances in self-supervised learning (ssl) in computer vision are primarily comparative, whose goal is to preserve invariant and discriminative semantics in latent representations by comparing siamese image views. however, the preserved high-level semantics do not contain enough local information, which is vital in medical image analysis (e.g., image-based diagnosis and tumor segmentation). to mitigate the locality problem of comparative ssl, we propose to incorporate the task of pixel restoration for explicitly encoding more pixel-level information into high-level semantics. we also address the preservation of scale information, a powerful tool in aiding image understanding but has not drawn much attention in ssl. the resulting framework can be formulated as a multi-task optimization problem on the feature pyramid. specifically, we conduct multi-scale pixel restoration and siamese feature comparison in the pyramid. in addition, we propose non-skip u-net to build the feature pyramid and develop sub-crop to replace multi-crop in 3d medical imaging. the proposed unified ssl framework (pcrlv2) surpasses its self-supervised counterparts on various tasks, including brain tumor segmentation (brats 2018), chest pathology identification (chestx-ray, chexpert), pulmonary nodule detection (luna), and abdominal organ segmentation (lits), sometimes outperforming them by large margins with limited annotations. codes and models are available at https://github.com/rl4m/pcrlv2.",AB_0349
