AB,NO
"due to the light absorption and scattering induced by the water medium, underwater images usually suffer from some degradation problems, such as low contrast, color distortion, and blurring details, which aggravate the difficulty of downstream underwater understanding tasks. therefore, how to obtain clear and visually pleasant images has become a common concern of people, and the task of underwater image enhancement (uie) has also emerged as the times require. among existing uie methods, generative adversarial networks (gans) based methods perform well in visual aesthetics, while the physical model-based methods have better scene adaptability. inheriting the advantages of the above two types of models, we propose a physical model-guided gan model for uie in this paper, referred to as pugan. the entire network is under the gan architecture. on the one hand, we design a parameters estimation subnetwork (par-subnet) to learn the parameters for physical model inversion, and use the generated color enhancement image as auxiliary information for the two-stream interaction enhancement sub-network (tsie-subnet). meanwhile, we design a degradation quantization (dq) module in tsie-subnet to quantize scene degradation, thereby achieving reinforcing enhancement of key regions. on the other hand, we design the dual-discriminators for the style-content adversarial constraint, promoting the authenticity and visual aesthetics of the results. extensive experiments on three benchmark datasets demonstrate that our pugan outperforms state-of-the-art methods in both qualitative and quantitative metrics. the code and results can be found from the link of https://rmcong.github.io/proj_pugan.html.",AB_0381
"it has long been recognized that the standard convolution is not rotation equivariant and thus not appropriate for downside fisheye images which are rotationally symmetric. this paper introduces rotational convolution, a novel convolution that rotates the convolution kernel by characteristics of downside fisheye images. with the four rotation states of the convolution kernel, rotational convolution can be implemented on discrete signals. rotational convolution improves the performance of different networks in semantic segmentation and object detection markedly, harming the inference speed slightly. finally, we demonstrate our methods' numerical accuracy, computational efficiency, and effectiveness on the public segmentation dataset theodore and our self-built detection dataset seu-fisheye. our code is available at: https://github.com/wx19941204/rotational-convolution-for-downside-fisheye-images.",AB_0381
"conventional social media platforms usually downscale high-resolution (hr) images to restrict their resolution to a specific size for saving transmission/storage cost, which makes those visual details inaccessible to other users. to bypass this obstacle, recent invertible image downscaling methods jointly model the downscaling/upscaling problems and achieve impressive performance. however, they only consider fixed integer scale factors and may be inapplicable to generic downscaling tasks towards resolution restriction as posed by social media platforms. in this paper, we propose an effective and universal scale-arbitrary invertible image downscaling network (aidn), to downscale hr images with arbitrary scale factors in an invertible manner. particularly, the hr information is embedded in the downscaled low-resolution (lr) counterparts in a nearly imperceptible form such that our aidn can further restore the original hr images solely from the lr images. the key to supporting arbitrary scale factors is our proposed conditional resampling module (crm) that conditions the downscaling/upscaling kernels and sampling locations on both scale factors and image content. extensive experimental results demonstrate that our aidn achieves top performance for invertible downscaling with both arbitrary integer and non-integer scale factors. also, both quantitative and qualitative evaluations show our aidn is robust to the lossy image compression standard. the source code and trained models are publicly available at https://github.com/doubiiu/aidn.",AB_0381
"unsupervised domain adaptation has limitations when encountering label discrepancy between the source and target domains. while open-set domain adaptation approaches can address situations when the target domain has additional categories, these methods can only detect them but not further classify them. in this paper, we focus on a more challenging setting dubbed domain adaptive zero-shot learning (dazsl), which uses semantic embeddings of class tags as the bridge between seen and unseen classes to learn the classifier for recognizing all categories in the target domain when only the supervision of seen categories in the source domain is available. the main challenge of dazsl is to perform knowledge transfer across categories and domain styles simultaneously. to this end, we propose a novel end-to-end learning mechanism dubbed three-way semantic consistent embedding (tsce) to embed the source domain, target domain, and semantic space into a shared space. specifically, tsce learns domain-irrelevant categorical prototypes from the semantic embedding of class tags and uses them as the pivots of the shared space. the source domain features are aligned with the prototypes via their supervised information. on the other hand, the mutual information maximization mechanism is introduced to push the target domain features and prototypes towards each other. by this way, our approach can align domain differences between source and target images, as well as promote knowledge transfer towards unseen classes. moreover, as there is no supervision in the target domain, the shared space may suffer from the catastrophic forgetting problem. hence, we further propose a ranking-based embedding alignment mechanism to maintain the consistency between the semantic space and the shared space. experimental results on both i2awa and i2webv clearly validate the effectiveness of our method. code is available at https://github.com/tiggers23/tsce-domain-adaptive-zero-shot-learning.",AB_0381
"transformer, the model of choice for natural language processing, has drawn scant attention from the medical imaging community. given the ability to exploit long-term dependencies, transformers are promising to help atypical convolutional neural networks to learn more contextualized visual representations. however, most of recently proposed transformer-based segmentation approaches simply treated transformers as assisted modules to help encode global context into convolutional representations. to address this issue, we introduce nnformer (i.e., not-another transformer), a 3d transformer for volumetric medical image segmentation. nnformer not only exploits the combination of interleaved convolution and self-attention operations, but also introduces local and global volume-based self-attention mechanism to learn volume representations. moreover, nnformer proposes to use skip attention to replace the traditional concatenation/summation operations in skip connections in u-net like architecture. experiments show that nnformer significantly outperforms previous transformer-based counterparts by large margins on three public datasets. compared to nnunet, the most widely recognized convnet-based 3d medical segmentation model, nnformer produces significantly lower hd95 and is much more computationally efficient. furthermore, we show that nnformer and nnunet are highly complementary to each other in model ensembling. codes and models of nnformer are available at https://git.io/jsf3i.",AB_0381
"video-language pre-training has attracted considerable attention recently for its promising performance on various downstream tasks. most existing methods utilize the modality-specific or modality-joint representation architectures for the cross-modality pre-training. different from previous methods, this paper presents a novel architecture named memory-augmented inter-modality bridge (membridge), which uses the learnable intermediate modality representations as the bridge for the interaction between videos and language. specifically, in the transformer-based cross-modality encoder, we introduce the learnable bridge tokens as the interaction approach, which means the video and language tokens can only perceive information from bridge tokens and themselves. moreover, a memory bank is proposed to store abundant modality interaction information for adaptively generating bridge tokens according to different cases, enhancing the capacity and robustness of the inter-modality bridge. through pre-training, membridge explicitly models the representations for more sufficient inter-modality interaction. comprehensive experiments show that our approach achieves competitive performance with previous methods on various downstream tasks including video-text retrieval, video captioning, and video question answering on multiple datasets, demonstrating the effectiveness of the proposed method. the code has been available at https://github.com/jahhaoyang/membridge.",AB_0381
"neurologically, filter pruning is a procedure of forgetting and remembering recovering. prevailing methods directly forget less important information from an unrobust baseline at first and expect to minimize the performance sacrifice. however, unsaturated base remembering imposes a ceiling on the slimmed model leading to suboptimal performance. and significantly forgetting at first would cause unrecoverable information loss. here, we design a novel filter pruning paradigm termed remembering enhancement and entropy-based asymptotic forgetting (reaf). inspired by robustness theory, we first enhance remembering by over-parameterizing baseline with fusible compensatory convolutions which liberates pruned model from the bondage of baseline at no inference cost. then the collateral implication between original and compensatory filters necessitates a bilateral-collaborated pruning criterion. specifically, only when the filter has the largest intra-branch distance and its compensatory counterpart has the strongest remembering enhancement power, they are preserved. further, ebbinghaus curve-based asymptotic forgetting is proposed to protect the pruned model from unstable learning. the number of pruned filters is increasing asymptotically in the training procedure, which enables the remembering of pretrained weights gradually to be concentrated in the remaining filters. extensive experiments demonstrate the superiority of reaf over many state-of-the-art (sota) methods. for example, reaf removes 47.55% flops and 42.98% parameters of resnet-50 only with 0.98% top-1 accuracy loss on imagenet. the code is available at https://github.com/zhangxin-xd/reaf.",AB_0381
"as an effective data augmentation method, mixup synthesizes an extra amount of samples through linear interpolations. despite its theoretical dependency on data properties, mixup reportedly performs well as a regularizer and calibrator contributing reliable robustness and generalization to deep model training. in this paper, inspired by universum learning which uses out-of-class samples to assist the target tasks, we investigate mixup from a largely under-explored perspective - the potential to generate in-domain samples that belong to none of the target classes, that is, universum. we find that in the framework of supervised contrastive learning, mixup-induced universum can serve as surprisingly high-quality hard negatives, greatly relieving the need for large batch sizes in contrastive learning. with these findings, we propose universum-inspired supervised contrastive learning (unicon), which incorporates mixup strategy to generate mixup-induced universum as universum negatives and pushes them apart from anchor samples of the target classes. we extend our method to the unsupervised setting, proposing unsupervised universum-inspired contrastive model (un-uni). our approach not only improves mixup with hard labels, but also innovates a novel measure to generate universum data. with a linear classifier on the learned representations, unicon shows state-of-the-art performance on various datasets. specially, unicon achieves 81.7% top-1 accuracy on cifar-100, surpassing the state of art by a significant margin of 5.2% with a much smaller batch size, typically, 256 in unicon vs. 1024 in supcon (khosla et al., 2020) using resnet-50. un-uni also outperforms sota methods on cifar-100. the code of this paper is released on https://github.com/hannaiiyanggit/unicon.",AB_0381
"perspective distortions and crowd variations make crowd counting a challenging task in computer vision. to tackle it, many previous works have used multi-scale architecture in deep neural networks (dnns). multi-scale branches can be either directly merged (e.g. by concatenation) or merged through the guidance of proxies (e.g. attentions) in the dnns. despite their prevalence, these combination methods are not sophisticated enough to deal with the per-pixel performance discrepancy over multi-scale density maps. in this work, we redesign the multi-scale neural network by introducing a hierarchical mixture of density experts, which hierarchically merges multi-scale density maps for crowd counting. within the hierarchical structure, an expert competition and collaboration scheme is presented to encourage contributions from all scales; pixel-wise soft gating nets are introduced to provide pixel-wise soft weights for scale combinations in different hierarchies. the network is optimized using both the crowd density map and the local counting map, where the latter is obtained by local integration on the former. optimizing both can be problematic because of their potential conflicts. we introduce a new relative local counting loss based on relative count differences among hard-predicted local regions in an image, which proves to be complementary to the conventional absolute error loss on the density map. experiments show that our method achieves the state-of-the-art performance on five public datasets, i.e. shanghaitech, ucf_cc_50, jhu-crowd++, nwpu-crowd and trancos. our codes will be available at https://github.com/zpdu/redesigning-multi-scale-neural-network-for-crowd-counting.",AB_0381
"neural video codecs have demonstrated great potential in video transmission and storage applications. existing neural hybrid video coding approaches rely on optical flow or gaussian-scale flow for prediction, which cannot support fine-grained adaptation to diverse motion content. towards more content-adaptive prediction, we propose a novel cross-scale prediction module that achieves more effective motion compensation. specifically, on the one hand, we produce a reference feature pyramid as prediction sources and then transmit cross-scale flows that leverage the feature scale to control the precision of prediction. on the other hand, for the first time, a weighted prediction mechanism is introduced even if only a single reference frame is available, which can help synthesize a fine prediction result by transmitting cross-scale weight maps. in addition to the cross-scale prediction module, we further propose a multi-stage quantization strategy, which improves the rate-distortion performance with no extra computational penalty during inference. we show the encouraging performance of our efficient neural video codec (envc) on several benchmark datasets. in particular, the proposed envc can compete with the latest coding standard h.266/vvc in terms of srgb psnr on uvg dataset for the low-latency mode. we also analyze in detail the effectiveness of the cross-scale prediction module in handling various video content, and provide a comprehensive ablation study to analyze those important components. test code is available at https://github.com/ustc-imcl/envc.",AB_0381
