AB,NO
"sketch-to-image synthesis aims to generate realistic images that match the input sketches or edge maps exactly. most known sketch-to-image synthesis methods use various generative adversarial networks (gans) that are trained with numerous pairs of sketches and real images. because of the convolution locality, the low-level layers of the generators in these gans lack global perception ability, causing feature maps derived from them easily to overlook global cues. since the global receptive field is crucial for acquiring the non-local structures and features of sketches, the absence of global contexts will impact the generation of high-quality images. some recent models turn to self-attention to construct global dependencies. however, they are not viable for large feature maps for the quadratic computational complexity concerning the size of feature maps. to address these problems, in this work, we propose sketch2photo - a new image synthesis approach that can capture global contexts as well as local features to generate photo-realistic images from weak or partial sketches or edge maps. we employ fast fourier convolution (ffc) residual blocks to create global receptive fields in the bottom layers of the network and incorporate swin transformer block (stb) units to obtain long-range global contexts for large-size feature maps efficiently. we also present an improved spatial attention pooling (isap) module to relax the strict alignment requirements between incomplete sketches and generated images. quantitative and qualitative experiments on multiple public datasets demonstrate the superiority of the proposed approach over many other sketch-to-image synthesis methods. the project code is available at https://github.com/hengliusky/skecth2photo.",AB_0018
"automatic registration of multiple point clouds is a significant preprocessing step for 3d computer vision tasks including semantic segmentation, 3d modelling, change detection, etc. many methods were proposed to deal with this problem and yet most of them are not fully utilizing the redundant information offered by multiple common overlaps among point clouds. the existing methods are also inefficient when dealing with large-scale point clouds. in this paper, a novel automatic registration framework is presented to align point clouds efficiently and robustly. first, the overall number of scans is grouped into several scan-blocks by a proposed blocking strategy, and we build the pairwise relationship among scans through a fully connected graph in each scan-block. second, perform loop-based coarse registration in each scan-block using a proposed false matches removal strategy. the proposed strategy can effectively identify grossly wrong scan-to-scan matches. third, the minimum spanning tree is extracted from the graph, and icp is applied along its edges. moreover, the lu-milios algorithm is used to further optimize all poses at once by utilizing all redundant information in each scan-block. finally, global block-to-block registration aligns all scan-blocks into a uniform coordinate reference. we test our framework on challenging whu-tls datasets, eth datasets, and robotic 3d scan datasets to evaluate the efficiency, accuracy, as well as robustness. the experiment results show that our method achieves the state-of -the-art accuracy, while the time performance is improved by more than 30% compared with the state-of-the-art algorithms. our source code is made available at https://github.com/wuhao-whu/hl-mrf for benchmarking purposes.",AB_0018
"localization of anatomical landmarks is essential for clinical diagnosis, treatment planning, and research. this paper proposes a novel deep network named feature aggregation and refinement network (farnet) for automatically detecting anatomical landmarks. farnet employs an encoder-decoder structure architecture. to alleviate the problem of limited training data in the medical domain, we adopt a backbone network pre-trained on natural images as the encoder. the decoder includes a multi-scale feature aggregation module for multi-scale feature fusion and a feature refinement module for high-resolution heatmap regression. coarse-to-fine supervisions are applied to the two modules to facilitate end-to-end training. we further propose a novel loss function named exponential weighted center loss for accurate heatmap regression, which focuses on the losses from the pixels near landmarks and suppresses the ones from far away. we evaluate farnet on three publicly available anatomical landmark detection datasets, including cephalometric, hand, and spine radiographs. our network achieves state-of-the-art performances on all three datasets. code is available at https://github.com/juvenileinwind/farnet.",AB_0018
"spatial-spectral classification (ssc) has become a trend for hyperspectral image (hsi) classification. however, most ssc methods mainly consider local information, so that some correlations may not be effectively discovered when they appear in regions that are not contiguous. although many ssc methods can acquire spatial-contextual characteristics via spatial filtering, they lack the ability to consider correlations in non-euclidean spaces. to address the aforementioned issues, we develop a new semisupervised hsi classification approach based on normalized spectral clustering with kernel-based learning (nsckl), which can aggregate local-to-global correlations to achieve a distinguishable embedding to improve hsi classification performance. in this work, we propose a normalized spectral clustering (nsc) scheme that can learn new features under a manifold assumption. specifically, we first design a kernel-based iterative filter (kif) to establish vertices of the undirected graph, aiming to assign initial connections to the nodes associated with pixels. the nsc first gathers local correlations in the euclidean space and then captures global correlations in the manifold. even though homogeneous pixels are distributed in noncontiguous regions, our nsc can still aggregate correlations to generate new (clustered) features. finally, the clustered features and a kernel-based extreme learning machine (kelm) are employed to achieve the semisupervised classification. the effectiveness of our nsckl is evaluated by using several hsis. when compared with other state-of-the-art (sota) classification approaches, our newly proposed nsckl demonstrates very competitive performance. the codes will be available at https://github.com/yuanchaosu/tcyb-nsckl.",AB_0018
"current artificial neural networks mainly conduct the learning process in the spatial domain but neglect the frequency domain learning. however, the learning course performed in the frequency domain can be more efficient than that in the spatial domain. in this paper, we fully explore frequency domain learning and propose a joint learning paradigm of frequency and spatial domains. this paradigm can take full advantage of the combined preponderances of frequency learning and spatial learning; specifically, frequency and spatial domain learning can effectively capture intrinsic global and local information, respectively. to achieve this, an innovative but effective linear learning block is proposed to conduct the learning process directly in the frequency domain. together with the prevailing spatial learning operation, i.e., convolution, a powerful and scalable joint learning framework is further proposed. exhaustive experiments on the diverse benchmark datasets - kitti, make3d, and cityscapes demonstrate the effectiveness and superiority of the proposed joint learning paradigm in dense image prediction tasks, including self-supervised depth estimation, ego-motion estimation, and semantic segmentation. in particular, the proposed model can achieve performance competitive to those of state-of-the-art methods in all three tasks, even without pretraining. moreover, the proposed model reduces the number of parameters by over 78% for self-supervised depth estimation on the kitti dataset while retaining the time complexity on par with other state-of-the-art methods; this provides a great chance to develop real-world applications. we hope that the proposed method can encourage more research in cross-domain learning. the codes are publicly available at https://github.com/shaochengjia/fslnet.",AB_0018
"as a vital image enhancement technology, infrared and visible image fusion aims to generate high-quality fused images with salient targets and abundant texture in extreme environments. however, current image fusion methods are all designed for infrared and visible images with normal illumination. in the night scene, existing methods suffer from weak texture details and poor visual perception due to the severe degradation in visible images, which affects subsequent visual applications. to this end, this paper advances a darkness -free infrared and visible image fusion method (divfusion), which reasonably lights up the darkness and facilitates complementary information aggregation. specifically, to improve the fusion quality of nighttime images, which suffer from low illumination, texture concealment, and color distortion, we first design a scene -illumination disentangled network (sidnet) to strip the illumination degradation in nighttime visible images while preserving informative features of source images. then, a texture-contrast enhancement fusion network (tcefnet) is devised to integrate complementary information and enhance the contrast and texture details of fused features. moreover, a color consistency loss is designed to mitigate color distortion from enhancement and fusion. finally, we fully consider the intrinsic relationship between low-light image enhancement and image fusion, achieving effective coupling and reciprocity. in this way, the proposed method is able to generate fused images with real color and significant contrast in an end-to-end manner. extensive experiments demonstrate that divfusion is superior to state-of-the-art algorithms in terms of visual quality and quantitative evaluations. particularly, low-light enhancement and dual-modal fusion provide more effective information to the fused image and boost high-level vision tasks. our code is publicly available at https://github.com/xinyu-xiang/divfusion.",AB_0018
"the cup-to-disc ratio (cdr) is one of the most significant indicator for glaucoma diagnosis. different from the use of costly fully supervised learning formulation with pixel-wise annotations in the literature, this study investigates the feasibility of accurate cdr measurement in fundus images using only tight bounding box supervision. for this purpose, we develop a two-task network named as cdrnet for accurate cdr measurement, one for weakly supervised image segmentation, and the other for bounding-box regression. the weakly supervised image segmentation task is implemented based on generalized multiple instance learning formulation and smooth maximum approximation, and the bounding-box regression task outputs class-specific bounding box prediction in a single scale at the original image resolution. to get accurate bounding box prediction, a class-specific bounding-box normalizer and an expected intersection-over-union are proposed. in the experiments, the proposed approach was evaluated by a testing set with 1200 images using cdr error and f-1 score for cdr measurement and dice coefficient for image segmentation. a grader study was conducted to compare the performance of the proposed approach with those of individual graders. the experimental results show that the proposed approach achieves cdr error of 0.0458 and f-1 score of 0.917 in cdr measurement and dice coefficients of 0.882 and 0.950 in optic cup and disc segmentation, respectively. these results indicate that the proposed approach outperforms the state-of-the-art performance obtained from the fully supervised image segmentation (fsis) approach using pixel-wise annotation for cdr measurement. its performance is also better than those of individual graders. in addition, the proposed approach gets performance close to the state-of-the-art obtained from fsis and the performance of individual graders for optic cup and disc segmentation. the codes are available at https://github.com/wangjuan313/cdrnet.",AB_0018
"although many negative sampling and generative adversarial network (gan) strategies have been applied to recommendation, this frequently encounter problems of poor interpretability and performance of negative sampling. to address these issues, we construct a recommendation model named ganrec, which organically integrates negative sampling and gans on a bipartite graph (bg) or knowledge graph (kg).the primary purpose of ganrec is to train a discriminator (i.e., a recommender) by generating high -quality negative samples that satisfy the perspective of graph theory, core idea of collaborative filtering, and distribution of positive interactions. the generator attempts to produce, distinguish, and select an informative and knowledge-aware negative item that can reflect the real needs of an user. experiments on the amazon-book, last-fm, and yelp2018 datasets show that the recommendation performance of the proposed method not only exceeds the kg-enhanced models, but also is better than those of state-of-the-art sampling methods. additionally, when no external knowledge is available, ganrec outperforms some bg-enhanced recommendation methods. our codes are available in https://github.com/yangzhi22/ganrec.",AB_0018
"brain tumor segmentation in multimodal mri has great significance in clinical diagnosis and treatment. the utilization of multimodal information plays a crucial role in brain tumor segmentation. however, most existing methods focus on the extraction and selection of deep semantic features, while ignoring some features with specific meaning and importance to the segmentation problem. in this paper, we propose a brain tumor segmentation method based on the fusion of deep semantics and edge information in multimodal mri, aiming to achieve a more sufficient utilization of multimodal information for accurate segmentation. the proposed method mainly consists of a semantic segmentation module, an edge detection module and a feature fusion module. in the semantic segmentation module, the swin transformer is adopted to extract semantic features and a shifted patch tokenization strategy is introduced for better training. the edge detection module is designed based on convolutional neural networks (cnns) and an edge spatial attention block (esab) is presented for feature enhancement. the feature fusion module aims to fuse the extracted semantic and edge features, and we design a multi-feature inference block (mfib) based on graph convolution to perform feature reasoning and information dissemination for effective feature fusion. the proposed method is validated on the popular brats benchmarks. the experimental results verify that the proposed method outperforms a number of state-of-the-art brain tumor segmentation methods. the source code of the proposed method is available at https://github.com/hxy-99/brats.",AB_0018
"multi-view subspace clustering aims to discover the hidden subspace structures from multiple views for robust clustering, and has been attracting considerable attention in recent years. despite significant progress, most of the previous multi-view subspace clustering algorithms are still faced with two limitations. first, they usually focus on the consistency (or commonness) of multiple views, yet often lack the ability to capture the cross-view inconsistencies in subspace representations. second, many of them overlook the local structures of multiple views and cannot jointly leverage multiple local structures to enhance the subspace representation learning. to address these two limitations, in this paper, we propose a jointly smoothed multi-view subspace clustering (jsmc) approach. specifically, we simultaneously incorporate the cross-view commonness and inconsistencies into the subspace representation learning. the view-consensus grouping effect is presented to jointly exploit the local structures of multiple views to regularize the view-commonness representation, which is further associated with the low-rank constraint via the nuclear norm to strengthen its cluster structure. thus the cross-view commonness and inconsistencies, the view-consensus grouping effect, and the low-rank representation are seamlessly incorporated into a unified objective function, upon which an alternating optimization algorithm is performed to achieve a robust subspace representation for clustering. experimental results on a variety of real-world multi-view datasets confirm the superiority of our approach. code available: https://github.com/huangdonghere/jsmc.",AB_0018
