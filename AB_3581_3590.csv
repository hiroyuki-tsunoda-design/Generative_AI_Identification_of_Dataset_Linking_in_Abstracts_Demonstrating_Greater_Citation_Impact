AB,NO
"modelling long-range dependencies is critical for scene understanding tasks in computer vision. although convolution neural networks (cnns) have excelled in many vision tasks, they are still limited in capturing long-range structured relationships as they typically consist of layers of local kernels. a fully-connected graph, such as the self-attention operation in transformers, is beneficial for such modelling, however, its computational overhead is prohibitive. in this paper, we propose a dynamic graph message passing network, that significantly reduces the computational complexity compared to related works modelling a fully-connected graph. this is achieved by adaptively sampling nodes in the graph, conditioned on the input, for message passing. based on the sampled nodes, we dynamically predict node-dependent filter weights and the affinity matrix for propagating information between them. this formulation allows us to design a self-attention module, and more importantly a new transformer-based backbone network, that we use for both image classification pretraining, and for addressing various downstream tasks (e.g. object detection, instance and semantic segmentation). using this model, we show significant improvements with respect to strong, state-of-the-art baselines on four different tasks. our approach also outperforms fully-connected graphs while using substantially fewer floating-point operations and parameters. code and models will be made publicly available at https://github.com/fudan-zvg/dgmn2.",AB_0359
"blind face restoration is a challenging task due to the unknown, unsynthesizable and complex degradation, yet is valuable in many practical applications. to improve the performance of blind face restoration, recent works mainly treat the two aspects, i.e., generic and specific restoration, separately. in particular, generic restoration attempts to restore the results through general facial structure prior, while on the one hand, cannot generalize to real-world degraded observations due to the limited capability of direct cnns' mappings in learning blind restoration, and on the other hand, fails to exploit the identity-specific details. on the contrary, specific restoration aims to incorporate the identity features from the reference of the same identity, in which the requirement of proper reference severely limits the application scenarios. generally, it is a challenging and intractable task to improve the photo-realistic performance of blind restoration and adaptively handle the generic and specific restoration scenarios with a single unified model. instead of implicitly learning the mapping from a low-quality image to its high-quality counterpart, this paper suggests a dmdnet by explicitly memorizing the generic and specific features through dual dictionaries. first, the generic dictionary learns the general facial priors from high-quality images of any identity, while the specific dictionary stores the identity-belonging features for each person individually. second, to handle the degraded input with or without specific reference, dictionary transform module is suggested to read the relevant details from the dual dictionaries which are subsequently fused into the input features. finally, multi-scale dictionaries are leveraged to benefit the coarse-to-fine restoration. the whole framework including the generic and specific dictionaries is optimized in an end-to-end manner and can be flexibly plugged into different application scenarios. moreover, a new high-quality dataset, termed celebref-hq, is constructed to promote the exploration of specific face restoration in the high-resolution space. experimental results demonstrate that the proposed dmdnet performs favorably against the state of the arts in both quantitative and qualitative evaluation, and generates more photo-realistic results on the real-world low-quality images. the codes, models and the celebref-hq dataset will be publicly available at https://github.com/csxmli2016/dmdnet.",AB_0359
"an efficient 3d point cloud learning architecture, named efficientlo-net, for lidar odometry is first proposed in this article. in this architecture, the projection-aware representation of the 3d point cloud is proposed to organize the raw 3d point cloud into an ordered data form to achieve efficiency. the pyramid, warping, and cost volume (pwc) structure for the lidar odometry task is built to estimate and refine the pose in a coarse-to-fine approach. a projection-aware attentive cost volume is built to directly associate two discrete point clouds and obtain embedding motion patterns. then, a trainable embedding mask is proposed to weigh the local motion patterns to regress the overall pose and filter outlier points. the trainable pose warp-refinement module is iteratively used with embedding mask optimized hierarchically to make the pose estimation more robust for outliers. the entire architecture is holistically optimized end-to-end to achieve adaptive learning of cost volume and mask, and all operations involving point cloud sampling and grouping are accelerated by projection-aware 3d feature learning methods. the superior performance and effectiveness of our lidar odometry architecture are demonstrated on kitti, m2dgr, and argoverse datasets. our method outperforms all recent learning-based methods and even the geometry-based approach, loam with mapping optimization, on most sequences of kitti odometry dataset. we open sourced our codes at: https://github.com/irmvlab/efficientlo-net.",AB_0359
"video instance segmentation (vis) is a new and inherently multi-task problem, which aims to detect, segment, and track each instance in a video sequence. existing approaches are mainly based on single-frame features or single-scale features of multiple frames, where either temporal information or multi-scale information is ignored. to incorporate both temporal and scale information, we propose a temporal pyramid routing (tpr) strategy to conditionally align and conduct pixel-level aggregation from a feature pyramid pair of two adjacent frames. specifically, tpr contains two novel components, including dynamic aligned cell routing (dacr) and cross pyramid routing (cpr), where dacr is designed for aligning and gating pyramid features across temporal dimension, while cpr transfers temporally aggregated features across scale dimension. moreover, our approach is a light-weight and plug-and-play module and can be easily applied to existing instance segmentation methods. extensive experiments on three datasets including youtube-vis (2019, 2021) and cityscapes-vps demonstrate the effectiveness and efficiency of the proposed approach on several state-of-the-art instance and panoptic segmentation methods. codes will be publicly available at https://github.com/lxtgh/temporalpyramidrouting.",AB_0359
"semantic image synthesis, translating semantic layouts to photo-realistic images, is a one-to-many mapping problem. though impressive progress has been recently made, diverse semantic synthesis that can efficiently produce semantic-level or even instance-level multimodal results, still remains a challenge. in this article, we propose a novel diverse semantic image synthesis framework from the perspective of semantic class distributions, which naturally supports diverse generation at both semantics and instance level. we achieve this by modeling class-level conditional modulation parameters as continuous probability distributions instead of discrete values, and sampling per-instance modulation parameters through instance-adaptive stochastic sampling that is consistent across the network. moreover, we propose prior noise remapping, through linear perturbation parameters encoded from paired references, to facilitate supervised training and exemplar-based instance style control at test time. to further extend the user interaction function of the proposed method, we also introduce sketches into the network. in addition, specially designed generator modules, progressive growing module and multi-scale refinement module, can be used as a general module to improve the performance of complex scene generation. extensive experiments on multiple datasets show that our method can achieve superior diversity and comparable quality compared to state-of-the-art methods. codes are available at https://github.com/tzt101/inade.git.",AB_0359
"in this article, we propose a new distortion quantification method for point clouds, the multiscale potential energy discrepancy (mped). currently, there is a lack of effective distortion quantification for a variety of point cloud perception tasks. specifically, in human vision tasks, a distortion quantification method is used to predict human subjective scores and optimize the selection of human perception task parameters, such as dense point cloud compression and enhancement. in machine vision tasks, a distortion quantification method usually serves as loss function to guide the training of deep neural networks for unsupervised learning tasks (e.g., sparse point cloud reconstruction, completion, and upsampling). therefore, an effective distortion quantification should be differentiable, distortion discriminable, and have low computational complexity. however, current distortion quantification cannot satisfy all three conditions. to fill this gap, we propose a new point cloud feature description method, the point potential energy (ppe), inspired by classical physics. we regard the point clouds are systems that have potential energy and the distortion can change the total potential energy. by evaluating various neighborhood sizes, the proposed mped achieves global-local tradeoffs, capturing distortion in a multiscale fashion. we further theoretically show that classical chamfer distance is a special case of our mped. extensive experiments show that the proposed mped is superior to current methods on both human and machine perception tasks. our code is available at https://github.com/qi-yangsjtu/mped.",AB_0359
"binary neural networks (bnns) have attracted broad research interest due to their efficient storage and computational ability. nevertheless, a significant challenge of bnns lies in handling discrete constraints while ensuring bit entropy maximization, which typically makes their weight optimization very difficult. existing methods relax the learning using the sign function, which simply encodes positive weights into +1s, and -1s otherwise. alternatively, we formulate an angle alignment objective to constrain the weight binarization to {0; +1} to solve the challenge. in this article, we show that our weight binarization provides an analytical solution by encoding high-magnitude weights into +1s, and 0s otherwise. therefore, a high-quality discrete solution is established in a computationally efficient manner without the sign function. we prove that the learned weights of binarized networks roughly follow a laplacian distribution that does not allow entropy maximization, and further demonstrate that it can be effectively solved by simply removing the l(2) regularization during network training. our method, dubbed sign-to-magnitude network binarization (siman), is evaluated on cifar-10 and imagenet, demonstrating its superiority over the sign-based state-of-the-arts. our source code, experimental settings, training logs and binary models are available at https://github. com/lmbxmu/siman",AB_0359
"multimodal fusion and multitask learning are two vital topics in machine learning. despite the fruitful progress, existing methods for both problems are still brittle to the same challenge-it remains dilemmatic to integrate the common information across modalities (resp. tasks) meanwhile preserving the specific patterns of each modality (resp. task). besides, while they are actually closely related to each other, multimodal fusion and multitask learning are rarely explored within the same methodological framework before. in this paper, we propose channel-exchanging-network (cen) which is self-adaptive, parameter-free, and more importantly, applicable for multimodal and multitask dense image prediction. at its core, cen adaptively exchanges channels between subnetworks of different modalities. specifically, the channel exchanging process is self-guided by individual channel importance that is measured by the magnitude of batch-normalization (bn) scaling factor during training. for the application of dense image prediction, the validity of cen is tested by four different scenarios: multimodal fusion, cycle multimodal fusion, multitask learning, and multimodal multitask learning. extensive experiments on semantic segmentation via rgb-d data and image translation through multi-domain input verify the effectiveness of cen compared to state-of-the-art methods. detailed ablation studies have also been carried out, which demonstrate the advantage of each component we propose. our code is available at https://github.com/yikaiw/cen.",AB_0359
"alternatively inferring on the visual facts and commonsense is fundamental for an advanced visual question answering (vqa) system. this ability requires models to go beyond the literal understanding of commonsense. the system should not just treat objects as the entrance to query background knowledge, but fully ground commonsense to the visual world and imagine the possible relationships between objects, e.g., fork, can lift, food. to comprehensively evaluate such abilities, we propose a vqa benchmark, compositional reasoning on vision and commonsense(cric), which introduces new types of questions about cric, and an evaluation metric integrating the correctness of answering and commonsense grounding. to collect such questions and rich additional annotations to support the metric, we also propose an automatic algorithm to generate question samples from the scene graph associated with the images and the relevant knowledge graph. we further analyze several representative types of vqa models on the cric dataset. experimental results show that grounding the commonsense to the image region and joint reasoning on vision and commonsense are still challenging for current approaches. the dataset is available at https://cricvqa.github.io.",AB_0359
"recently, vision transformers (vits) have been broadly explored in visual recognition. with low efficiency in encoding fine-level features, the performance of vits is still inferior to the state-of-the-art cnns when trained from scratch on a midsize dataset like imagenet. through experimental analysis, we find it is because of two reasons: 1) the simple tokenization of input images fails to model the important local structure such as edges and lines, leading to low training sample efficiency; 2) the redundant attention backbone design of vits leads to limited feature richness for fixed computation budgets and limited training samples. to overcome such limitations, we present a new simple and generic architecture, termed vision outlooker (volo), which implements a novel outlook attention operation that dynamically conduct the local feature aggregation mechanism in a sliding window manner across the input image. unlike self-attention that focuses on modeling global dependencies of local features at a coarse level, our outlook attention targets at encoding finer-level features, which is critical for recognition but ignored by self-attention. outlook attention breaks the bottleneck of self-attention whose computation cost scales quadratically with the input spatial dimension, and thus is much more memory efficient. compared to our tokens-to-token vision transformer (t2t-vit), volo can more efficiently encode fine-level features that are essential for high-performance visual recognition. experiments show that with only 26.6 m learnable parameters, volo achieves 84.2% top-1 accuracy on imagenet-1 k without using extra training data, 2.7% better than t2t-vit with a comparable number of parameters. when the model size is scaled up to 296 m parameters, its performance can be further improved to 87.1%, setting a new record for imagenet-1 k classification. in addition, we also take the proposed volo as pretrained models and report superior performance on downstream tasks, such as semantic segmentation. code is available at https://github.com/sail-sg/volo.",AB_0359
