AB,NO
"convolutional neural networks (cnns) dominate image processing but suffer from local inductive bias, which is addressed by the transformer framework with its inherent ability to capture global context through self-attention mechanisms. however, how to inherit and integrate their advantages to improve compressed sensing is still an open issue. this paper proposes csformer, a hybrid framework to explore the representation capacity of local and global features. the proposed approach is well-designed for end-to-end compressive image sensing, composed of adaptive sampling and recovery. in the sampling module, images are measured block-by-block by the learned sampling matrix. in the reconstruction stage, the measurements are projected into an initialization stem, a cnn stem, and a transformer stem. the initialization stem mimics the traditional reconstruction of compressive sensing but generates the initial reconstruction in a learnable and efficient manner. the cnn stem and transformer stem are concurrent, simultaneously calculating fine-grained and long-range features and efficiently aggregating them. furthermore, we explore a progressive strategy and window-based transformer block to reduce the parameters and computational complexity. the experimental results demonstrate the effectiveness of the dedicated transformer-based architecture for compressive sensing, which achieves superior performance compared to state-of-the-art methods on different datasets. our codes is available at: https://github.com/lineves7/csformer.",AB_0383
"multi-view action recognition aims to identify action categories from given clues. existing studies ignore the negative influences of fuzzy views between view and action in disentangling, commonly arising the mistaken recognition results. to this end, we regard the observed image as the composition of the view and action components, and give full play to the advantages of multiple views via the adaptive cooperative representation among these two components, forming a dual-recommendation disentanglement network (drdn) for multi-view action recognition. specifically, 1) for the action, we leverage a multi-level specific information recommendation (sir) to enhance the interaction among intricate activities and views. sir offers a more comprehensive representation of activities, measuring the trade-off between global and local information. 2) for the view, we utilize a pyramid dynamic recommendation (pdr) to learn a complete and detailed global representation by transferring features from different views. it is explicitly restricted to resist the fuzzy noise influence, focusing on positive knowledge from other views. our drdn aims for complete action and view representation, where pdr directly guides action to disentangle with view features and sir considers mutual exclusivity of view and action clues. extensive experiments have indicated that the multi-view action recognition method drdn we proposed achieves state-of-the-art performance over powerful competitors on several standard benchmarks. the code will be available at https://github.com/51cloud/drdn.",AB_0383
"multi-view stereo (mvs) aims to reconstruct a 3d point cloud model from multiple views. in recent years, learning-based mvs methods have received a lot of attention and achieved excellent performance compared with traditional methods. however, these methods still have apparent shortcomings, such as the accumulative error in the coarse-to-fine strategy and the inaccurate depth hypotheses based on the uniform sampling strategy. in this paper, we propose the nr-mvsnet, a coarse-to-fine structure with the depth hypotheses based on the normal consistency (dhnc) module, and the depth refinement with reliable attention (drra) module. specifically, we design the dhnc module to generate more effective depth hypotheses, which collects the depth hypotheses from neighboring pixels with the same normals. as a result, the predicted depth can be smoother and more accurate, especially in texture-less and repetitive-texture regions. on the other hand, we update the initial depth map in the coarse stage by the drra module, which can combine attentional reference features and cost volume features to improve the depth estimation accuracy in the coarse stage and address the accumulative error problem. finally, we conduct a series of experiments on the dtu, blendedmvs, tanks & temples, and eth3d datasets. the experimental results demonstrate the efficiency and robustness of our nr-mvsnet compared with the state-of-the-art methods. our implementation is available at https://github.com/wdkyh/nr-mvsnet.",AB_0383
"single-image deraining aims to restore the image that is degraded by the rain streaks, where the long-standing bottleneck lies in how to disentangle the rain streaks from the given rainy image. despite the progress made by substantial existing works, several crucial questions - e.g., how to distinguish rain streaks and clean image, while how to disentangle rain streaks from low-frequency pixels, and further prevent the blurry edges - have not been well investigated. in this paper, we attempt to solve all of them under one roof. our observation is that rain streaks are bright stripes with higher pixel values that are evenly distributed in each color channel of the rainy image, while the disentanglement of the high-frequency rain streaks is equivalent to decreasing the standard deviation of the pixel distribution for the rainy image. to this end, we propose a self-supervised rain streaks learning network to characterize the similar pixel distribution of the rain streaks from a macroscopic viewpoint over various low-frequency pixels of gray-scale rainy images, coupling with a supervised rain streaks learning network to explore the specific pixel distribution of the rain streaks from a microscopic viewpoint between each paired rainy and clean images. building on this, a self-attentive adversarial restoration network comes up to prevent the further blurry edges. these networks compose an end-to-end macroscopic-and-microscopic rain streaks disentanglement network, named (mrsd)-r-2-net, to learn rain streaks, which is further removed for single image deraining. the experimental results validate its advantages on deraining benchmarks against the state-of-the-arts. the code is available at: https://github.com/xinjiangaohfut/mmrsd-net",AB_0383
"hyperspectral image (hsi) classification is challenging due to spatial variability caused by complex imaging conditions. prior methods suffer from limited representation ability, as they train specially designed networks from scratch on limited annotated data. we propose a tri-spectral image generation pipeline that transforms hsi into high-quality tri-spectral images, enabling the use of off-the-shelf imagenet pretrained backbone networks for feature extraction. motivated by the observation that there are many homogeneous areas with distinguished semantic and geometric properties in hsis, which can be used to extract useful contexts, we propose an end-to-end segmentation network named dcn-t. it adopts transformers to effectively encode regional adaptation and global aggregation spatial contexts within and between the homogeneous areas discovered by similarity-based clustering. to fully exploit the rich spectrums of the hsi, we adopt an ensemble approach where all segmentation results of the tri-spectral images are integrated into the final prediction through a voting scheme. extensive experiments on three public benchmarks show that our proposed method outperforms state-of-the-art methods for hsi classification. the code will be released at https://github.com/dotwang/dcn-t.",AB_0383
"robust keypoint detection on omnidirectional images against large perspective variations, is a key problem in many computer vision tasks. in this paper, we propose a perspectively equivariant keypoint learning framework named omnikl for addressing this problem. specifically, the framework is composed of a perspective module and a spherical module, each one including a keypoint detector specific to the type of the input image and a shared descriptor providing uniform description for omnidirectional and perspective images. in these detectors, we propose a differentiable candidate position sorting operation for localizing keypoints, which directly sorts the scores of the candidate positions in a differentiable manner and returns the globally top-k keypoints on the image. this approach does not break the differentiability of the two modules, thus they are end-to-end trainable. moreover, we design a novel training strategy combining the self-supervised and co-supervised methods to train the framework without any labeled data. extensive experiments on synthetic and real-world 360 degrees image datasets demonstrate the effectiveness of omnikl in detecting perspectively equivariant keypoints on omnidirectional images. our source code are available online at https://github.com/vandeppce/sphkpt.",AB_0383
"multi-codebook quantization (mcq) is a generalized version of existing codebook-based quantizations for approximate nearest neighbor (ann) search. specifically, mcq picks one codeword for each sub-codebook independently and takes the sum of picked codewords to approximate the original vector. the objective function involves no constraints, therefore, mcq theoretically has the potential to achieve the best performance because solutions of other codebook-based quantization methods are all covered by mcq's solution space under the same codebook size setting. however, finding the optimal solution to mcq is proved to be np-hard due to its encoding process, i.e., converting an input vector to a binary code. to tackle this, researchers apply constraints to it to find near-optimal solutions or employ heuristic algorithms that are still time-consuming for encoding. different from previous approaches, this paper takes the first attempt to find a deep solution to mcq. the encoding network is designed to be as simple as possible, so the very complex encoding problem becomes simply a feed-forward. compared with other methods on three datasets, our method shows state-of-the-art performance. notably, our method is $11\times $ - $38\times $ faster than heuristic algorithms for encoding, which makes it more practical for the real scenery of large-scale retrieval. our code is publicly available: https://github.com/deepmcq/deepq.",AB_0383
"change captioning is to describe the fine-grained change between a pair of images. the pseudo changes caused by viewpoint changes are the most typical distractors in this task, because they lead to the feature perturbation and shift for the same objects and thus overwhelm the real change representation. in this paper, we propose a viewpoint-adaptive representation disentanglement network to distinguish real and pseudo changes, and explicitly capture the features of change to generate accurate captions. concretely, a position-embedded representation learning is devised to facilitate the model in adapting to viewpoint changes via mining the intrinsic properties of two image representations and modeling their position information. to learn a reliable change representation for decoding into a natural language sentence, an unchanged representation disentanglement is designed to identify and disentangle the unchanged features between the two position-embedded representations. extensive experiments show that the proposed method achieves the state-of-the-art performance on the four public datasets. the code is available at https://github.com/tuyunbin/vard.",AB_0383
"low-light images incur several complicated degradation factors such as poor brightness, low contrast, color degradation, and noise. most previous deep learning-based approaches, however, only learn the mapping relationship of single channel between the input low-light images and the expected normal-light images, which is insufficient enough to deal with low-light images captured under uncertain imaging environment. moreover, too deeper network architecture is not conducive to recover low-light images due to extremely low values in pixels. to surmount aforementioned issues, in this paper we propose a novel multi-branch and progressive network (mbpnet) for low-light image enhancement. to be more specific, the proposed mbpnet is comprised of four different branches which build the mapping relationship at different scales. the followed fusion is performed on the outputs obtained from four different branches for the final enhanced image. furthermore, to better handle the difficulty of delivering structural information of low-light images with low values in pixels, a progressive enhancement strategy is applied in the proposed method, where four convolutional long short-term memory networks (lstm) are embedded in four branches and an recurrent network architecture is developed to iteratively perform the enhancement process. in addition, a joint loss function consisting of the pixel loss, the multi-scale perceptual loss, the adversarial loss, the gradient loss, and the color loss is framed to optimize the model parameters. to evaluate the effectiveness of proposed mbpnet, three popularly used benchmark databases are used for both quantitative and qualitative assessments. the experimental results confirm that the proposed mbpnet obviously outperforms other state-of-the-art approaches in terms of quantitative and qualitative results. the code will be available at https://github.com/kbzhang0505/mbpnet.",AB_0383
"we present twist, a simple and theoretically explainable self-supervised representation learning method by classifying large-scale unlabeled datasets in an end-to-end way. we employ a siamese network terminated by a softmax operation to produce twin class distributions of two augmented images. without supervision, we enforce the class distributions of different augmentations to be consistent. however, simply minimizing the divergence between augmentations will generate collapsed solutions, i.e., outputting the same class distribution for all images. in this case, little information about the input images is preserved. to solve this problem, we propose to maximize the mutual information between the input image and the output class predictions. specifically, we minimize the entropy of the distribution for each sample to make the class prediction assertive, and maximize the entropy of the mean distribution to make the predictions of different samples diverse. in this way, twist can naturally avoid the collapsed solutions without specific designs such as asymmetric network, stop-gradient operation, or momentum encoder. as a result, twist outperforms previous state-of-the-art methods on a wide range of tasks. specifically on the semi-supervised classification task, twist achieves 61.2% top-1 accuracy with 1% imagenet labels using a resnet-50 as backbone, surpassing previous best results by an improvement of 6.2%. codes and pre-trained models are available at https://github.com/bytedance/twist",AB_0383
