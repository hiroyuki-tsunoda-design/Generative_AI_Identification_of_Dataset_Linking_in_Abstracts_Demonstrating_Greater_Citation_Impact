AB,NO
"a major responsibility of radiologists in routine clinical practice is to read follow-up chest radiographs (cxrs) to identify changes in a patient's condition. diagnosing meaningful changes in follow-up cxrs is challenging because radiologists must differentiate disease changes from natural or benign variations. here, we suggest using a multi-task siamese convolutional vision transformer (music-vit) with an anatomy-matching module (amm) to mimic the radiologist's cognitive process for differentiating baseline change from no-change. music-vit uses the convolutional neural networks (cnns) meet vision transformers model that combines cnn and transformer architecture. it has three major components: a siamese network architecture, an amm, and multi-task learning. because the input is a pair of cxrs, a siamese network was adopted for the encoder. the amm is an attention module that focuses on related regions in the cxr pairs. to mimic a radiologist's cognitive process, music-vit was trained using multi-task learning, normal/abnormal and change/no-change classification, and anatomymatching. among 406 k cxrs studied, 88 k change and 115 k no-change pairs were acquired for the training dataset. the internal validation dataset consisted of 1,620 pairs. to demonstrate the robustness of music-vit, we verified the results with two other validation datasets. music-vit respectively achieved accuracies and area under the receiver operating characteristic curves of 0.728 and 0.797 on the internal validation dataset, 0.614 and 0.784 on the first external validation dataset, and 0.745 and 0.858 on a second temporally separated validation dataset. all code is available at https://github.com/chokyungjin/music-vit.",AB_0091
"we develop the interface tool wan2respack, which connects respack (software that derives the low energy effective hamiltonians of solids) with wannier90 (software that constructs wannier functions). wan2respack converts the wannier functions obtained by wannier90 into those used in respack, which is then used to derive the low-energy effective hamiltonians of solids. in this paper, we explain the basic usage of wan2respack and show its application to standard compounds of correlated materials, namely, the correlated metal srvo3 and the high-tc superconductor la2cuo4. furthermore, we compare the low-energy effective hamiltonians of these compounds using wannier functions obtained by wannier90 and those obtained by respack. we confirm that both types of wannier functions give the same hamiltonians. this benchmark comparison demonstrates that wan2respack correctly converts wannier functions in the wannier90 format into those in the respack format. program summary program title: wan2respack cpc library link to program files: https://doi .org /10 .17632 /6zfj2dkv5b .1 licensing provisions: gnu general public license version 3 programming language: fortran and python3 external routines/libraries: quantum espresso (version 6.6), wannier90 (version 3.0.0), respack (version 20200113), tomli. nature of problem: using respack, one can derive low-energy effective hamiltonians of solids from maximally localized wannier functions. however, due to the differences in the representation of wannier functions, the wannier functions obtained by wannier90 cannot be directly used in respack. solution method: wan2respack converts the wannier functions in the wannier90 format into those in the respack format. using the converted wannier functions, one can derive the low-energy effective hamiltonians using respack. & copy; 2023 elsevier b.v. all rights reserved.",AB_0091
"backdoor watermarking is a promising paradigm to protect the copyright of deep neural network (dnn) models. in the existing works on this subject, researchers have intensively focused on watermarking robustness, while the concept of fidelity, which is concerned with the preservation of the model's original functionality, has received less attention. in this paper, focusing on deep image classification models, we show that the existing shared notion of the sole measurement of learning accuracy is inadequate to characterize backdoor fidelity. meanwhile, we show that the analogous concept of embedding distortion in multimedia watermarking, interpreted as the total weight loss (twl) in dnn backdoor watermarking, is also problematic for fidelity measurement. to address this challenge, we propose the concept of deep fidelity, which states that the backdoor watermarked dnn model should preserve both the feature representation and decision boundary of the unwatermarked host model. to achieve deep fidelity, we propose two loss functions termed penultimate feature loss (pfl) and softmax probability-distribution loss (spl) to preserve feature representation, while the decision boundary is preserved by the proposed fix last layer (fixll) treatment, inspired by the recent discovery that deep learning with a fixed classifier causes no loss of learning accuracy. with the above designs, both embedding from scratch and fine-tuning strategies are implemented to evaluate the deep fidelity of backdoor embedding, whose advantages over the existing methods are verified via experiments using resnet18 for mnist and cifar-10 classifications, and wide residual network (i.e., wrn28_10) for cifar-100 task. pytorch codes are available at https://github.com/ghua-ac/dnn_watermark.",AB_0091
"in this paper, a new and effective meta-heuristic method named corona-virus search optimizer (cvso) is proposed inspired based on the movement and search of the corona-virus among different societies, prevalence, and death of individuals. the cvso has local and global search suitable balance due to the use of evolutionary strategies as well as appropriate social learning. this algorithm has desirable optimization power, easy implementation and requires only a control parameter. the cvso algorithm is implemented on 30 modern and standard test functions of cec2014 and also 10 test functions of cec2019. the performance and capability of cvso is compared with new and well-known meta-heuristics including particle swarm optimization (pso), artificial bee colony (abc) tree growth algorithm (tga), and lion optimization algorithm (loa), bat algorithm (ba) and harris hawks optimization (hho). the results showed that the cvso can be more effective than other algorithms to solve the test functions and also various complex real-world engineering optimization problems in terms of optimization accuracy and convergence rate. also, the superiority of cvso is demonstrated in the designing of a hybrid energy system by achieving a lower cost of m$ 2.7492 compared to tga, loa, abc, pso, ba, and hho algorithms, which achieve costs of m$ 2.7878, m$ 2.7556, m$ 2.8521, m$ 2.9503, m$ 3.0942, and m$ 2.9614, respectively. in addition, due to the optimal balance between local and global search and strength, it is able to avoid getting stuck in locally optimal solutions while obtaining global optimal solutions with a higher convergence rate than other algorithms. the source code of cvso is publicly available at https://github.com/ira jfaraji/cvso.",AB_0091
"our recent study has found that physics-informed neural networks (pinn) tend to be local approximators after training. this observation led to the development of a novel physics-informed radial basis network (pirbn), which is capable of maintaining the local approximating property throughout the entire training process. unlike deep neural networks, a pirbn comprises only one hidden layer and a radial basis activation function. under appropriate conditions, we demonstrated that the training of pirbns using gradient descendent methods can converge to gaussian processes. besides, we studied the training dynamics of pirbn via the neural tangent kernel (ntk) theory. in addition, comprehensive investigations regarding the initialisation strategies of pirbn were conducted. numerical examples demonstrated that pirbn is more effective than pinn in solving nonlinear partial differential equations with high-frequency features and ill-posed computational domains. moreover, the existing pinn numerical techniques, such as adaptive learning, decomposition and different types of loss functions, are applicable to pirbn. the programs that can regenerate all numerical results are available at https://github.com/jinshuaibai/pirbn.& copy; 2023 the author(s). published by elsevier b.v. this is an open access article under the cc by-nc-nd license ().",AB_0091
"realistic benchmarks of reproducible bugs and fixes are vital to good experimental evaluation of debugging and testing approaches. however, there is no suitable bug benchmark suite that can systematically evaluate the debugging and testing methods of quantum programs until now. this paper proposes bugs4q, a benchmark of forty-two real, manually validated qiskit bugs from three popular platforms (github, stackoverflow, and stack exchange) in programming, supplemented with test cases to reproduce buggy behaviors. bugs4q also provides interfaces for accessing the buggy and fixed versions of the qiskit programs and executing the corresponding source code and unit tests, facilitating the reproducible empirical studies and comparisons of qiskit program debugging and testing tools. bugs4q is publicly available at https://github.com/z-928/bugs4q-framework. editor's note: open science material was validated by the journal of systems and software open science board.",AB_0091
"cyber-physical systems (cpss) have been widely adopted in various industry domains to support many important tasks that impact our daily lives, such as automotive vehicles, robotics manufacturing, and energy systems. as artificial intelligence (ai) has demonstrated its promising abilities in diverse tasks like decision-making, prediction, and optimization, a growing number of cpss adopt ai components in the loop to further extend their efficiency and performance. however, these modern ai-enabled cpss have to tackle pivotal problems that the ai-enabled control systems might need to compensate the balance across multiple operation requirements and avoid possible defections in advance to safeguard human lives and properties. modular redundancy and ensemble method are two widely adopted solutions in the traditional cpss and ai communities to enhance the functionality and flexibility of a system. nevertheless, there is a lack of deep understanding of the effectiveness of such ensemble design on ai-cpss across diverse industrial applications. considering the complexity of ai-cpss, existing ensemble methods fall short of handling such huge state space and sophisticated system dynamics. furthermore, an ideal control solution should consider the multiple system specifications in real-time and avoid erroneous behaviors beforehand. such that, a new specification-oriented ensemble control system is of urgent need for ai-cpss. in this paper, we propose siege, a semantics-guided ensemble control framework to initiate an early exploratory study of ensemble methods on ai-cpss and aim to construct an efficient, robust, and reliable control solution for multi-tasks ai-cpss. we first utilize a semantic-based abstraction to decompose the large state space, capture the ongoing system status and predict future conditions in terms of the satisfaction of specifications. we propose a series of new semantics-aware ensemble strategies and an end-to-end deep reinforcement learning (drl) hierarchical ensemble method to improve the flexibility and reliability of the control systems. our large-scale, comprehensive evaluations over five subject cpss show that 1) the semantics abstraction can efficiently narrow the large state space and predict the semantics of incoming states, 2) our semantics-guided methods outperform state-of-the-art individual controllers and traditional ensemble methods, and 3) the drl hierarchical ensemble approach shows promising capabilities to deliver a more robust, efficient, and safety-assured control system. to enable further research along this direction to build better ai-enabled cps, we made all of the code and experimental results data publicly (https://sites.google.com/view/ai-cps-siege/ home).",AB_0091
"the global network of gravitational-wave observatories now includes five detectors, namely ligo hanford, ligo livingston, virgo, kagra, and geo 600. these detectors collected data during their third observing run, o3, composed of three phases: o3a starting in 2019 april and lasting six months, o3b starting in 2019 november and lasting five months, and o3gk starting in 2020 april and lasting two weeks. in this paper we describe these data and various other science products that can be freely accessed through the gravitational wave open science center at https://gwosc.org. the main data set, consisting of the gravitational-wave strain time series that contains the astrophysical signals, is released together with supporting data useful for their analysis and documentation, tutorials, as well as analysis software packages.",AB_0091
"the wpr-lq-7 is a uhf (1.3575 ghz) wind profiler radar used for routine measurements of the lower troposphere at shigaraki mu observatory (34.85 degrees n, 136.10 degrees e; japan) at a vertical resolution of 100m and a time resolution of 10 min. following studies carried out with the 46.5mhz middle and upper atmosphere (mu) radar (luce et al., 2018), we tested models used to estimate the rate of turbulence kinetic energy (tke) dissipation from the doppler spectral width in the altitude range similar to 0.7 to 4.0 km above sea level (a.s.l.). for this purpose, we compared lq-7-derived epsilon using processed data available online (http://www.rish.kyoto-u.ac.jp/radar- group/blr/shigaraki/data/, last access: 24 july 2023) with direct estimates of epsilon (epsilon(u)) from datahawk uavs. the statistical results reveal the same trends as reported by luce et al. (2018) with themuradar, namely (1) the simple formulation based on dimensional analysis epsilon l-out = sigma(3)/l-out, with l-out similar to 70 m, provides the best statistical agreement with epsilon(u). (2) the model epsilon(n) predicting a sigma(2) n law ( n is brunt-vaisala frequency) for stably stratified conditions tends to overestimate for epsilon(u) less than or similar to 5 x 10(-4) m(2) s(-3) and to underestimate for epsilon(u) greater than or similar to 5 x 10(-4) m(2) s(-3). we also tested a model epsilon(s) predicting a sigma(2) s law (s is the vertical shear of horizontal wind) supposed to be valid for low richardson numbers (ri = n-2/s-2). from the case study of a turbulent layer produced by a kelvin-helmholtz (k-h) instability, we found that epsilon(s) and epsilon l-out are both very consistent with epsilon(u), while epsilon(n) underestimates epsilon(u) in the core of the turbulent layer where n is minimum. we also applied the thorpe method from data collected from a nearly simultaneous radiosonde and tested an alternative interpretation of the thorpe length in terms of the corrsin length scale defined for weakly stratified turbulence. a statistical analysis showed that epsilon(s) also provides better statistical agreement with epsilon(u) and is much less biased than epsilon(n). combining estimates of n and shear from datahawk and radar data, respectively, a rough estimate of the richardson number at a vertical resolution of 100m (ri(100)) was obtained. we performed a statistical analysis on the ri dependence of the models. the main outcome is that epsilon(s) compares well with epsilon(u) for low ri(100) (ri(100) less than or similar to 1), while epsilon(n) fails. epsilon(lout) varies as epsilon(s) with ri(100), so that epsilon(lout) remains the best (and simplest) model in the absence of information on ri. also, sigma appears to vary as ri(100)(-1/2) when ri(100) greater than or similar to 0.4 and shows a degree of dependence on s-100 (vertical shear at a vertical resolution of 100 m) otherwise.",AB_0091
"few-shot learning (fsl) that aims to recognize novel classes with few labeled samples is troubled by its data scarcity. though recent works tackle fsl with data augmentation-based methods, these models fail to maintain the discrimination and diversity of the generated samples due to the distribution shift and intra-class bias caused by the data scarcity, therefore greatly undermining the performance. to this end, we use causal mechanisms, which are constant among independent variables across data distribution, to alleviate such effects. in this sense, we decompose the image information into two independent components: sample-specific and class-agnostic information, and further propose a novel counterfactual generation framework (cgf) to learn the underlying causal mechanisms to synthesize faithful samples for fsl. specifically, based on the counterfactual inference, we design a class-agnostic feature extractor to capture the sample-specific information, together with a counterfactual generation network to simulate the data generation process from a causal perspective. moreover, to leverage the power of cgf in counterfactual inference, we further develop a novel classifier that classifies samples based on their distributions of counterfactual generations. extensive experiments demonstrate the effectiveness of cgf on four fsl benchmarks, e.g., 80.12/86.13% accuracy on 5-way 1/5-shot miniimagenet fsl tasks, significantly improving the performance. our codes and models are available at https://github.com/eric-hang/cgf.",AB_0091
