AB,NO
"the random forest family has been extensively studied due to its wide applications in machine learning and data analytics. however, the representation abilities of forests have not been explored yet. the existing forest representation is mainly based on feature hashing on the indices of leaf nodes. feature hashing typically disregards the information from tree structures, i.e., the relationships between leaf nodes. furthermore, the visualisation abilities of feature hashing are limited. on the contrary, the skip-gram model has been widely explored in word and node embedding due to its excellent representation ability. this paper proposes distributed representation learning for trained forests (drl-tf) to extract co-occurrence relationships of samples and tree structures, and further boost the representation abilities of the trained forest using the skip-gram model. the experimental results demonstrate that the proposed drl-tf outperforms the challenging baselines. to the best of the authors' knowledge, the visualisation by drl-tf is the first tool to analyse the trained forests. the code is available at: https://github.com/machao199271/ drl-tf.",AB_0348
"snapshot compressive imaging (sci) systems aim to capture high-dimensional (=3d) images in a single shot using 2d detectors. sci devices consist of two main parts: a hardware encoder and a software decoder. the hardware encoder typically consists of an (optical) imaging system designed to capture compressed measurements. the software decoder, on the other hand, refers to a reconstruction algorithm that retrieves the desired high-dimensional signal from those measurements. in this paper, leveraging the idea of deep unrolling, we propose an sci recovery algorithm, namely gap-net, which unfolds the generalized alternating projection (gap) algorithm. at each stage, gap-net passes its current estimate of the desired signal through a trained convolutional neural network (cnn). the cnn operates as a denoiser projecting the estimate back to the desired signal space. for the gap-net that employs trained auto-encoder-based denoisers, we prove a probabilistic global convergence result. finally, we investigate the performance of gap-net in solving video sci and spectral sci problems. in both cases, gap-net demonstrates competitive performance on both synthetic and real data. in addition to its high accuracy and speed, we show that gap-net is flexible with respect to signal modulation implying that a trained gap-net decoder can be applied in different systems. our code is available at https://github.com/mengziyi64/gap-net.",AB_0348
"limited by the size, location, number of samples and other factors of the small object itself, the small object is usually insufficient, which degrades the performance of the small object detection algorithms. to address this issue, we construct a novel feature enhancement network (fenet) to improve the per-formance of small object detection. firstly, an improved data augmentation method based on collision detection and spatial context extension (cdci) is proposed to effectively expand the possibility of small object detection. then, based on the idea of granular computing, a multi-granular deformable convolu-tion network is constructed to acquire the offset feature representation at the different granularity levels. finally, we design a high-resolution block (hr block) and build high-resolution block-based feature pyramid by parallel embedding hr block in fpn (hr-fpn) to make full use different granularity and res-olution features. by above strategies, fenet can acquire sufficient feature information of small objects. in this paper, we firstly applied the multi-granularity deformable convolution to feature extraction of small objects. meanwhile, a new feature fusion module is constructed by optimizing feature pyramid to maintain the detailed features and enrich the semantic information of small objects. experiments show that fenet achieves excellent performance compared with performance of other methods when applied to the publicly available coco dataset, visdrone dataset and tinyperson dataset. the code is available at https://github.com/cowarder/fenet . & copy; 2023 elsevier ltd. all rights reserved.",AB_0348
"transfer learning aims to apply previously learned knowledge to new unknown domains by mining the potential relationships between data from different domains. because of its ability to process data from different domains, the projection transfer learning methods have attracted much attention. however, most of the methods only focus on the reconstruction between different domains and do not fully use the data. here, we put forward a novel transfer learning method called low-rank constraint-based multiple projections learning (lrmpl) for cross-domain classification. lrmpl works on both the overall reconstruction of two domains and aligns the data sharing the same label but from different domains by imposing low-rank constraints on the reconstruction coefficient matrix. in this way, the obtained feature representation is transferrable and discriminative. moreover, lrmpl considers the information preserving during the feature representation learning by integrating a modified pca-like item into the objective function. to learn a suitable classifier parameter and feature representation, lrmpl also unifies the classifier and feature learning into a single optimization objective. extensive experiments on several benchmark datasets show lrmpl outperforms the existing traditional transfer learning methods in cross-domain classification. the demo code is available in https://github.com/892384750/ lrmpl.& copy; 2023 elsevier b.v. all rights reserved.",AB_0348
"recent years' development of ai technology brings more convenience to our life while at the same time increasing the risk of personal information leakage. in this work, we try to protect personal information contained in the images by generating adversarial examples to fool the image captioning models. the generated adversarial examples are user-oriented which means the users can manipulate or hide sensitive information on the text output as they wish. by doing so, our personal information can be well protected from image captioning models. to fulfill the task, we adopt five kinds of adversarial attack. experimental results show our method can successfully protect user security. the pytorch & reg; implementations can be downloaded from an open-source github project (https://github.com/dlut-lab-zmn/imagecaptioning-attack/). & copy; 2023 elsevier b.v. all rights reserved.",AB_0348
"cross-modal retrieval has developed remarkably recently and received extensive attention as an essential method for multimodal interaction study. however, most existing models are limited to one of the applications in cross-modal retrieval, i.e., text-image retrieval, and neglect the audio modality, which is widely distributed in data and can be integrated into the models to improve retrieval performance. to address this issue, we propose a text-image-audio cross-modal retrieval (tiar) model that, given any or two modalities, implements the retrieval of the remaining modalities. tiar consists of three modal-specific encoders to extract the features and a cross-modal encoder to generate joint contextualized representations for all modalities. to evaluate our model, we present two new cross-modal retrieval tasks, named cross-unimodal and cross-bimodal retrieval, that are applicable to three modalities. then, during testing, we propose a weighted multimodal re-ranking (wmr) algorithm which integrates comprehensive ranking information in the similarity matrices of all tasks to improve the performance without additional training. the experiment results show that tiar-wmr outperforms state-of-the-art models in traditional text-image retrieval on flickr30k, coco, and ade20k datasets. moreover, the retrieval performance of tiar-wmr is further boosted in the two proposed tasks when two input modalities are integrated. the code is available at .https://github.com/peidechi/tiar.",AB_0348
"scene graph generation (sgg) is a typical computer vision task that detects objects and corresponding predicates in an image. existing sgg methods focus on modeling visual contexts to generate scene graphs and are conducted on well-annotated datasets with high-quality images. however, the quality is unguaranteed for images in social media posts, so that some images may be incomplete or occluded by some obstacles, hence might not provide sufficient visual context for sgg. therefore, previous methods might result in missing or false visual relationship detection due to lacking visual contexts. to effectively generate the scene graphs in social media, we study multimodal scene graph generation (msg) in this paper. msg aims to develop visual scene graphs from images in social media posts with the support of text sentences. however, leveraging textual contents by simple multimodal alignment such as object-level alignment neglects the inherent pair-wise mapping between multimodal object pairs. to address the limitations, we propose a method named deep pair-wise relation alignment for knowledge-enhanced (drake) multimodal scene graph generation. the model supplements the missing visual contexts with well-aligned textual knowledge. it first represents the textual information into object-aware knowledge representation with the help of vision data. furthermore, our proposed drake facilitates the interaction of the info between multimodal pair-wise representations. a multimodal context enhancement layer can be devised to help the model generate the scene graph. to evaluate the model performance of sgg on social media images, we propose a social media sgg dataset called msg. we comprehensively analyze the effectiveness of our proposed method on the msg dataset. the experimental results on the msg dataset indicate that our model outperforms the previous methods. to fairly compare our method with other sgg models, we also conduct experiments on the visual genome dataset for more analysis the msg dataset is released on https://github.com/fuze4ever/msg.",AB_0348
"siamese trackers have received a lot of attentions due to their promising performance and real-time high speed. however, the robustness is generally limited especially in challenging conditions, such as occlusion. motivated by that tracking failure does not always occur and the target movement tends to follow certain patterns, in the paper, we propose a generic, fast and flexible approach to improve the robustness of siamese trackers with two light-load novel modules: trajectory guidance module (tgm) and selective refinement module (srm). specifically, tgm encourages to pay a soft attention on possible target location based on short-term historical trajectory. srm selectively remedies the tracking results at the risk of failure with little impact on the speed. the proposed algorithm can be easily establish upon state-of-the-art siamese trackers and obtains better performance on seven benchmarks with high real-time tracking speed. the code is available at https://github.com/tjummg/tgsr.",AB_0348
"existing deep embedding clustering methods fail to sufficiently utilize the available off-the-shelf information from feature embeddings and cluster assignments, limiting their performance. to this end, we propose a novel method, namely deep attention-guided graph clustering with dual self-supervision (dagc). specifically, dagc first utilizes a heterogeneity-wise fusion module to adaptively integrate the features of the auto-encoder and the graph convolutional network in each layer and then uses a scale-wise fusion module to dynamically concatenate the multi-scale features in different layers. such modules are capable of learning an informative feature embedding via an attention-based mechanism. in addition, we design a distribution-wise fusion module that leverages cluster assignments to acquire clustering results directly. to better explore the off-the-shelf information from the cluster assignments, we develop a dual self-supervision solution consisting of a soft self-supervision strategy with a kullback-leibler divergence loss and a hard self-supervision strategy with a pseudo supervision loss. extensive experiments on nine benchmark datasets validate that our method consistently outperforms state-of-the-art methods. especially, our method improves the ari by more than 10.29% over the best baseline. the code will be publicly available at https://github.com/zhihaopeng-cityu/dagc.",AB_0348
"in this letter, we propose a novel semi-supervised subspace clustering method, which is able to simultaneously augment the initial supervisory information and construct a discriminative affinity matrix. by representing the limited amount of supervisory information as a pairwise constraint matrix, we observe that the ideal affinity matrix for clustering shares the same low-rank structure as the ideal pairwise constraint matrix. thus, we stack the two matrices into a 3-d tensor, where a global low-rank constraint is imposed to promote the affinity matrix construction and augment the initial pairwise constraints synchronously. besides, we use the local geometry structure of input samples to complement the global low-rank prior to achieve better affinity matrix learning. the proposed model is formulated as a laplacian graph regularized convex low-rank tensor representation problem, which is further solved with an alternative iterative algorithm. in addition, we propose to refine the affinity matrix with the augmented pairwise constraints. comprehensive experimental results on eight commonly-used benchmark datasets demonstrate the superiority of our method over state-of-the-art methods. the code is publicly available at https://github.com/guanxinglu/subspace-clustering.",AB_0348
