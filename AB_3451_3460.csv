AB,NO
"few-shot fine-grained recognition (fs-fgr) aims to recognize novel fine-grained categories with the help of limited available samples. undoubtedly, this task inherits the main challenges from both few-shot learning and fine-grained recognition. first, the lack of labeled samples makes the learned model easy to overfit. second, it also suffers from high intra-class variance and low inter-class differences in the datasets. to address this challenging task, we propose a two-stage background suppression and foreground alignment framework, which is composed of a background activation suppression (bas) module, a foreground object alignment (foa) module, and a local-to-local (l2l) similarity metric. specifically, the bas is introduced to generate a foreground mask for localization to weaken background disturbance and enhance dominative foreground objects. the foa then reconstructs the feature map of each support sample according to its correction to the query ones, which addresses the problem of misalignment between support-query image pairs. to enable the proposed method to have the ability to capture subtle differences in confused samples, we present a novel l2l similarity metric to further measure the local similarity between a pair of aligned spatial features in the embedding space. what's more, considering that background interference brings poor robustness, we infer the pairwise similarity of feature maps using both the raw image and the refined image. extensive experiments conducted on multiple popular fine-grained benchmarks demonstrate that our method outperforms the existing state of the art by a large margin. the source codes are available at: https://github.com/cser-tang-hao/bsfa-fsfg.",AB_0346
"previous works for lidar-based 3d object detection mainly focus on the single-frame paradigm. in this paper, we propose to detect 3d objects by exploiting temporal information in multiple frames, i.e., point cloud videos. we empirically categorize the temporal information into short-term and long-term patterns. to encode the short-term data, we present a grid message passing network (gmpnet), which considers each grid (i.e., the grouped points) as a node and constructs a k-nn graph with the neighbor grids. to update features for a grid, gmpnet iteratively collects information from its neighbors, thus mining the motion cues in grids from nearby frames. to further aggregate long-term frames, we propose an attentive spatiotemporal transformer gru (ast-gru), which contains a spatial transformer attention (sta) module and a temporal transformer attention (tta) module. sta and tta enhance the vanilla gru to focus on small objects and better align moving objects. our overall framework supports both online and offline video object detection in point clouds. we implement our algorithm based on prevalent anchor-based and anchor-free detectors. evaluation results on the challenging nuscenes benchmark show superior performance of our method, achieving first on the leaderboard (at the time of paper submission) without any bells and whistles. our source code is available at https://github.com/shenjianbing/gmp3d.",AB_0346
"the growth in social media has led to the increasing spread of unverified or false information. automat-ically detecting rumours and accessing their veracity, i.e., false rumours, true rumours, or unverified rumours, is an important and challenging task in social media analytics. this paper aims to build an effective and scalable stream classification framework for early fine-grained rumour classification based on community response. we propose a causal diffused graph-transformer network (cdgtn) to extract features from the source-reply graph in a social media conversation. then, we propose source-guided incremental attention pooling (sgiap) to aggregate the encoded features with discrete timestamps. to improve the performance of early classification, we propose a stacked early classification loss (secloss), which aims to minimize the classification loss over the time instances. this can greatly improve the effectiveness of early classification of rumours. to improve the efficiency of streaming rumour verification, we propose a continued inference algorithm based on prefix-sum, which can greatly reduce the computational complexity of stream classification of rumours. furthermore, we annotated the first chinese rumour verification dataset, by extending the existing chinese-twitter dataset, namely cr-twitter, originally for rumour detection. we conducted experiments on the twitter15, twitter16, pheme, weibo, and the extended cr-twitter datasets for rumour classification, to verify our proposed stream classification framework. the experimental results show that our proposed framework can significantly boost the effectiveness and efficiency of early stream classification of rumours. models and datasets are released at: https://thcheung.github.io/cdgtn/.& copy; 2023 elsevier b.v. all rights reserved.",AB_0346
"semi-supervised object detection methods have become increasingly popular in computer vision owing to their success in reducing data labeling costs. however, low-quality pseudo-labels exhibit difficulties in correction and severely limit the performance of existing state-of-the-art models. to address these problems, based on the contemporary teacher-student dual framework, we develop a novel self-correcting pseudo-label module to generate more reliable predictions for unlabeled data. simultaneously, to mitigate the inherent class bias in pseudo-labels, we integrate multi-label classification results for measuring the re-balanced focal loss to enable class-agnostic, robust semi-supervised learning. furthermore, pseudo-label-guided copy-paste is proposed as a simple yet efficient data augmentation technique to enhance instance representation learning within diverse complex scenes. the above key designs constitute our proposed approach, which we call robust teacher. extensive experiments on pascal voc and ms coco demonstrate that robust teacher achieves competitive results and outperforms state-of-the-art models by a large margin. our comprehensive ablation studies further verify the effectiveness of the key components. the code will be publicly available at https://github.com/complicateddd/robustt.",AB_0346
"the deep structure in the image contains certain information of the image, which is helpful to perceive the quality of the image. inspired by deep level image features extracted via deep learning methods, we propose a no-reference blurred image quality evaluation model based on the structure of structure features. in spatial domain, the novel weighted local binary patterns are proposed which leverage maximum local variation maps to extract structural features from multi-resolution images. in spectral domain, gradient information of multi-scale log-gabor filtered images is extracted as the structure of structure features, and combined with entropy features. then, the features extracted from both domains are fused to form a quality perception feature vector and mapped into the quality score via support vector regression (svr). experiments are conducted to evaluate the performance of the proposed method on various iqa databases, including the live, csiq, tid2008, tid2013, cid2013, clive, and bid. the experimental results show that compared with some state-of-the-art methods, our proposed method achieves better evaluation results and is more in line with the human visual system. the source code will be released at https://github.com/jamesc0321/s2s_features/.",AB_0346
"few-shot learning (fsl) is a challenging problem that aims to learn and generalize from limited examples. recent works have adopted a combination of meta-learning and transfer learning strategies for fsl tasks. these methods perform pre-training and transfer the learned knowledge to metalearning. however, it remains unclear whether this transfer pattern is appropriate, and the objectives of the two learning strategies have not been explored. in addition, the inference of meta-learning in fsl relies on sample relations that require further consideration. in this paper, we uncover an overlooked discrepancy in learning objectives between pre-training and meta-learning strategies and propose a simple yet effective learning paradigm for the few-shot classification task. specifically, the proposed method comprises two components: (i) detach: we formulate an effective learning paradigm, adaptive meta-transfer (a-met), which adaptively eliminates undesired representations learned by pre-training to address the discrepancy. (ii) unite: we propose a global similarity compatibility measure (gscm) to jointly consider sample correlation at a global level for more consistent predictions. the proposed method is simple to implement without any complex components. extensive experiments on four public benchmarks demonstrate that our method outperforms other state-of-the-art methods under more challenging scenarios with large domain differences between the base and novel classes and less support information available. code is available at: https://github.com/yaoyz96/a-met. & copy; 2023 elsevier b.v. all rights reserved.",AB_0346
"at present, image completion models are often used to handle images in public datasets and are not competent for tasks in practical scenarios such as usv scenes. on one hand, the practical missing regions are often located at the boundaries, which presents a challenge for the model to extract image features. on the other hand, real images are often blurred, and filling the content without optimizing the image quality will limit the application of completed images. to address these challenges, a novel image completion model using an attention mechanism and a joint enhancive discriminator, which can effectively fill in the missing and enhance the image quality in various missing situations, has been proposed in this paper. first, an attention mechanism is used as the condition of the generator to extract the related missing information according to different weights. to ensure that the output of the algorithm is semantically consistent with the original image and has a higher quality than the original image, a joint discriminator is introduced to constrain the conditional generator from different aspects. in particular, the cloud model (a cognitive model) in the joint enhancive discriminator can promote the quality and practicability of the generated images. experimental results show that our model achieves better performance than state-of-the-art models in both qualitative and quantitative measurements on four public datasets and one usv real dataset. compared with the baselines, our method has an average improvement of 4.90% and 1.00% in image completion evaluation and image quality evaluation, respectively. we also verify the performance of our model for practical application in real scenarios. the code is available at https://github.com/wrq-cqupt/ic.",AB_0346
"downsampling operations such as max pooling or strided convolution are ubiquitously utilized in convolutional neural networks (cnns) to aggregate local features, enlarge receptive field, and minimize computational overhead. however, for a semantic segmentation task, pooling features over the local neighbourhood may result in the loss of important spatial information, which is conducive for pixel-wise predictions. to address this issue, we introduce a simple yet effective pooling operation called the haar wavelet-based downsampling (hwd) module. this module can be easily integrated into cnns to enhance the performance of semantic segmentation models. the core idea of hwd is to apply haar wavelet transform for reducing the spatial resolution of feature maps while preserving as much information as possible. furthermore, to investigate the benefits of hwd, we propose a novel metric, named as feature entropy index (fei), which measures the degree of information uncertainty after downsampling in cnns. specifically, the fei can be used to indicate the ability of downsampling methods to preserve essential information in semantic segmentation. our comprehensive experiments demonstrate that the proposed hwd module could (1) effectively improve the segmentation performance across different modality image datasets with various cnn architectures, and (2) efficiently reduce information uncertainty compared to the conventional downsampling methods. our implementation are available at https:// github.com/apple1986/hwd.",AB_0346
"discrepancy representation metric completely determines the transfer diagnosis performance of deep domain adaptation methods. maximum mean discrepancy (mmd) based on the mean statistic, as the commonly used metric, has poor discrepancy representation in some cases. mmd is generally known from the aspect of kernel function, but the inherent relationship between the two is unknown. to deal with these issues, the authors theoretically explore their relationship first. with the revealed relationship, a novel discrepancy representation metric named maximum mean square discrepancy (mmsd), which can comprehensively reflect the mean and variance information of data samples in the reproducing kernel hilbert space, is constructed for enhancing domain confusion. additionally, for the real application under limited samples and ensuring the effectiveness of mmsd, biased and unbiased empirical mmsd statistics are developed, and the error bounds between the two empirical statistics and the real distribution discrepancy are derived. the proposed mmsd is successfully applied to the end-to-end fault diagnosis of planetary gearbox of wind turbine without labeled target-domain samples. the experimental results on twelve cross-load transfer tasks validate that mmsd has a better ability of discrepancy representation and a higher diagnosis accuracy compared with other well-known discrepancy representation metrics. the related code can be downloaded from https://qinyi-team.github.io/2023/06/maximum-mean-square-discrepancy. & copy; 2023 elsevier b.v. all rights reserved.",AB_0346
"humanaction recognition based on still images is one of themost challenging computer vision tasks. in the past decade, convolutional neural networks (cnns) have developed rapidly and achieved good performance in human action recognition tasks based on still images. due to the absence of the remote perception ability of cnns, it is challenging to have a global structural understanding of human behavior and the overall relationship between the behavior and the environment. recently, transformer-based models have been making a splash in computer vision, even reaching sota in several vision tasks. we explore the transformer's capability in human action recognition based on still images and add a simple but effective feature fusion module based on the swin-transformer model. more specifically, we propose a newtransformer-basedmodel for behavioral feature extraction that uses a pre-trained swintransformer as the backbone network. swin-transformer's distinctive hierarchical structure, combined with the feature fusion module, is used to extract and fuse multi-scale behavioral information. extensive experiments were conducted on five still image-based human action recognition datasets, including the li's action dataset, the stanford-40 dataset, the ppmi-24 dataset, the auc-v1 dataset, and the auc-v2 dataset. results indicate that our proposed swin-fusion model achieves better behavior recognition than previously improved cnnbased models by sharing and reusing feature maps of different scales at multiple stages, without modifying the original backbone training method and with only increasing training resources by 1.6%. the code and models will be available at https://github.com/ cts4444/ swin-fusion.",AB_0346
