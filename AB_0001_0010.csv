AB,NO
"semantic segmentation of large unstructured 3d point clouds is important problem for 3d object recognition which in turn is essential to solving more complex tasks such as scene understanding. the problem is highly challenging owing to large scale of data, varying point density and localization errors of 3d points. nevertheless, with recent successes of deep neural network architectures to solve complex 2d perceptual problems, several researchers have shown interest to translate the developed 2d networks to 3d point cloud segmentation by a prior voxelization step for an explicit neighborhood representation. however, such a 3d grid representation loses the fine details and inherent structure due to quantization artifacts. for this purpose, this paper proposes an approach to performing semantic segmentation of 3d point clouds by exploiting the idea of super-point based graph construction. the proposed architecture is composed of two cascaded modules including a light-weight representation learning module which uses unsupervised geometric grouping to partition the large-scale unstructured 3d point cloud and a deep context aware sequential network based on long short memory units and graph convolutions with embedding residual learning for semantic segmentation. the proposed model is evaluated on two standard benchmark datasets and achieves competitive performance with the existing state-of-the-art datasets. the code and the obtained results have been made public at. https://github.com/ saba155/dcarn.",AB_0001
"automated medical image segmentation plays an important role in many clinical applications, which however is a very challenging task, due to complex background texture, lack of clear boundary and significant shape and texture variation between images. many researchers proposed an encoder-decoder architecture with skip connections to combine low-level feature maps from the encoder path with high-level feature maps from the decoder path for automatically segmenting medical images. the skip connections have been shown to be effective in recovering fine-grained details of the target objects and may facilitate the gradient back-propagation. however, not all the feature maps transmitted by those connections contribute positively to the network performance. in this paper, to adaptively select useful information to pass through those skip connections, we propose a novel 3d network with self-supervised function, named selective information passing network. we evaluate our proposed model on the miccai prostate mr image segmentation 2012 grant challenge dataset, tcia pancreas ct-82 and miccai 2017 liver tumor segmentation challenge dataset. the experimental results across these datasets show that our model achieved improved segmentation results and outperformed other state-of-the-art methods. the source code of this work is available at https://github.com/ahukui/sipnet.",AB_0001
"motion estimation is a fundamental step in dynamic medical image processing for the assessment of target organ anatomy and function. however, existing image-based motion estimation methods, which optimize the motion field by evaluating the local image similarity, are prone to produce implausible estimation, especially in the presence of large motion. in addition, the correct anatomical topology is difficult to be preserved as the image global context is not well incorporated into motion estimation. in this study, we provide a novel motion estimation framework of dense-sparse-dense (dsd), which comprises two stages. in the first stage, we process the raw dense image to extract sparse landmarks to represent the target organ's anatomical topology, and discard the redundant information that is unnecessary for motion estimation. for this purpose, we introduce an unsupervised 3-d landmark detection network to extract spatially sparse but representative landmarks for the target organ's motion estimation. in the second stage, we derive the sparse motion displacement from the extracted sparse landmarks of two images of different time points. then, we present a motion reconstruction network to construct the motion field by projecting the sparse landmarks' displacement back into the dense image domain. furthermore, we employ the estimated motion field from our two-stage dsd framework as initialization and boost the motion estimation quality in light-weight yet effective iterative optimization. we evaluate our method on two dynamic medical imaging tasks to model cardiac motion and lung respiratory motion, respectively. our method has produced superior motion estimation accuracy compared to the existing comparative methods. besides, the extensive experimental results demonstrate that our solution can extract well-representative anatomical landmarks without any requirement of manual annotation. our code is publicly available online: https://github.com/yyguo-sjtu/dsd-3d-unsupervised-landmark-detection-based-motion-estimation.",AB_0001
"state-of-the-art methods in the image-to-image translation are capable of learning a mapping from a source domain to a target domain with unpaired image data. though the existing methods have achieved promising results, they still produce visual artifacts, being able to translate low-level information but not high-level semantics of input images. one possible reason is that generators do not have the ability to perceive the most discriminative parts between the source and target domains, thus making the generated images low quality. in this article, we propose a new attention-guided generative adversarial networks (attentiongan) for the unpaired image-to-image translation task. attentiongan can identify the most discriminative foreground objects and minimize the change of the background. the attention-guided generators in attentiongan are able to produce attention masks, and then fuse the generation output with the attention masks to obtain high-quality target images. accordingly, we also design a novel attention-guided discriminator which only considers attended regions. extensive experiments are conducted on several generative tasks with eight public datasets, demonstrating that the proposed method is effective to generate sharper and more realistic images compared with existing competitive models. the code is available at https://github.com/ha0tang/attentiongan.",AB_0001
"most modern satellites can provide two types of images: 1) panchromatic (pan) image and 2) multispectral (ms) image. the former has high spatial resolution and low spectral resolution, while the latter has high spectral resolution and low spatial resolution. to obtain images with both high spectral and spatial resolution, pansharpening has emerged to fuse the spatial information of the pan image and the spectral information of the ms image. however, most pansharpening methods fail to preserve spatial and spectral information simultaneously. in this article, we propose a framelet-based convolutional neural network (cnn) for pansharpening which makes it possible to pursue both high spectral and high spatial resolution. our network consists of three subnetworks: 1) feature embedding net; 2) feature fusion net; and 3) framelet prediction net. different from conventional cnn methods directly inferring high-resolution ms images, our approach learns to predict their framelet coefficients from available pan and ms images. the introduction of multilevel feature aggregation and hybrid residual connection makes full use of spatial information of pan image and spectral information of ms image. quantitative and qualitative experiments at reduced- and full-resolution demonstrate that the proposed method achieves more appealing results than other state-of-the-art pansharpening methods. the source code and trained models are available at https://github.com/tingmac/frmlnet.",AB_0001
"multiview spectral clustering (mvsc) has achieved state-of-the-art clustering performance on multiview data. most existing approaches first simply concatenate multiview features or combine multiple view-specific graphs to construct a unified fusion graph and then perform spectral embedding and cluster label discretization with k-means to obtain the final clustering results. they suffer from an important drawback: all views are treated as fixed when fusing multiple graphs and equal when handling the out-of-sample extension. they cannot adaptively differentiate the discriminative capabilities of multiview features. to alleviate these problems, we propose a flexible mvsc with self-adaptation (fmscs) method in this article. a self-adaptive learning scheme is designed for structured graph construction, multiview graph fusion, and out-of-sample extension. specifically, we learn a fusion graph with a desirable clustering structure by adaptively exploiting the complementarity of different view features under the guidance of a proper rank constraint. meanwhile, we flexibly learn multiple projection matrices to handle the out-of-sample extension by adaptively adjusting the view combination weights according to the specific contents of unseen data. finally, we derive an alternate optimization strategy that guarantees desirable convergence to iteratively solve the formulated unified learning model. extensive experiments demonstrate the superiority of our proposed method compared with state-of-the-art mvsc approaches. for the purpose of reproducibility, we provide the code and testing datasets at https://github.com/shidan0122/fmics.",AB_0001
"this article presents two kernel-based rock detection methods for a mars rover. rock detection on planetary surfaces is particularly pivotal for planetary vehicles regarding navigation and obstacle avoidance. however, the diverse morphologies of martian rocks, the sparsity of pixel-wise features, and engineering constraints are great challenges to current pixel-wise object detection methods, resulting in inaccurate and delayed object location and recognition. we therefore propose a region-wise rock detection framework and design two detection algorithms, kernel principle component analysis (kpca)-based rock detection (kprd) and kernel low-rank representation (klrr)-based rock detection (klrd), using hypotheses of feature and sub-spatial separability. kprd is based on kpca and is expert in real-time detection yet with less accurate performance. klrd is based on kprd with klrr which can generate more precise rock detection results with less delay. to validate the efficiency of the proposed methods, we build a small-scale martian rock dataset, marsdata, containing various rocks. preliminary experimental results show that our methods are efficient in dealing with complex images containing rocks, shadows, and gravel. the code and data are available at: https://github.com/cvir-lab/marsdata.",AB_0001
"existing no-reference (nr) image quality assessment (iqa) metrics are still not convincing for evaluating the quality of the camera-captured images. toward tackling this issue, we, in this article, establish a novel nr quality metric for quantifying the quality of the camera-captured images reliably. since the image quality is hierarchically perceived from the low-level preliminary visual perception to the high-level semantic comprehension in the human brain, in our proposed metric, we characterize the image quality by exploiting both the low-level image properties and the high-level semantics of the image. specifically, we extract a series of low-level features to characterize the fundamental image properties, including the brightness, saturation, contrast, noiseness, sharpness, and naturalness, which are highly indicative of the camera-captured image quality. correspondingly, the high-level features are designed to characterize the semantics of the image. the low-level and high-level perceptual features play complementary roles in measuring the image quality. to infer the image quality, we employ the support vector regression (svr) to map all the informative features to a single quality score. thorough tests conducted on two standard camera-captured image databases demonstrate the effectiveness of the proposed quality metric in assessing the image quality and its superiority over the state-of-the-art nr quality metrics. the source code of the proposed metric for camera-captured images is released at https://github.com/yt2015?tab=repositories.",AB_0001
"current methods aggregate multilevel features from the backbone and introduce edge information to get more refined saliency maps. however, little attention is paid to how to suppress the regions with similar saliency appearances in the background. these regions usually exist in the vicinity of salient objects and have high contrast with the background, which is easy to be misclassified as foreground. to solve this problem, we propose a gated feature interaction network (gfinet) to integrate multiple saliency features, which can utilize nonboundary features with background information to suppress pseudosalient objects and simultaneously apply boundary features to supplement edge details. different from previous methods that only consider the complementarity between saliency and boundary, the proposed network introduces nonboundary features into the decoder to filter the pseudosalient objects. specifically, gfinet consists of global features aggregation branch (gfab), boundary and nonboundary features' perception branch (b&nfpb), and gated feature interaction module (gfim). according to the global features generated by gfab, boundary and nonboundary features produced by b&nfpb and gfim employ a gate structure to adaptively optimize the saliency information interchange between abovementioned features and, thus, predict the final saliency maps. besides, due to the imbalanced distribution between the boundary pixels and nonboundary ones, the binary cross-entropy (bce) loss is difficult to predict the pixels near the boundary. therefore, we design a border region aware (bra) loss to further boost the quality of boundary and nonboundary, which can guide the network to focus more on the hard pixels near the boundary by assigning different weights to different positions. compared with 12 counterparts, experimental results on five benchmark datasets show that our method has better generalization and improves the state-of-the-art approach by 4.85% averagely in terms of the regional and boundary evaluation measures. in addition, our model is more efficient with an inference speed of 50.3 fps when processing a 320 x 320 image. code has been made available at https://github.com/lesonly/gfinet.",AB_0001
"in this article, we propose a novel filter pruning method for deep learning networks by calculating the learned representation median (rm) in frequency domain (lrmf). in contrast to the existing filter pruning methods that remove relatively unimportant filters in the spatial domain, our newly proposed approach emphasizes the removal of absolutely unimportant filters in the frequency domain. through extensive experiments, we observed that the criterion for ``relative unimportance'' cannot be generalized well and that the discrete cosine transform (dct) domain can eliminate redundancy and emphasize low-frequency representation, which is consistent with the human visual system. based on these important observations, our lrmf calculates the learned rm in the frequency domain and removes its corresponding filter, since it is absolutely unimportant at each layer. thanks to this, the time-consuming fine-tuning process is not required in lrmf. the results show that lrmf outperforms state-of-the-art pruning methods. for example, with resnet110 on cifar-10, it achieves a 52.3% flops reduction with an improvement of 0.04% in top-1 accuracy. with vgg16 on cifar-100, it reduces flops by 35.9% while increasing accuracy by 0.5%. on imagenet, resnet18 and resnet50 are accelerated by 53.3% and 52.7% with only 1.76% and 0.8% accuracy loss, respectively. the code is based on pytorch and is available at https://github.com/zhangxin-xd/lrmf.",AB_0001
