AB,NO
"pansharpening refers to the fusion of a low spatial-resolution multispectral image with a high spatial-resolution panchromatic image. in this paper, we propose a novel low-rank tensor completion (lrtc)-based framework with some regularizers for multispectral image pansharpening, called lrtcfpan. the tensor completion technique is commonly used for image recovery, but it cannot directly perform the pansharpening or, more generally, the super-resolution problem because of the formulation gap. different from previous variational methods, we first formulate a pioneering image super-resolution (isr) degradation model, which equivalently removes the downsampling operator and transforms the tensor completion framework. under such a framework, the original pansharpening problem is realized by the lrtc-based technique with some deblurring regularizers. from the perspective of regularizer, we further explore a local-similarity-based dynamic detail mapping (ddm) term to more accurately capture the spatial content of the panchromatic image. moreover, the low-tubal-rank property of multispectral images is investigated, and the low-tubal-rank prior is introduced for better completion and global characterization. to solve the proposed lrtcfpan model, we develop an alternating direction method of multipliers (admm)-based algorithm. comprehensive experiments at reduced-resolution (i.e., simulated) and full-resolution (i.e., real) data exhibit that the lrtcfpan method significantly outperforms other state-of-the-art pansharpening methods. the code is publicly available at: https://github.com/zhongchengwu/code_lrtcfpan",AB_0385
"currently, cross-scene hyperspectral image (hsi) classification has drawn increasing attention. it is necessary to train a model only on source domain (sd) and directly transferring the model to target domain (td), when td needs to be processed in real time and cannot be reused for training. based on the idea of domain generalization, a single-source domain expansion network (sdenet) is developed to ensure the reliability and effectiveness of domain extension. the method uses generative adversarial learning to train in sd and test in td. a generator including semantic encoder and morph encoder is designed to generate the extended domain (ed) based on encoder-randomization-decoder architecture, where spatial randomization and spectral randomization are specifically used to generate variable spatial and spectral information, and the morphological knowledge is implicitly applied as domain invariant information during domain expansion. furthermore, the supervised contrastive learning is employed in the discriminator to learn class-wise domain invariant representation, which drives intra-class samples of sd and ed. meanwhile, adversarial training is designed to optimize the generator to drive intra-class samples of sd and ed to be separated. extensive experiments on two public hsi datasets and one additional multispectral image (msi) dataset demonstrate the superiority of the proposed method when compared with state-of-the-art techniques. the codes will be available from the website:https://github.com/yuxiangzhang-bit/ieee_tip_sdenet.",AB_0385
"real-world face super-resolution (sr) is a highly ill-posed image restoration task. the fully-cycled cycle-gan architecture is widely employed to achieve promising performance on face sr, but is prone to produce artifacts upon challenging cases in real-world scenarios, since joint participation in the same degradation branch will impact final performance due to huge domain gap between real-world and synthetic lr ones obtained by generators. to better exploit the powerful generative capability of gan for real-world face sr, in this paper, we establish two independent degradation branches in the forward and backward cycle-consistent reconstruction processes, respectively, while the two processes share the same restoration branch. our semi-cycled generative adversarial networks (scgan) is able to alleviate the adverse effects of the domain gap between the real-world lr face images and the synthetic lr ones, and to achieve accurate and robust face sr performance by the shared restoration branch regularized by both the forward and backward cycle-consistent learning processes. experiments on two synthetic and two real-world datasets demonstrate that, our scgan outperforms the state-of-the-art methods on recovering the face structures/details and quantitative metrics for real-world face sr. the code will be publicly released at https://github.com/haohou-98/scgan.",AB_0385
"accurate correspondence selection between two images is of great importance for numerous feature matching based vision tasks. the initial correspondences established by off-the-shelf feature extraction methods usually contain a large number of outliers, and this often leads to the difficulty in accurately and sufficiently capturing contextual information for the correspondence learning task. in this paper, we propose a preference-guided filtering network (pgfnet) to address this problem. the proposed pgfnet is able to effectively select correct correspondences and simultaneously recover the accurate camera pose of matching images. specifically, we first design a novel iterative filtering structure to learn the preference scores of correspondences for guiding the correspondence filtering strategy. this structure explicitly alleviates the negative effects of outliers so that our network is able to capture more reliable contextual information encoded by the inliers for network learning. then, to enhance the reliability of preference scores, we present a simple yet effective grouped residual attention block as our network backbone, by designing a feature grouping strategy, a feature grouping manner, a hierarchical residual-like manner and two grouped attention operations. we evaluate pgfnet by extensive ablation studies and comparative experiments on the tasks of outlier removal and camera pose estimation. the results demonstrate outstanding performance gains over the existing state-of-the-art methods on different challenging scenes. the code is available at https://github.com/guobaoxiao/pgfnet.",AB_0385
"comprehensive understanding of video content requires both spatial and temporal localization. however, there lacks a unified video action localization framework, which hinders the coordinated development of this field. existing 3d cnn methods take fixed and limited input length at the cost of ignoring temporally long-range cross-modal interaction. on the other hand, despite having large temporal context, existing sequential methods often avoid dense cross-modal interactions for complexity reasons. to address this issue, in this paper, we propose a unified framework which handles the whole video in sequential manner with long-range and dense visual-linguistic interaction in an end-to-end manner. specifically, a lightweight relevance filtering based transformer (ref-transformer) is designed, which is composed of relevance filtering based attention and temporally expanded mlp. the text-relevant spatial regions and temporal clips in video can be efficiently highlighted through the relevance filtering and then propagated among the whole video sequence with the temporally expanded mlp. extensive experiments on three sub-tasks of referring video action localization, i.e., referring video segmentation, temporal sentence grounding, and spatiotemporal video grounding, show that the proposed framework achieves the state-of-the-art performance in all referring video action localization tasks. the code has been available at https://github.com/tjummg/saw.",AB_0385
"multi-modal image registration aims to spatially align two images from different modalities to make their feature points match with each other. captured by different sensors, the images from different modalities often contain many distinct features, which makes it challenging to find their accurate correspondences. with the success of deep learning, many deep networks have been proposed to align multi-modal images, however, they are mostly lack of interpretability. in this paper, we first model the multi-modal image registration problem as a disentangled convolutional sparse coding (dcsc) model. in this model, the multi-modal features that are responsible for alignment (ra features) are well separated from the features that are not responsible for alignment (nra features). by only allowing the ra features to participate in the deformation field prediction, we can eliminate the interference of the nra features to improve the registration accuracy and efficiency. the optimization process of the dcsc model to separate the ra and nra features is then turned into a deep network, namely interpretable multi-modal image registration network (inmir-net). to ensure the accurate separation of ra and nra features, we further design an accompanying guidance network (ag-net) to supervise the extraction of ra features in inmir-net. the advantage of inmir-net is that it provides a universal framework to tackle both rigid and non-rigid multi-modal image registration tasks. extensive experimental results verify the effectiveness of our method on both rigid and non-rigid registrations on various multi-modal image datasets, including rgb/depth images, rgb/near-infrared (nir) images, rgb/multi-spectral images, t1/t2 weighted magnetic resonance (mr) images and computed tomography (ct)/mr images. the codes are available at https://github.com/lep990816/interpretable-multi-modal-image-registration.",AB_0385
"this paper addresses the problem of face video inpainting. existing video inpainting methods target primarily at natural scenes with repetitive patterns. they do not make use of any prior knowledge of the face to help retrieve correspondences for the corrupted face. they therefore only achieve sub-optimal results, particularly for faces under large pose and expression variations where face components appear very differently across frames. in this paper, we propose a two-stage deep learning method for face video inpainting. we employ 3dmm as our 3d face prior to transform a face between the image space and the uv (texture) space. in stage i, we perform face inpainting in the uv space. this helps to largely remove the influence of face poses and expressions and makes the learning task much easier with well aligned face features. we introduce a frame-wise attention module to fully exploit correspondences in neighboring frames to assist the inpainting task. in stage ii, we transform the inpainted face regions back to the image space and perform face video refinement that inpaints any background regions not covered in stage i and also refines the inpainted face regions. extensive experiments have been carried out which show our method can significantly outperform methods based merely on 2d information, especially for faces under large pose and expression variations. project page: https://ywq.github.io/fvip.",AB_0385
"and efficient polyp segmentation plays a crucial role in colonoscopy, which is important for the prevention of colorectal cancer. despite cnn-based methods have achieved great progress in the polyp segmentation task, they are incapable of modeling long-range dependencies. transformer-based models utilize self-attention mechanism to overcome this problem while suffering from heavy computing cost. benefiting from simple structures, mlp-based models seem to be an alternative. however, they struggle with dealing with flexible input scales and modeling long-term dependencies. both of these two factors are important for image segmentation, which could explain why the mlp architecture performs poorly compared to the transformer. to remedy this issue, we propose a novel polyp mixer, which utilizes mlp-based structures in both encoder and decoder. in particular, we use cyclemlp as the encoder to overcome the fixed input scale issue. besides, we propose a multi-head mixer by converting the current cyclemlp into a multi-head fashion, allowing our model to explore rich context information from various subspaces. in addition, we build a powerful contextual bridger module between the encoder and decoder, which can capture semantics from larger receptive fields and combine them with various decoder layers. experiments demonstrate the proposed method with fewer parameters (similar to 16m) achieves sota on 4 public benchmarks. our code will be released at https://github.com/shijinghuihub/polyp-mixer",AB_0385
"alignment (lca) is an important preprocessing procedure for fusing lidar and camera data. for it, one key issue is to extract unified cross-modality representation for characterizing the heterogeneous lidar and camera data effectively and robustly. the main challenge is to resist the modality gap and visual data degradation during feature learning, while still maintaining strong representative power. to address this, a novel modality adapted local-to-global representation learning method is proposed. the research efforts are paid in 2 main folders via modality adaptation and capturing global spatial context. first for modality gap resistance, lidar and camera data is projected into the same depth map domain for unified representation learning. particularly, lidar data is converted to depth map according to pre-acquired extrinsic parameters. thanks to the recent advantage of deep learning based monocular depth estimation, camera data is transformed into depth map in data driven manner, which is jointly optimized with lca. secondly to capture global spatial context, vit (vision transformer) is introduced to lca. the concept of lca token is proposed for aggregating the local spatial patterns to form global spatial representation with transformer encoding. and, it is shared by all the samples. in this way, it can involve global sample-level information to leverage generalization ability. the experiments on kitti dataset verify superiority of our proposition. furthermore, the proposed approach is more robust to camera data degeneration (e.g., imaging blurring and noise) often faced by the practical applications. under some challenging test cases, the performance advancement of our method is over 1.9 cm/4.1(? )on translation / rotation error. while our model size (8.77m) is much smaller than existing methods (e.g., lccnet of 66.75m). the source code will be released at https://github.com/zaf233/rlca upon acceptance.",AB_0385
"the recent success of learning-based image rain and noise removal can be attributed primarily to well-designed neural network architectures and large labeled datasets. however, we discover that current image rain and noise removal methods result in low utilization of images. to alleviate the reliance of deep models on large labeled datasets, we propose the task-driven image rain and noise removal (trnr) based on a patch analysis strategy. the patch analysis strategy samples image patches with various spatial and statistical properties for training and can increase image utilization. furthermore, the patch analysis strategy encourages us to introduce the n-frequency-k-shot learning task for the task-driven approach trnr. trnr allows neural networks to learn from numerous n-frequency-k-shot learning tasks, rather than from a large amount of data. to verify the effectiveness of trnr, we build a multi-scale residual network (msresnet) for both image rain removal and gaussian noise removal. specifically, we train msresnet for image rain removal and noise removal with a few images (for example, 20.0% train-set of rain100h). experimental results demonstrate that trnr enables msresnet to learn more effectively when data is scarce. trnr has also been shown in experiments to improve the performance of existing methods. furthermore, msresnet trained with a few images using trnr outperforms most recent deep learning methods trained data-driven on large labeled datasets. these experimental results have confirmed the effectiveness and superiority of the proposed trnr. the source code is available on https://github.com/schizophreni/msresnet-trnr.",AB_0385
