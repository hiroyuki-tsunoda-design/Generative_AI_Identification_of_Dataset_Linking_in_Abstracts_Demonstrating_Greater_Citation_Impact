AB,NO
"this paper focuses on the unsupervised salient object detection (usod) issue. we come up with a two-stage activation-to-saliency (a2s) framework that effectively excavates saliency cues to train a robust saliency detector. it is worth noting that our method does not require any manual annotation in the whole process. in the first stage, we transform an unsupervisedly pre-trained network to aggregate multi-level features into a single activation map, where an adaptive decision boundary (adb) is proposed to assist the training of the transformed network. moreover, a new loss function is proposed to facilitate the generation of high-quality pseudo labels. in the second stage, a self-rectification learning strategy is developed to train a saliency detector and refine the pseudo labels online. in addition, we construct a lightweight saliency detector using two residual attention modules (rams) to learn robust saliency information. extensive experiments on several sod benchmarks prove that our framework reports significant performance compared with existing usod methods. moreover, training our framework on 3,000 images consumes about 1 hour, which is over 10 times faster than previous state-of-the-art methods. code has been published at https://github.com/moothes/a2s-usod.",AB_0373
"the state of the arts (sotas) of single image super-resolution always exploit guidance from gradient prior. the fusion of gradient guidance is implemented by channel-wise concatenation followed by a convolutional layer. however, the kernels sharing in spatial positions cannot adaptively tune the effect of gradient guidance for all feature positions. to resolve this problem, a novel network module is proposed to simulate the traditional joint trilateral filter (jtf) by extending the definition domain from pixels to features. moreover, to improve the efficiency and flexibility, the functions of jtf kernel generation for image features and gradient features are explicitly learned instead of individual kernel weights, e.g., the exponential functions in the traditional jtf. based on the proposed jtf modules, this paper follows the gradient-guided framework which simultaneously infers high-resolution (hr) image features and hr gradient features within two parallel branches, respectively. specifically, by treating image features and gradient features as cross guidance to each other, the proposed jtf modules adaptively adjust the fusion patterns for local features via a bi-directional way. by doing so, the quality of image features and gradient features is alternatively enhanced. compared with sotas, the proposed jtf-sisr shows improvement which is evaluated for multiple upsampling scales and degradation modes on 5 synthetic datasets, i.e., set5, set14, b100, urban100 and manga109, and 1 real dataset, i.e., realsrset. the code is public in https://github.com/a239xjc/jtf-sisr.",AB_0373
"the high-resolution transformer (hrformer) can maintain high-resolution representation and share global receptive fields. it is friendly towards salient object detection (sod) in which the input and output have the same resolution. however, two critical problems need to be solved for two-modality sod. one problem is two-modality fusion. the other problem is the hrformer output's fusion. to address the first problem, a supplementary modality is injected into the primary modality by using global optimization and an attention mechanism to select and purify the modality at the input level. to solve the second problem, a dual-direction short connection fusion module is used to optimize the output features of hrformer, thereby enhancing the detailed representation of objects at the output level. the proposed model, named hrtransnet, first introduces an auxiliary stream for feature extraction of supplementary modality. then, features are injected into the primary modality at the beginning of each multi-resolution branch. next, hrformer is applied to achieve forwarding propagation. finally, all the output features with different resolutions are aggregated by intra-feature and inter-feature interactive transformers. application of the proposed model results in impressive improvement for driving two-modality sod tasks, e.g., rgb-d, rgb-t, and light field sod.https://github.com/liuzywen/hrtransnet",AB_0373
"visual question answering (vqa) has made great progress recently due to the increasing ability to under-stand and encode multi-modal inputs based on deep learning. however, existing vqa models are usually based on assumptions of clean labels, and it is contradictory to real scenarios where labels are expen-sive and inevitably contain noises. in this paper, we take the lead in addressing this issue by establish-ing the first benchmark of controlled semantic noisy labels for vqa task, evaluating existing methods, and coming up with corresponding solutions. specifically, through analyzing human labels of existing vqa datasets, we first design a controlled semantic label noise by imitating human mislabeling behavior, which is more reasonable than conventional random noise. then, we evaluate several popular vqa mod -els on these new benchmark datasets and show that their performance degrades significantly compared to the original setting. to this end, we propose a semantic noisy label correction (snlc) to mitigate impacts of noisy labels, including a semantic cross-entropy (sce) loss and a semantic embedding con-trastive (sec) loss. extensive experiments demonstrate the effectiveness of the proposed method snlc. the proposed approach achieves a stable improvement on several existing models. the source code is available at https://github.com/zchoi/snlc .(c) 2023 published by elsevier ltd.",AB_0373
"multi-target tracking is a complex problem in computer vision, involving occlusion, target trajectory disappearance, etc. in response to these questions, we first propose a trajectory coordinate bi-directional sensing attention module (basiccamodule). this module enables the network to be more focused on essential areas. secondly, we propose the feature confusion edge enhancement (fcee) module, which enhances the confusion region of adjacent feature edges. after detecting the network output feature map, we propose the adaptive feature separation header (fdh) module. this module can activate essential features adaptively to reduce the number of target identity switches. finally, appropriate hyperparameters are set and optimized for the bytetrack tracking algorithm, resolving fragment tracking loss and long memory matching problems. after a large amount of experimental data, our approach is highly competitive with the existing state-of-the-art algorithms. moreover, it can also keep online tracking while maintaining high tracking accuracy. the code is publicly available at: https://github .com / cccj23 /spatio -temporal -trajectory-tracking (c) 2023 elsevier inc. all rights reserved.",AB_0373
"single image-based dehazing has achieved remarkable progress with the development of deep learning technologies. end-to-end neural networks have been proposed to learn a direct hazy-to-clear image translation to recover the clear structures and edges cues from the hazy inputs. however, the frequency domain information is explored insufficiently and lots of intermediate structure and texture related cues of current dehazing networks are ignored, which limits the performances of current approaches. to handle these limitations mentioned above, a wavelet spatial attention based multi-stream feedback network (wsamf-net) is proposed for effective single image dehazing. specifically, the proposed wavelet spatial attention utilizes both frequency-domain and spatial-domain information to enhance the extracted features for better structures and edges. meanwhile, an enhanced multi-stream based cross feature fusion strategy, including vertical and horizontal attentions, is proposed to reweight and fuse the intermediate features of each stream to acquire more meaningful aggregated features, while the weight sharing strategy is used to achieve a good trade-off between performance and parameters. besides, feedback mechanism is also designed to provide strong reconstruction ability. furthermore, we propose a critical real-world industrial dataset (ids) with images captured in real-world industrial quarry scenarios for research uses. extensive experiments on various benchmarking datasets, including both synthetic and real-world datasets, demonstrate the superiority of our wsamf-net over state-of-the-art single image dehazing methods. the ids dataset will be available at https://github.com/xbsong/ids-datasethttps://github.com/xbsong/ids-dataset.",AB_0373
"nowadays, deep learning has made rapid progress in the field of multi-exposure image fusion. however, it is still challenging to extract available features while retaining texture details and color. to address this difficult issue, in this paper, we propose a coordinated learning network for detail-refinement in an end-to-end manner. firstly, we obtain shallow feature maps from extreme over/under-exposed source images by a collaborative extraction module. secondly, smooth attention weight maps are generated under the guidance of a self-attention module, which can draw a global connection to correlate patches in different locations. with the cooperation of the two aforementioned used modules, our proposed network can obtain a coarse fused image. moreover, by assisting with an edge revision module, edge details of fused results are refined and noise is suppressed effectively. we conduct subjective qualitative and objective quantitative comparisons between the proposed method and twelve state-of-the-art methods on two available public datasets, respectively. the results show that our fused images significantly outperform others in visual effects and evaluation metrics. in addition, we also perform ablation experiments to verify the function and effectiveness of each module in our proposed method. the source code can be achieved at https://github.com/lok-18/lcndr.",AB_0373
"recent works have shown that optical flow can be learned by deep networks from unlabelled image pairs based on brightness constancy assumption and smoothness prior. current approaches additionally impose an augmentation regularization term for continual self-supervision, which has been proved to be effective on difficult matching regions. however, this method also amplify the inevitable mismatch in unsupervised setting, blocking the learning process towards optimal solution. to break the dilemma, we propose a novel mutual distillation framework to transfer reliable knowledge back and forth between the teacher and student networks for alternate improvement. concretely, taking estimation of off-the-shelf unsupervised approach as pseudo labels, our insight locates at defining a confidence selection mechanism to extract relative good matches, and then add diverse data augmentation for distilling adequate and reliable knowledge from teacher to student. thanks to the decouple nature of our method, we can choose a stronger student architecture for sufficient learning. finally, better student prediction is adopted to transfer knowledge back to the efficient teacher without additional costs in real deployment. rather than formulating it as a supervised task, we find that introducing an extra unsupervised term for multi-target learning achieves best final results. extensive experiments show that our approach, termed mdflow, achieves state-of-the-art real-time accuracy and generalization ability on challenging benchmarks. code is available at https://github.com/ltkong218/mdflow.",AB_0373
"recent works have achieved remarkable performance for action recognition with human skeletal data by utilizing graph convolutional models. existing models mainly focus on developing graph convolutional operations to encode structural properties of a skeletal graph, whose topology is manually predefined and fixed over all action samples. some recent works further take sample-dependent relationships among joints into consideration. however, the complex relationships between arbitrary pairwise joints are difficult to learn and the temporal features between frames are not fully exploited by simply using traditional convolutions with small local kernels. in this paper, we propose a motif-based graph convolution method, which makes use of sample-dependent latent relations among non-physically connected joints to impose a high-order locality and assigns different semantic roles to physical neighbors of a joint to encode hierarchical structures. furthermore, we propose a sparsity-promoting loss function to learn a sparse motif adjacency matrix for latent dependencies in non-physical connections. for extracting effective temporal information, we propose an efficient local temporal block. it adopts partial dense connections to reuse temporal features in local time windows, and enrich a variety of information flow by gradient combination. in addition, we introduce a non-local temporal block to capture global dependencies among frames. our model can capture local and non-local relationships both spatially and temporally, by integrating the local and non-local temporal blocks into the sparse motif-based graph convolutional networks (smotif-gcns). comprehensive experiments on four large-scale datasets show that our model outperforms the state-of-the-art methods. our code is publicly available at https://github.com/wenyh1616/samotif-gcn.",AB_0373
"recently, much progress has been made in unsupervised denoising learning. however, existing methods more or less rely on some assumptions on the signal and/or degradation model, which limits their practical performance. how to construct an optimal criterion for unsupervised denoising learning without any prior knowledge on the degradation model is still an open question. toward answering this question, this work proposes a criterion for unsupervised denoising learning based on the optimal transport theory. this criterion has favorable properties, e.g., approximately maximal preservation of the information of the signal, whilst achieving perceptual reconstruction. furthermore, though a relaxed unconstrained formulation is used in practical implementation, we prove that the relaxed formulation in theory has the same solution as the original constrained formulation. experiments on synthetic and real-world data, including realistic photographic, microscopy, depth, and raw depth images, demonstrate that the proposed method even compares favorably with supervised methods, e.g., approaching the psnr of supervised methods while having better perceptual quality. particularly, for spatially correlated noise and realistic microscopy images, the proposed method not only achieves better perceptual quality but also has higher psnr than supervised methods. besides, it shows remarkable superiority in harsh practical conditions with complex noise, e.g., raw depth images. code is available at https://github.com/wangweisjtu/otur.",AB_0373
