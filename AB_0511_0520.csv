AB,NO
"background: since the beginning of the coronavirus disease 2019 pandemic, there has been an explosion of sequencing of the severe acute respiratory syndrome coronavirus 2 (sars-cov-2) virus, making it the most widely sequenced virus in the history. several databases and tools have been created to keep track of genome sequences and variants of the virus; most notably, the gisaid platform hosts millions of complete genome sequences, and it is continuously expanding every day. a challenging task is the development of fast and accurate tools that are able to distinguish between the different sars-cov-2 variants and assign them to a clade. results: in this article, we leverage the frequency chaos game representation (fcgr) and convolutional neural networks (cnns) to develop an original method that learns how to classify genome sequences that we implement into cougar-g, a tool for the clade assignment problem on sars-cov-2 sequences. on a testing subset of the gisaid, cougar-g achieved an 96.29% overall accuracy, while a similar tool, covidex, obtained a 77, 12% overall accuracy. as far as we know, our method is the first using deep learning and fcgr for intraspecies classification. furthermore, by using some feature importance methods, cougar-g allows to identify k-mers that match sars-cov-2 marker variants. conclusions: by combining fcgr and cnns, we develop a method that achieves a better accuracy than covidex (which is based on random forest) for clade assignment of sars-cov-2 genome sequences, also thanks to our training on a much larger dataset, with comparable running times. our method implemented in cougar-g is able to detect k-mers that capture relevant biological information that distinguishes the clades, known as marker variants. availability: the trained models can be tested online providing a fasta file (with 1 or multiple sequences) at https://huggingface. co/spaces/biaslab/sars-cov-2-classification-fcgr. cougar-g is also available at https://github.com/algolab/cougar-g under the gpl.",AB_0052
"identifying cancer type-specific driver mutations is crucial for illuminating distinct pathologic mechanisms across various tumors and providing opportunities of patient-specific treatment. however, although many computational methods were developed to predict driver mutations in a type-specific manner, the methods still have room to improve. here, we devise a novel feature based on sequence co-evolution analysis to identify cancer type-specific driver mutations and construct a machine learning (ml) model with state-of-the-art performance. specifically, relying on 28 000 tumor samples across 66 cancer types, our ml framework outperformed current leading methods of detecting cancer driver mutations. interestingly, the cancer mutations identified by sequence co-evolution feature are frequently observed in interfaces mediating tissue-specific protein-protein interactions that are known to associate with shaping tissue-specific oncogenesis. moreover, we provide pre-calculated potential oncogenicity on available human proteins with prediction scores of all possible residue alterations through user-friendly website (http://sbi.postech.ac.kr/w/cancerce). this work will facilitate the identification of cancer type-specific driver mutations in newly sequenced tumor samples.",AB_0052
"counterfactual explanations (ces) are a powerful means for understanding how decisions made by algorithms can be changed. researchers have proposed a number of desiderata that ces should meet to be practically useful, such as requiring minimal effort to enact, or complying with causal models. in this paper, we consider the interplay between the desiderata of robustness (i.e., that enacting ces remains feasible and cost-effective even if adverse events take place) and sparsity (i.e., that ces require only a subset of the features to be changed). in particular, we study the effect of addressing robustness separately for the features that are recommended to be changed and those that are not. we provide def-initions of robustness for sparse ces that are workable in that they can be incorporated as penalty terms in the loss functions that are used for discovering ces. to carry out our experiments, we create and release code where five data sets (commonly used in the field of fair and explainable machine learning) have been enriched with feature-specific anno-tations that can be used to sample meaningful perturbations. our experiments show that ces are often not robust and, if adverse perturbations take place (even if not worst-case), the intervention they prescribe may require a much larger cost than anticipated, or even become impossible. however, accounting for robustness in the search process, which can be done rather easily, allows discovering robust ces systematically. robust ces make ad-ditional intervention to contrast perturbations much less costly than non-robust ces. we also find that robustness is easier to achieve for the features to change, posing an impor-tant point of consideration for the choice of what counterfactual explanation is best for the user. our code is available at: https://github .com /marcovirgolin /robust -counterfactuals.(c) 2022 the author(s). published by elsevier b.v. this is an open access article under the cc by license ().",AB_0052
"herein, a robust and reproducible explainable artificial intelligence (xai) approach is presented, which allows prediction of developmental toxicity, a challenging human-health endpoint in toxicology. the application of xai as an alternative method is of the utmost importance with developmental toxicity being one of the most animal-intensive areas of regulatory toxicology. in this work, the established caesar (computer assisted evaluation of industrial chemical substances according to regulations) training set made of 234 chemicals for model learning is employed. two test sets, including as a whole 585 chemicals, were instead used for validation and generalization purposes. the proposed framework favorably compares with the state-of-the-art approaches in terms of accuracy, sensitivity, and specificity, thus resulting in a reliable support system for developmental toxicity ensuring informativeness, uncertainty estimation, generalization, and transparency. based on the extreme gradient boosting (xgb) algorithm, our predictive model provides easy interpretative keys based on specific molecular descriptors and structural alerts enabling one to distinguish toxic and nontoxic chemicals. inspired by the organisation for economic co-operation and development (oecd) principles for the validation of quantitative structure-activity relationships (qsars) for regulatory purposes, the results are summarized in a standard report in portable document format, enclosing also details concerned with a density-based model applicability domain and shap (shapley additive explanations) explainability, the latter particularly useful to better understand the effective roles played by molecular features. notably, our model has been implemented in tiresia (toxicology intelligence and regulatory evaluations for scientific and industry applications), a free of charge web platform available at http://tiresia.uniba.it.",AB_0052
"numerous knowledge graphs (kgs) are being created to make recommender systems (rss) not only intelligent but also knowledgeable. integrating a kg in the recommendation process allows the underlying model to extract reasoning paths between recommended products and already experienced products from the kg. these paths can be leveraged to generate textual explanations to be provided to the user for a given recommendation. however, the existing explainable recommendation approaches based on kg merely optimize the selected reasoning paths for product relevance, without considering any user-level property of the paths for explanation. in this paper, we propose a series of quantitative properties that monitor the quality of the reasoning paths from an explanation perspective, based on recency, popularity, and diversity. we then combine in-and post-processing approaches to optimize for both recommendation quality and reasoning path quality. experiments on three public data sets show that our approaches significantly increase reasoning path quality according to the proposed properties, while preserving recommendation quality. source code, data sets, and kgs are available at https://tinyurl.com/bdbfzr4n.(c) 2022 published by elsevier b.v.",AB_0052
"we tackle the cross-domain visual localization problem of estimating camera position and orientation from real images without three-dimensional (3d) spatial mapping or modeling. recent studies have shown suboptimal performance in this task owing to the photometric and geometric differences between synthetic and real images. in this study, we present a deep learning approach that uses a channel-wise transformer localization (ct-loc) framework. inspired by the human behavior of looking for structural landmarks to estimate one's location, ct-loc encodes the most salient features of task -relevant objects in target scenes. to evaluate the efficacy of the proposed method in a real-world application, we built a complex and large-scale dataset of the interior of the mechanical room during operations and conducted extensive performance comparisons with the publicly available state-of-the-art university of melbourne corridor and virtual kitti 2 datasets. compared with the otherwise best-performing bim-posenet indoor camera localization model, our method significantly reduces position and orientation errors through the application of attention weights and saliency maps while also learning only the visual structural patterns (e.g., floors and doors) that are most relevant to localization tasks. our model successfully ignores uninformative objects. this approach yields higher -level robust camera-pose regression localization results without requiring prebuilt maps. the code is available at https://github.com/kdaeho27/ct-loc.(c) 2022 elsevier ltd. all rights reserved.",AB_0052
"convolutional neural networks (cnns) with u-shaped architectures have dominated medical image segmen-tation, which is crucial for various clinical purposes. however, the inherent locality of convolution makes cnns fail to fully exploit global context, essential for better recognition of some structures, e.g., brain lesions. transformers have recently proven promising performance on vision tasks, including semantic segmentation, mainly due to their capability of modeling long-range dependencies. nevertheless, the quadratic complexity of attention makes existing transformer-based models use self-attention layers only after somehow reducing the image resolution, which limits the ability to capture global contexts present at higher resolutions. therefore, this work introduces a family of models, dubbed factorizer, which leverages the power of low-rank matrix factorization for constructing an end-to-end segmentation model. specifically, we propose a linearly scalable approach to context modeling, formulating nonnegative matrix factorization (nmf) as a differentiable layer integrated into a u-shaped architecture. the shifted window technique is also utilized in combination with nmf to effectively aggregate local information. factorizers compete favorably with cnns and transformers in terms of accuracy, scalability, and interpretability, achieving state-of-the-art results on the brats dataset for brain tumor segmentation and isles'22 dataset for stroke lesion segmentation. highly meaningful nmf components give an additional interpretability advantage to factorizers over cnns and transformers. moreover, our ablation studies reveal a distinctive feature of factorizers that enables a significant speed-up in inference for a trained factorizer without any extra steps and without sacrificing much accuracy. the code and models are publicly available at https://github.com/pashtari/factorizer.",AB_0052
"motivation: particle tracking is an important step of analysis in a variety of scientific fields and is particularly indispensable for the construction of cellular lineages from live images. although various supervised machine learning methods have been developed for cell tracking, the diversity of the data still necessitates heuristic methods that require parameter estimations from small amounts of data. for this, solving tracking as a linear assignment problem (lap) has been widely applied and demonstrated to be efficient. however, there has been no implementation that allows custom connection costs, parallel parameter tuning with ground truth annotations, and the functionality to preserve ground truth connections, limiting the application to datasets with partial annotations.results: we developed laptrack, a lap-based tracker which allows including arbitrary cost functions and inputs, parallel parameter tuning and ground-truth track preservation. analysis of real and artificial datasets demonstrates the advantage of custom metric functions for tracking score improvement from distance-only cases. the tracker can be easily combined with other python-based tools for particle detection, segmentation and visualization.availability and implementation: laptrack is available as a python package on pypi, and the notebook examples are shared at https://github.com/yfukai/laptrack. the data and code for this publication are hosted at https://github. com/noneqphyslivingmatterlab/laptrack-optimisation.",AB_0052
"a program for calculating atomic integrals over laguerre-type basis functions is provided. the a functions are used. they can generate complete orthonormal systems for bound states of atoms. the orthogonality property makes it possible to use large-scale basis sets without loss of accuracy. we have previously published the results of our nonrelativistic hartree-fock calculations for he-og. total energies were obtained to 30 significant figures for the atoms in the first to third periods. the integrals for this computation were calculated according to the analytical formulation given by freund and hill using a multiple-precision arithmetic package. this program for calculating integrals has been rewritten using gauss-laguerre quadrature. this version is twice as fast as the previous one while maintaining precision. since quadruple precision arithmetic implemented in fortran is used in this version, the program can be further accelerated by openmp parallelization. this version has been released.program summaryprogram title: aihfltfcpc library link to program files: https://doi .org /10 .17632 /t4nxzbyssc .1licensing provisions: mitprogramming language: fortran 95nature of problem: the basis set expansion method is widely used in studies of the electronic structure of atoms using the hartree-fock method or the post-hf method. the a functions which are able to generate complete orthonormal systems for bound states of atoms are used in the present paper. because of orthogonality, the number of expansion terms can be increased without problems arising from linear dependence, enabling accurate electronic structure calculations. however, evaluation of 2-electron repulsion integrals demands much cpu-time. it is therefore vital in studies of the electronic structure of atoms to calculate 2-electron integrals efficiently.solution method: numerical integration based on gauss-laguerre quadrature is used to evaluate efficiently 2-electron repulsion integrals over the a functions. for this purpose, quadruple precision arithmetic is used, implemented in the fortran programming language. accuracy to 30 significant figures is confirmed for the integrals generated, even when the principal quantum number n is up to 200. the most time-consuming step in generating the integral is multiply-add arithmetic in which the values of the integrands at abscissae (zeros of a laguerre polynomial) are multiplied by the weights. this step can be parallelized by openmp.(c) 2022 elsevier b.v. all rights reserved.",AB_0052
"motivation: most of the conventional deep neural network-based methods for drug-drug interaction (ddi) extraction consider only context information around drug mentions in the text. however, human experts use heterogeneous background knowledge about drugs to comprehend pharmaceutical papers and extract relationships between drugs. therefore, we propose a novel method that simultaneously considers various heterogeneous information for ddi extraction from the literature. results: we first construct drug representations by conducting the link prediction task on a heterogeneous pharmaceutical knowledge graph (kg) dataset. we then effectively combine the text information of input sentences in the corpus and the information on drugs in the heterogeneous kg (hkg) dataset. finally, we evaluate our ddi extraction method on the ddiextraction-2013 shared task dataset. in the experiment, integrating heterogeneous drug information significantly improves the ddi extraction performance, and we achieved an f-score of 85.40%, which results in state-of-the-art performance. we evaluated our method on the drugprot dataset and improved the performance significantly, achieving an f-score of 77.9%. further analysis showed that each type of node in the hkg contributes to the performance improvement of ddi extraction, indicating the importance of considering multiple pieces of information. availability and implementation: our code is available at https://github.com/tticoin/hkg-ddie.git contact: makoto-miwa@toyota-ti.ac.jp",AB_0052
