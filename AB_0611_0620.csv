AB,NO
"neural architecture search (nas), that automates the design process of high-performing neural network architectures, is a multi-objective optimization problem. a single ideal architecture, that optimizes both predictive performance (e.g., the network accuracy) and computational costs (e.g., the model size, the number of parameters, the number of floating-point operations), does not exist. instead, there is a pareto front of multiple candidate architectures where each one represents an optimal trade-off between the competing objectives. multi-objective evolutionary algorithms (moeas) are often employed to approximate such pareto-optimal fronts for nas problems. in this article, we introduce a local search method, namely potential solution improving (psi), that aims to improve certain potential solutions on approximation fronts to enhance the performance of moeas. the main bottleneck in nas is the considerable computation cost that incurs from having to train a large number of candidate architectures to evaluate their accuracy. recently, the synaptic flow has been proposed as a metric that relatively characterizes the performance of deep neural networks without running any training epoch. we thus propose that our psi method can make use of this training-free metric as a proxy for network accuracy during local search steps. we conduct experiments with the well-known moea non-dominated sorting genetic algorithm ii (nsga-ii) coupled with the training-free psi local search in solving nas problems created from the standard benchmarks nas-bench-101 and nas-bench-201. experimental results confirm the efficiency enhancements brought about by our proposed method, which reduces the computational cost by four times compared to the baseline approach. the source code for the experiments in the article can be found at: https://github.com/elo-lab/moenas-tf-psi.",AB_0062
"urban pollution is a massive global problem, especially in industrialized and developing nations. visual pollution is an issue concerned with the external noticeable appearance of the mod-ern urban areas causing human health disorders, emotional distress, driving distraction, environ-mental hazards, etc. amidst the plethora of different forms of environmental pollution, visual pollution deteriorates the aesthetics of an urban environment, endorsing the importance of research and assessing it from different dimensions. the main objective of this study is to initialize a new concept of automatic identification and classification of visible contaminants related to textile industries using computer vision techniques. in this work, deep learning techniques have been applied for the automatic detection and classification of three categories of textile-based visual pol-lutants, i.e., cloth garbage, advertising billboards and signages, and textile dyeing waste materials. initially, 1,709 visual pollutants images were obtained through web crawling of search engines. additionally, 954 images were collected from two local garments factories, roadside vendors and shopping malls of bangladesh. next, the dataset was manually annotated by an open-source label-ing tool. finally, various deep learning techniques, faster r-cnn, yolov5, and efficientdet, have been used to classify the obtained dataset automatically. the efficientdet framework achieved the best performance with 97% and 93% training and test accuracies, respectively. the yolov5 approach exhibits acceptable precision with a considerably lower number of epochs. the proposed automated classification system is expected to create future visual pollution ratings for the textile industries. consequently, the corresponding stakeholders (industry owners, government authorities, factory workers, etc.) can introduce regulatory frameworks and control the proliferation of visual pollution. the open-source images obtained by web crawling, locally collected visual pollutants dataset and implementation code of this work are available at: https://github.com/sadiaa-frin163/textile-visual-pollutants-dataset.(c) 2022 the authors. published by elsevier bv on behalf of faculty of engineering, alexandria university. this is an open access article under the cc by license ( 4.0/).",AB_0062
"this work describes a rhynchonellid and spire-bearing brachiopod fauna from the ixtaltepec formation of oaxaca, mexico. leiorhynchoidea perrilliatae, allorhynchus scientiana, and anthracospirifer oaxacaensis are new species. the specific determination, along with information of previously described taxa, allowed the establishment of precise relative ages of the different fossiliferous intervals (api-1 to api-8) of the formation. the occurrence of serpukhovian taxa in api-1 to api-3 allowed assignment of the strata to the upper mississippian. the presence of bashkirian species allowed the assignment of the rocks of api-5 and api-6 to the lower pennsylvanian. likewise, middle pennsylvanian brachiopods in api-7 and api-8 enabled correlation of the strata with the moscovian stage. this study shows that the ixtaltepec formation is represented by a succession of well-delimited serpukhovian, bashkirian, and moscovian rocks. regarding paleogeography, the brachiopod fauna displays clear taxonomic variations that concur with global geological changes that occurred between the serpukhovian to moscovian. in the serpukhovian intervals, we recorded numerous cosmopolitan taxa of tropical waters, coinciding with the migration pathway of the rheic ocean. for the bashkirian, we observed a north american provincialism; however, because of the presence of australian and south american species, it is proposed that the austropanthalassic-rheic corridor had a close connection with oaxaca. the main provincialism was observed in the moscovian association because most of those taxa have been reported from different localities in the united states. this study supports that the main resemblance between oaxacan and north american faunas continued until the pennsylvanian and not the mississippian, as was previously proposed. uuid: http://zoobank.org/181e49cf-a08a-4e99-8200-65f4a90dafcd",AB_0062
"the description of the audiovisual documents aims essentially at providing meaningful and explanatory information about their content. despite the multiple efforts made by several researchers to extract descriptions, the lack of pertinent semantic descriptions always persists. we introduce, in this paper, a new approach to improve the semantic descriptions of the cinematic audiovisual documents. to ensure a high description level, we combine different sources of information related to the content (the script of the movie and the superposed text of the image). this process is mainly based on a semantic segmentation algorithm. the structured topic model (stm) and the lscom ontology (http://www.ee.columbia.edu/in/dvmm/iscom/) (large scale concept ontologymultimedia) are adapted for knowledge and descriptions extraction. deep classification techniques, such as ls tm (long short-term memory) and softmax regression, are used to classify the generic topics into specific topics. the performance of the developed approach is assessed as follows. first, stm topic is adapted and evaluated using the cmu movie summary corpus. then, the topics detection and classification processes are applied and their results are compared to those provided by human judgments employing the movilens dataset. finally, quantitative evaluation is performed utilizing the m-vad (montreal video annotation dataset) [44] and mpii-md (large scale movie description datasets) [35] databases. the comparative study proves that the suggested approach outperforms the existing ones in terms of the precision of the obtained topics.",AB_0062
"there is a warning light for the loss of plant habitats worldwide that entails concerted efforts to conserve plant biodiversity. thus, plant species classification is crucial to address this environmental challenge. in recent years, there has been a considerable increase in studies related to plant taxonomy. while some researchers try to improve their recognition performance using novel approaches, others concentrate on computational optimization for their framework. in addition, a few studies are diving into feature extraction to gain significantly in terms of accuracy. this paper proposes an effective method for the leaf recognition problem. in our proposed approach, a leaf goes through some pre-processing to extract its refined color image, vein image, xy-projection histogram, handcrafted shape, texture features, and fourier descriptors. these attributes are then transformed into a better representation by neural network-based encoders before a support vector machine (svm) model is utilized to classify different leaves. overall, our approach performs a state-of-the-art result on the flavia leaf dataset, achieving the accuracy of 99.69% on test sets under random 10-fold cross-validation and bypassing the previous methods. another important contribution is the trade-offs in classification performance while minimizing the feature categories used. in order to tackle this challenge, we designed several empirical experiments to analyze the performance of different combinations of feature sources and choose the best combination for features for the main problem. we also release our codes (scripts are available at https://github.com/tayerquach/flavia_recognition) for contributing to the research community in the leaf classification problem.",AB_0062
"ethnopharmacological relevance: scorpion sting is a public health concern with limited clinical symptomatic treatment. the clinical treatment uses anti-scorpion antivenom and prazosin (alpha-adrenergic inhibitor), often in combination with insulin, to reduce scorpion venom-induced hyperglycemia and other complications. however, these therapies also possess some limitations, necessitating urgent exploration of ethnomedicines, mainly traditional medicinal plants, to treat scorpion stings. unfortunately, several conventional treatments are not scientifically validated, thus raising questions about their quality and utility. therefore, pharmacological reassessment of such medicinal plants to alleviate scorpion stings' complications is essential. aim of the study: the principal objectives of this study are to provide a brief overview of medically important scorpions of the world, outline the extant traditional practices, and comprehensively review plants used in conventional ethnic medicines to treat scorpion stings over time. modern technological advances in identifying and characterizing plant bioactive molecules are also mentioned in this review. materials and methods: the traditionally used medicinal plants against scorpion stings were reviewed from the available literature in the database. the plant list (http://www.theplantlist.org/) was used to validate the scientific names of the plants mentioned in this study. the search targeted literature on conventional treatments and crude plant extracts or their bioactive components with proven neutralization capacity against scorpion stings. search words used were 'scorpion sting,' 'treatment for a scorpion sting,' 'antivenom and scorpion sting,' 'traditional treatment for scorpion stings, and 'natural compounds against scorpion stings'. results: a list of more than 200 medicinal plants traditionally used in several countries for treating scorpion stings is presented in this review. though some myth-based remedies are practiced to treat scorpion stings, no empirical evidence exists to validate this aspect of traditional knowledge. only 38 traditional medicinal plant extracts have been tested under in-vivo and in-vitro conditions to determine their neutralization potency of scorpion envenomation. although a few bioactive plant constituents showing scorpion venom neutralization potency have been characterized, they are not yet commercially available for clinical application. conclusions: there is tremendous potential locked in medicinal plants' traditional knowledge for scorpion envenomation treatment. translating this knowledge into the clinical application will require pharmacological reassessment, in tandem with isolation and characterization of active compounds to prove their prophylactic prowess. almost equally important would be the formulation of stringent strategies to conserve such medicinal plants from overexploitation.",AB_0062
"one of the major challenges in drug development is having acceptable levels of efficacy and safety throughout all the phases of clinical trials followed by the successful launch in the market. while there are many factors such as molecular properties, toxicity parameters, mechanism of action at the target site, etc. that regulates the thera-peutic action of a compound, a holistic approach directed towards data-driven studies will invariably strengthen the predictive toxicological sciences. our quest for the current study is to find out various reasons as to why an investigational candidate would fail in the clinical trials after multiple iterations of refinement and optimization. we have compiled a dataset that comprises of approved and withdrawn drugs as well as toxic compounds and essentially have used time-split based approach to generate the training and validation set. five highly robust and scalable machine learning binary classifiers were used to develop the predictive models that were trained with features like molecular descriptors and fingerprints and then validated rigorously to achieve acceptable performance in terms of a set of performance metrics. the mean auc scores for all the five classifiers with the hold-out test set were obtained in the range of 0.66-0.71. the models were further used to predict the probability score for the clinical candidate dataset. the top compounds predicted to be toxic were analyzed to estimate different dimensions of toxicity. apparently, through this study, we propose that with the appropriate use of feature extraction and machine learning methods, one can estimate the likelihood of success or failure of investigational drugs candidates thereby opening an avenue for future trends in computational toxicological studies. the models developed in the study can be accessed at https://github.com/gnsastry/predicting_clinical_t rials.git.",AB_0062
"stroke, categorized under cardiovascular and circulatory diseases, is considered the second foremost cause of death worldwide, causing approximately 11% of deaths annually. stroke diagnosis using a computed tomography (ct) scan is considered ideal for identifying whether the stroke is hemorrhagic or ischemic. however, most methods for stroke classification are based on a single slice-level prediction mechanism, meaning that the most imperative ct slice has to be manually selected by the radiologist from the original ct volume. this paper proposes an integration of convolutional neural network (cnn), vision transformers (vit), and automl to obtain slice-level predictions as well as patient-wise prediction results. while the cnn with inductive bias captures local features, the transformer captures long-range dependencies between sequences. this collaborative local-global feature extractor improves upon the slice-wise predictions of the ct volume. we propose stroke-specific feature extraction from each slice-wise prediction to obtain the patient wise prediction using automl. while the slice-wise predictions helps the radiologist to verify close and corner cases, the patient-wise predictions makes the outcome clinically relevant and closer to real-world scenario. the proposed architecture has achieved an accuracy of 87% for single slice-level prediction and an accuracy of 92% for patient-wise prediction. for comparative analysis of slice-level predictions, standalone architectures of vgg-16, vgg-19, resnet50, and vit were considered. the proposed architecture has outperformed the standalone architectures by 9% in terms of accuracy. for patient-wise predictions, automl considers 13 different ml algorithms, of which 3 achieve an accuracy of more than 90%. the proposed architecture helps in reducing the manual effort by the radiologist to manually select the most imperative ct from the original ct volume and shows improvement over other standalone architectures for classification tasks. the proposed architecture can be generalized for volumetric scans aiding in the patient diagnosis of head and neck, lungs, diseases of hepatobiliary tract, genitourinary diseases, women's imaging including breast cancer and various musculoskeletal diseases. code for proposed stroke-specific feature extraction with the pre-trained weights of the trained model is available at: https://github.com/rishiraj-cs/strokevit_with_automl.",AB_0062
"alzheimer's disease (ad) is an age-related neurodegenerative disorder, which is the most common cause of dementia in elderly individuals. it is characterized by selective neuronal cell death that affects the brain area related to memory and learning. so far, various computational research targeting ad have been reported, but we are still far from finding a precise treatment strategy for ad. it appeared of interest to us to carry out a two-dimensional quantitative structure-activity relationship (2d-qsar) analysis against multiple targets of ad using large datasets to determine the essential structural features which are responsible for the inhibition of the enzymes/targets. in the present research, we have implemented 2d-qsar modeling against twelve major targets (ache, buche, bace1, 0-amyloid, 5-ht6, cdk-5, gamma-secretase, glutaminyl cyclase, gsk-30, mao-b, nmda and phosphodiester (pde10a) enzymes) of ad for the identifications of novel multitarget inhibitors. the models were used to check the applicability domain of a pool of similar to 19 million compounds obtained from the four chemical drug-like databases (zinc12, asinex, nci, and interbioscreen databases) and provided prioritized compounds for experimental detection of their performance as anti-alzheimer's drug. additionally, we have also developed the quantitative structure activity-activity relationship (qsaar) and selectivity-based models to explore the most important features contributing to the dual inhibition against the respective targets. further-more, we have also performed chemical read-across predictions using the read-across-v3.1 tool (https://dtclab. webs.com/software-tools), the results for the external validation metrics were found to be better than the 2d-qsar-derived predictions. furthermore, molecular docking experiments have been performed to understand the molecular interactions between ligands and enzymes at the atomic level, and the observations are compared with the structural features acquired from qsar models that justified the mechanistic aspect of binding phe-nomena. the proposed models and read-across hypotheses could be used as potential tools to identify essential molecular features for designing suitable drug(s) for alzheimer's therapy using rational design of multi-target inhibitors.",AB_0062
"exploring the transport properties of different materials brings new avenue for basic understanding of emergent phenomena and practical applications in many different fields. here, we report a program named as track (transport properties for correlated materials using kubo formalism) which is written in python 3 for calculating temperature dependent electrical conductivity, electronic part of thermal conductivity, seebeck coefficient and lorenz number. in this code, kubo linear-response formalism is utilized for computing these parameters using both interacting and non-interacting electronic structure methods. the formula for transport coefficients is accordingly modified to obtain the transport parameters under relaxation time approximation using band-theory. the basic inputs of this program are the structural information, dense k-points sampling in the irreducible part of the brillouin zone and the information of velocity matrix elements, which can be calculated using third-party ab-initio package. track is expected to calculate the transport properties of different class of materials. the code has been benchmarked by performing calculation on three different types of materials namely vanadium (v), fesi and lacoo3, which are metal, semiconductor and mott insulator, respectively. the temperature dependent behaviour of the transport coefficients for these materials show fairly good agreement with the corresponding experimental data.program summaryprogram title: trackcpc library link to program files: https://doi .org /10 .17632 /jdt9tfkt4v.1developer's repository link: https://sourceforge .net /projects /track-code /files /track .zip /downloadlicensing provisions: gplv3programming language: python3external routines/libraries: numpy, scipy, timenature of problem: calculating the transport parameters using the kubo linear-response formalism.solution method: we present a python3 open source code for calculating transport coefficients using kubo formalism. both interacting and non-interacting electronic structure methodology are possible to be used for computing transport parameters.(c) 2022 elsevier b.v. all rights reserved.",AB_0062
