AB,NO
"signal recognition technology is a currently active area in both civilian and military applications. recently, deep learning has aroused extensive attempts in radar signal recognition due to its remarkable capability of automatic feature extraction. however, existing radar signal recognition networks overly depend on the probability-based decision model, resulting in poor robustness. this paper develops a novel deep metric learning frame to enhance the robustness of the recognition system. first, a multiscale atrous pyramid network (mapnet) is proposed to efficiently learn high-resolution and distinct feature representation. second, a variance loss is designed to constrain the intra-class feature distribution in metric space. third, according to the distribution of training signals in metric space, recognition results are recalibrated to provide explicit rejection probabilities for unknowns. extensive experiments and evaluations demonstrate that the proposed model can accurately classify known signals while robustly identifying unknown signals. the signal database and model can be freely accessed at https://github .com /bryantky /mapnet.(c) 2023 elsevier inc. all rights reserved.",AB_0362
"accurate segmentation of polyps from colonoscopy images plays a critical role in the diagnosis and cure of colorectal cancer. although effectiveness has been achieved in the field of polyp segmentation, there are still several challenges. polyps often have a diversity of size and shape and have no sharp bound-ary between polyps and their surrounding. to address these challenges, we propose a novel cross-level feature aggregation network (cfa-net) for polyp segmentation. specifically, we first propose a boundary prediction network to generate boundary-aware features, which are incorporated into the segmentation network using a layer-wise strategy. in particular, we design a two-stream structure based segmentation network, to exploit hierarchical semantic information from cross-level features. furthermore, a cross-level feature fusion (cff) module is proposed to integrate the adjacent features from different levels, which can characterize the cross-level and multi-scale information to handle scale variations of polyps. further, a boundary aggregated module (bam) is proposed to incorporate boundary information into the segmen-tation network, which enhances these hierarchical features to generate finer segmentation maps. quan-titative and qualitative experiments on five public datasets demonstrate the effectiveness of our cfa-net against other state-of-the-art polyp segmentation methods. the source code and segmentation maps will be released at https://github.com/taozh2017/cfanet .(c) 2023 elsevier ltd. all rights reserved.",AB_0362
"despite the promising development of automatic visual inspection (avi) in the manufacturing indus-try, detecting small-sized defects with fewer pixels coverage remains a challenging problem due to its insufficient attention and lack of semantic information. most exsiting convolutional inspection methods overlook the long-range dependence of context and lack adaptive fusion strategies to exploit heteroge-neous features. to address these issues in avi, this paper proposes a novel contextual information and spatial attention based network (canet), which consists of two steps, namely cablock and laplacianfpn, for effective perception and exploitation of small defect features. specifically, cablock extracts seman-tic information with rich context by encoding spatial long-range dependence and decoding contextual information as channel-specific bias through a spatial attention encoder (sae) and a context block de-coder (cbd), respectively. laplacianfpn further performs adaptive feature fusion considering both feature consistency and heterogeneity via two parallel branches. as a benchmark, a self-built engine surface de-fects (esd) dataset collected in real industry containing 89.70% small defects is constructed. experimental results show that canet achieves map-50 improvements of 1.5% and 4.3% compared to state-of-the-art methods on neu-det and esd, which demonstrates the effectiveness of the proposed method. the code is now available at https://github.com/xiuqhou/canet .(c) 2023 elsevier ltd. all rights reserved.",AB_0362
"the inequality constraint in stochastic configuration networks (scns) is key to their universal ap-proximation capability. scns incrementally add new units to neural networks under a supervisory mechanism. however, with an increasing number of hidden nodes, the output weights which are eval-uated by the least squares method, are frequently unstable. this instability phenomenon (arbitrarily small perturbations leading to large changes in the solutions) can be classified as an ill-posed problem. instability is one of the most important manifestations of ill-posed problems, and the tikhonov regularization method, which is widely used to address ill-posed problems, is undesirable. this study examined the supervisory mechanism of scns and algebraic properties of the hidden output matrix, thereby proposing a new greedy stochastic configuration network (gscn) for ill-posed problems. first, under the premise of ensuring universal approximation capability, gscn tries to optimize the randomly assigned input weights and biases based on the hunter-prey optimization (hpo) algorithm. the optimized hidden parameters which can reduce residual error the most are selected for the newly added hidden nodes. second, singular value decomposition (svd) and orthogonal-triangular (qr) decomposition with column pivoting are introduced to extract a linearly independent subset of the hidden output matrix for ill-posed problems in scns. finally, one function approximation, six benchmark regressions, two classification datasets, and bearing fault diagnosis are employed to evaluate the performance of gscn. experimental results indicate that gscn shows superior performance with respect to fast convergence, generalization, and stability compared with other contrast methods. this code is available at https://github.com/gzu-public-bigdata/gscn. (c) 2023 elsevier b.v. all rights reserved.",AB_0362
"camera-based 3d object detectors are welcome due to their wider deployment and lower price than lidar sensors. we first revisit the prior stereo detector dsgn for its stereo volume construction ways for representing both 3d geometry and semantics. we polish the stereo modeling and propose the advanced version, dsgn++, aiming to enhance effective information flow throughout the 2d-to-3d pipeline in three main aspects. first, to effectively lift the 2d information to stereo volume, we propose depth-wise plane sweeping (dps) that allows denser connections and extracts depth-guided features. second, for grasping differently spaced features, we present a novel stereo volume - dual-view stereo volume (dsv) that integrates front-view and top-view features and reconstructs sub-voxel depth in the camera frustum. third, as the foreground region becomes less dominant in 3d space, we propose a multi-modal data editing strategy - stereo-lidar copy-paste, which ensures cross-modal alignment and improves data efficiency. without bells and whistles, extensive experiments in various modality setups on the popular kitti benchmark show that our method consistently outperforms other camera-based 3d detectors for all categories. code is available at https://github.com/chenyilun95/dsgn2.",AB_0362
"though network pruning receives popularity in reducing the complexity of convolutional neural networks (cnns), it remains an open issue to concurrently maintain model accuracy as well as achieve significant speedups on general cpus. in this paper, we propose a novel 1xn pruning pattern to break this limitation. in particular, consecutive n output kernels with the same input channel index are grouped into one block, which serves as a basic pruning granularity of our pruning pattern. our 1xn pattern prunes these blocks considered unimportant. we also provide a workflow of filter rearrangement that first rearranges the weight matrix in the output channel dimension to derive more influential blocks for accuracy improvements and then applies similar rearrangement to the next-layer weights in the input channel dimension to ensure correct convolutional operations. moreover, the output computation after our 1xn pruning can be realized via a parallelized block-wise vectorized operation, leading to significant speedups on general cpus. the efficacy of our pruning pattern is proved with experiments on ilsvrc-2012. for example, given the pruning rate of 50% and n=4, our pattern obtains about 3.0% improvements over filter pruning in the top-1 accuracy of mobilenet-v2. meanwhile, it obtains 56.04ms inference savings on cortex-a7 cpu over weight pruning. our project is made available at https://github.com/lmbxmu/1xn.",AB_0362
"deep neural networks executing with low precision at inference time can gain acceleration and compression advantages over their high-precision counterparts, but need to overcome the challenge of accuracy degeneration as the bit width decreases. this work focuses on under 4-bit quantization that has a significant accuracy degeneration. we start with ternarization, a balance between efficiency and accuracy that quantizes both weights and activations into ternary values. we find that the hard threshold a introduced in previous ternary networks for determining quantization intervals and the suboptimal solution of a limit the performance of the ternary model. to alleviate it, we present soft threshold ternary networks (sttn), which enables the model to automatically determine ternarized values instead of depending on a hard threshold. based on it, we further generalize the idea of soft threshold from ternarization to arbitrary bit-width, named soft threshold quantized networks (stqn). we observe that previous quantization relies on the rounding-to-nearest function, constraining the quantization solution space and leading to a significant accuracy degradation, especially in low-bit (= 3-bits) quantization. instead of relying on the traditional rounding to-nearest function, stqn is able to determine quantization intervals by itself adaptively. accuracy experiments on image classification, object detection and instance segmentation, as well as efficiency experiments on field-programmable gate array (fpga) demonstrate that the proposed framework can achieve a prominent tradeoff between accuracy and efficiency. code is available at: https://github.com/weixiangxu/sttn.",AB_0362
"robust matrix completion refers to recovering a low-rank matrix given a subset of the entries corrupted by gross errors, and has various applications since many real-world signals can be modeled as low-rank matrices. most of the existing methods only perform well for noise-free data or those with zero-mean white gaussian noise, and their performance will be degraded in the presence of outliers. in this paper, based on the factorization framework, we propose a novel robust matrix completion scheme via using the truncated-quadratic loss function, which is non-convex and non-smooth, and half-quadratic theory is adopted for its optimization. by introducing an auxiliary variable, half-quadratic optimization (ho) can transform the loss function into two tractable forms, that is, additive and multiplicative formulations. block coordinate descent method is then exploited as their solver. compared with the additive form, the multiplicative variant has lower computational cost since we attempt to take the observations contaminated by outliers as missing entries. numerical simulations and experimental results based on image inpainting and hyperspectral image recovery demonstrate that our algorithms are superior to the state-of-the-art methods in terms of restoration accuracy and runtime. matlab code is available at https://github.com/bestzywang.",AB_0362
"to mine the spectral-spatial information of target pixel in hyperspectral image classification (hsic), convolutional neural network (cnn)-based models widely adopt patch-based input pattern, where a patch represents its central pixel and the neighbor pixels play auxiliary roles in the classification process. however, compared to the central pixel, its neighbor pixels often have different contributions for classification. although many existing patch-based cnns could adaptively emphasize the spatial neighbor information, most of them ignore the latent relationship between the center pixel and its neighbor pixels. moreover, efficient spectral-spatial feature extraction has been a difficult yet vital topic for hsic. to address the mentioned problems, a central vector oriented self-similarity network (cvssn) is proposed for hsic. specifically, based on two similarity measures, we firstly design an adaptive weight addition based spectral vector self-similarity module (awa-svss) in input space and a euclidean distance based feature vector self-similarity module (ed-fvss) in feature space to fully mine the central vector oriented spatial relationships. besides, a spectral-spatial information fusion module (ssif) is formulated as a new pattern to fuse the central 1d spectral vector and the corresponding 3d patch for efficient spectral-spatial feature learning of the subsequent modules. moreover, we implement a channel spatial separation convolution module (css-conv) and a scale information complementary convolution module (sic-conv) for efficient spectral-spatial feature learning. extensive experimental results on four popular hsi data sets demonstrate the effectiveness and efficiency of the proposed method compared with other state-of-the-art methods. the source code is available at https://github.com/lms-07/cvssn.",AB_0362
"the robust reversible watermarking (rrw) requires high robustness and capacity on the condition of reversibility and imperceptibility, which still remains a big challenge nowadays. in this paper, we propose a two-stage rrw scheme that improves robustness and capacity through embedding optimization and rounded error compensation. the first stage inserts a robust watermark into the selected pseudo-zernike moments (pzms) by using an adaptive normalization method and an optimized embedding strategy. specifically, the adaptive normalization method achieves both an invariance to pixel amplitude variation and a balance between robustness and imperceptibility, and the optimized embedding strategy reduces embedding distortions remarkably. the watermarked pzms are inversely transformed to generate the robustly watermarked image, in which rounded errors caused in the inverse transformation is compensated elaborately and thus a larger capacity can be obtained at the same embedding distortion. the second stage embeds a reversible watermark consisting of errors between the robust watermark embedded image and the original one, aiming at achieving the reversibility in case of no attacks. extensive experimental simulations show that the proposed scheme provides strong robustness against common signal processing, including awgn, salt-and-pepper noise, jpeg, jpeg2000, median filtering, mean filtering, geometrical transformations involving rotation and scaling, and a compressive sensing attack exemplified by two-dimensional compressive sensing, which outperforms the state-of-the-art schemes. our code is available at https://github.com/yichao-tang/pzms-rrw.",AB_0362
