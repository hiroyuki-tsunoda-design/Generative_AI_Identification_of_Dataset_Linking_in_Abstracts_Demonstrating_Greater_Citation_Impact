AB,NO
"the increased availability of high-throughput technologies has enabled biomedical researchers to learn about disease etiology across multiple omics layers, which shows promise for improving cancer subtype identification. many computational methods have been developed to perform clustering on multi-omics data, however, only a few of them are applicable for partial multi-omics in which some samples lack data in some types of omics. in this study, we propose a novel multi-omics clustering method based on latent sub-space learning (mcls), which can deal with the missing multi-omics for clustering. we utilize the data with complete omics to construct a latent subspace using pca-based feature extraction and singular value decomposition (svd). the data with incomplete multi-omics are then projected to the latent subspace, and spectral clustering is performed to find the clusters. the proposed mcls method is evaluated on seven different cancer datasets on three levels of omics in both full and partial cases compared to several state-of-the-art methods. the experimental results show that the proposed mcls method is more efficient and effective than the compared methods for cancer subtype identification in multi-omics data analysis, which provides important references to a comprehensive understanding of cancer and biological mechanisms.availolity: the proposed method can be freely accessible at https://github.com/shangcs/mcls.",AB_0093
"ontology alignment is essential for data integration and interoperability across multiple applications across diverse disciplines. in recent decades, significant advancements have been made in the development of advanced methods and systems for ontology alignment. empirical results have suggested that ontological semantics can be effectively employed to enhance the alignment process. besides, structural information is crucial for ontology alignment as it reflects the relations among adjacent concepts in the ontology. previous works are mainly based on external lexicon and predefined rules based on ontological structure. recently, deep learning has imposed positive impacts on ontology alignment and obtained substantial improvement. this paper proposes a new method based on ontology embedding incorporating the semantic and structural features. it utilizes the distance between the embedding of two ontological concepts to be aligned as the criterion for alignment.the proposed method is used to align two widely used food ontologies and three chinese food classification ontologies. the experimental results show that our method enhances the performance compared to several state-of-the-art alignment systems, demonstrating the importance of learning semantic representation and structural representation. furthermore, the proposed method is evaluated on several different tracks of the ontology alignment evaluation initiative (oaei), and experimental results show that our method outperforms other baselines in effectiveness. the data and code can be obtained from: https://github.com/haozhigang1111/ ontology-alignment.git. & copy; 2023 elsevier b.v. all rights reserved.",AB_0093
"rna secondary structure is essential for predicting the tertiary structure and understanding rna function. recent research tends to stack numerous modules to design large deep-learning models. this can increase the accuracy to more than 70%, as well as significant training costs and prediction efficiency. we proposed a model with three feature extractors called gcnfold. structure extractor utilizes a three-layer graph convolutional network (gcn) to mine the structural information of rna, such as stems, hairpin, and internal loops. structure and sequence fusion embeds structural information into sequences with transformer encoders. long-distance dependency extractor captures long-range pairwise relationships by unet. the experiments indicate that gcnfold has a small number of parameters, a fast inference speed, and a high accuracy among all models with over 80% accuracy. additionally, gcnfold-small takes only 90ms to infer an rna secondary structure and can achieve close to 90% accuracy on average. the gcnfold code is available on github https://github.com/enbinyang/gcnfold.",AB_0093
"although optimal medical therapy (omt) after coronary revascularization is advocated for intensive secondary prevention, its criteria and effect on long-term outcomes are uncertain. using data from the asan-multivessel (asan medical center-multivessel revascularization) registry, we identified 8,311 patients who underwent coronary artery bypass grafting (cabg) (n = 3,115) or percutaneous coronary intervention (pci) (n = 5,196). omt was defined as the combination of minimum of 3 medications in 4 drug classes (antiplatelet drugs, statins, b blockers, and angiotensin-converting enzyme inhibitors or angiotensin receptor blockers). two primary outcomes were all-cause mortality and serious composite outcome of death, spontaneous myocardial infarction, or stroke at 10 years. of 8,311 patients, 4,321 (52.0%) followed omt. in the 3,397 propensity-score matched cohort, omt status compared with non-omt status was significantly associated with a lower risk of all-cause mortality (10.7% vs 18.7%; hazard ratio [hr] 0.55, 95% confidence interval [ci] 0.47 to 0.65) and serious composite outcome (14.5% vs 22.5%, hr 0.635, 95% ci 0.55 to 0.73) at 10 years. the association on 10-year mortality was more prominent in the pci group (hr 0.45, 95% ci 0.36 to 0.56) than in the cabg group (hr 0.72, 95% ci 0.58 to 0.90) with a significant interaction (p = 0.001). overall findings were consistent using different omt criteria (all 4 types of medications). in conclusion, omt significantly lowered the risks of mortality and major cardiovascular events at 10 years in patients with multivessel revascularization. the omt impact on mortality was more remarkable in the pci group than in the cabg group. this work was registered at http:// clinicaltrials.gov (identifier: nct02039752).& copy; 2023 elsevier inc. all rights reserved. (am j cardiol 2023;203:81-91)",AB_0093
"a video question answering task essentially boils down to how to fuse the information between text and video effectively to predict an answer. most works employ a transformer encoder as a crossmodal encoder to fuse both modalities by leveraging the full self-attention mechanism. due to the high computational cost of the self-attention and the high dimensional data of video, they either have to settle for: (1) only training the cross-modal encoder on offline-extracted video and text features or (2) training the cross-modal encoder with the video and text feature extractor, but only using sparselysampled video frames. training only from offline-extracted features suffers from the disconnection between the extracted features and the data of the downstream task because the video and text feature extractors are trained independently on different domains, e.g., action recognition for the video feature extractor and semantic classification for the text feature extractor. training using sparsely-sampled video frames might suffer from information loss if the video contains very rich information or has many frames. to alleviate those issues, we propose lightweight recurrent cross-modal encoder (lrce) that replaces the self-attention operation with a single learnable special token to summarize the text and video features. as a result, our model incurs a significantly lower computational cost. additionally, we perform a novel multi-segment sampling which sparsely samples the video frames from different segments of the video to provide more fine-grained information. through extensive experiments on three videoqa datasets, we demonstrate the lrce achieves significant performance gains compared to previous works. the code of our proposed method is available at https://github.com/sejong-vli/vqalrce-kbs-2023.",AB_0093
"phenotypes (i.e., symptoms and clinical signs) are essential for clinical diagnosis and research related to symptom science and precision health. as clinical observational manifestations of a disease, symptoms are clinically significant because they act as direct causes for patients to seek medical care and the primary indicators for clinicians to provide diagnosis/treatments. however, a comprehensive phenotypic knowledge base and high-quality symptom-gene associations are lacking. therefore, a thorough understanding of the relationships between symptoms and other entities is urgently needed to support scientific research and clinical health care. in this paper, we constructed a systematic, largescale, and high-quality symptom -gene associations network system named sympgan (accessible at http://www.sympgan.org/). we provide access to the database with millions of associations between symptoms, genes, diseases, and drugs, as well as the system for users to search, analyze, knowledge inference, and present data visualization. we utilize state-of-the-art machine learning and deep learning algorithms as the backbone to form the final dataset. in addition, we utilize the robertapubmed neural network for name entity recognition to assist in data screening. the knowledge graph is adopted to organize the relationships between different entities. we adopt conve, tucker, and hyper methods for knowledge completion experiments to validate the quality of final knowledge graph triples. based on the results, we provide online automatic knowledge inference interfaces. the system, sympgan, has promising value for disease diagnosis, decision support in health care, precision health, and scientific research, as researchers and practitioners can easily access information about symptoms, diseases, targets, gene ontology, and drugs.& copy; 2023 elsevier b.v. all rights reserved.",AB_0093
"using protein structure to predict function, interactions, and evolutionary history is still an open challenge, with existing approaches relying extensively on protein homology and families. here, we present machaon, a data-driven method combining orientation invariant metrics on phi-psi angles, inter-residue contacts and surface complexity. it can be readily applied on whole structures or segments-such as domains and binding sites. machaon was applied on sars-cov-2 spike monomers of native, delta and omicron variants and identified correlations with a wide range of viral proteins from close to distant taxonomy ranks, as well as host proteins, such as ace2 receptor. machaon's meta-analysis of the results highlights structural, chemical and transcriptional similarities between the spike monomer and human proteins, indicating a multi-level viral mimicry. this extended analysis also revealed relationships of the spike protein with biological processes such as ubiquitination and angiogenesis and highlighted different patterns in virus attachment among the studied variants. available at: https://machaonweb.com.",AB_0093
"the green fraction (gf), which is the fraction of green vegetation in a given viewing direction, is closely related to the light interception ability of the crop canopy. monitoring the dynamics of gf is therefore of great interest for breeders to identify genotypes with high radiation use efficiency. the accuracy of gf estimation depends heavily on the quality of the segmentation dataset and the accuracy of the image segmentation method. to enhance segmentation accuracy while reducing annotation costs, we developed a self-supervised strategy for deep learning semantic segmentation of rice and wheat field images with very contrasting field backgrounds. first, the digital plant phenotyping platform was used to generate large, perfectly labeled simulated field images for wheat and rice crops, considering diverse canopy structures and a wide range of environmental conditions (sim dataset). we then used the domain adaptation model cycle-consistent generative adversarial network (cyclegan) to bridge the reality gap between the simulated and real images (real dataset), producing simulation-to-reality images (sim2real dataset). finally, 3 different semantic segmentation models (u-net, deeplabv3+, and segformer) were trained using 3 datasets (real, sim, and sim2real datasets). the performance of the 9 training strategies was assessed using real images captured from various sites. the results showed that segformer trained using the sim2real dataset achieved the best segmentation performance for both rice and wheat crops (rice: accuracy = 0.940, f1-score = 0.937; wheat: accuracy = 0.952, f1-score = 0.935). likewise, favorable gf estimation results were obtained using the above strategy (rice: r2 = 0.967, rmse = 0.048; wheat: r2 = 0.984, rmse = 0.028). compared with segformer trained using a real dataset, the optimal strategy demonstrated greater superiority for wheat images than for rice images. this discrepancy can be partially attributed to the differences in the backgrounds of the rice and wheat fields. the uncertainty analysis indicated that our strategy could be disrupted by the inhomogeneity of pixel brightness and the presence of senescent elements in the images. in summary, our self-supervised strategy addresses the issues of high cost and uncertain annotation accuracy during dataset creation, ultimately enhancing gf estimation accuracy for rice and wheat field images. the best weights we trained in wheat and rice are available: https://github.com/phenix-lab/sim2real-seg.",AB_0093
"summary biological pattern formation is one of the complex system phenomena in nature, requiring theoretical analysis based on mathematical modeling and computer simulations for in-depth understanding. we propose a python framework named lpf to systematically explore the highly diverse wing color patterns of ladybirds using reaction-diffusion models. lpf supports gpu-accelerated array computing for numerical analysis of partial differential equation models, concise visualization of ladybird morphs, and evolutionary algorithms for searching mathematical models with deep learning models for computer vision. availability and implementation lpf is available on github at https://github.com/cxinsys/lpf.",AB_0093
"one of the key components of this research has been the mapping of antarctic bed topography and ice thickness parameters that are crucial for modelling ice flow and hence for predicting future ice loss and the ensuing sea level rise. supported by the scientific committee on antarctic research (scar), the bedmap3 action group aims not only to produce new gridded maps of ice thickness and bed topography for the international scientific community, but also to standardize and make available all the geophysical survey data points used in producing the bedmap gridded products. here, we document the survey data used in the latest iteration, bedmap3, incorporating and adding to all of the datasets previously used for bedmap1 and bedmap2, including ice bed, surface and thickness point data from all antarctic geophysical campaigns since the 1950s. more specifically, we describe the processes used to standardize and make these and future surveys and gridded datasets accessible under the findable, accessible, interoperable, and reusable (fair) data principles. with the goals of making the gridding process reproducible and allowing scientists to re-use the data freely for their own analysis, we introduce the new scar bedmap data portal (https://bedmap.scar.org, last access: 1 march 2023) created to provide unprecedented open access to these important datasets through a web-map interface. we believe that this data release will be a valuable asset to antarctic research and will greatly extend the life cycle of the data held within it. data are available from the uk polar data centre: https://data.bas.ac.uk (last access: 5 may 2023 ). see the data availability section for the complete list of datasets.",AB_0093
