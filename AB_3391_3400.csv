AB,NO
"multi-view stereo (mvs) is a fundamental problem in geometric computer vision which aims to reconstruct a scene using multi-view images with known camera parameters. however, the mainstream approaches represent the scene with a fixed all-pixel depth range and equal depth interval partition, which will result in inadequate utilization of depth planes and imprecise depth estimation. in this paper, we present a novel multi-stage coarse to-fine framework to achieve adaptive all-pixel depth range and depth interval. we predict a coarse depth map in the first stage, then an adaptive depth range prediction module is proposed in the second stage to zoom in the scene by leveraging the reference image and the obtained depth map in the first stage and predict a more accurate all-pixel depth range for the following stages. in the third and fourth stages, we propose an adaptive depth interval adjustment module to achieve adaptive variable interval partition for pixel-wise depth range. the depth interval distribution in this module is normalized by z-score, which can allocate dense depth hypothesis planes around the potential ground truth depth value and vice versa to achieve more accurate depth estimation. extensive experiments on four widely used benchmark datasets (dtu, tnt, blendedmvs, eth 3d) demonstrate that our model achieves state-of-the-art performance and yields competitive generalization ability. particularly, our method achieves the highest acc and overall on the dtu dataset, while attaining the highest recall and f1-score on the tanks and temples intermediate and advanced dataset. moreover, our method also achieves the lowest e1 and e3 on the blendedmvs dataset and the highest acc and f1-score on the eth 3d dataset, surpassing all listed methods. project website: https://github.com/zs670980918/arai-mvsnet",AB_0340
"accurate user modeling is crucial for point-of-interest (poi) recommendation as it can significantly improve user satisfaction with recommended pois and enrich user experience. however, existing methods typically rely on simple time-series models for user check-in sequences, which ignore similar information of global users and fail to capture the user-preference knowledge hidden in complex social networks. to address this issue, we propose a novel knowledge-driven and user-aware poi recommendation method called kdrank. first, we construct a knowledge graph containing users' personal attributes for poi recommendation, which can reflect users' historical check-in preferences. second, we derive users' knowledge representations using a cross-embedding method, which facilitates feature interaction by sharing information between segments of knowledge representations to achieve a more precise representation of low-dimensional embedding. third, we propose a knowledge aggregation module to combine users' knowledge and historical check-in features to achieve knowledge enhancement of check-in data. furthermore, to enhance global user awareness of our model, we introduce an attention mechanism that focuses on the most similar and significant users in the global context. it allows kdrank to capture more personalized user preferences and increases the precision of poi recommendations. the effectiveness of the proposed method was evaluated on two real datasets, and the results indicated its ability to increase the poi recommendation accuracy. the code associated with this study is available at https://github.com/itshardtocode/kdrank. & copy; 2023 elsevier b.v. all rights reserved.",AB_0340
"scene graph generation (sgg) is a sophisticated task that suffers from both complex visual features and the long-tail problem. recently, various unbiased strategies have been proposed by designing novel loss functions and data balancing strategies. unfortunately, these unbiased methods fail to emphasize language priors in the feature refinement perspective. inspired by the fact that predicates are highly correlated with semantics hidden in subject-object pair and global context, we propose landmark (language-guided representation enhancement framework) that learns predicate-relevant representations from language-vision interactive patterns, global language context, and object-predicate correlation. specifically, we first project object labels to three distinctive semantic embeddings for different representation learning. then, language attention module (lam) and experience estimation module (eem) processes subject-object word embeddings to attention vector and predicate distribution, respectively. language context module (lcm) encodes global context from each word embedding, which avoids isolated learning from local information. finally, module outputs are used to update visual representations and the sgg model's prediction. all language representations are purely generated from object categories so that no extra knowledge is needed. this framework is model-agnostic and consistently improves performance on existing sgg models. besides, representation-level unbiased strategies endow landmark with compatibility of other methods. code is available at https://github.com/rafa-cxg/pysgg-cxg.",AB_0340
"heterogeneous information networks (hins) have been proven to be powerful in modeling various real-world networks, such as academic networks and social media networks. by distinguishing the types of relationships and nodes, hins obtain the capability to model multifarious data, while bringing challenges to data mining and analysis. specifically, how to comprehensively mine the semantic information in hins remains an open question. although recently studied network schemas such as meta-paths or meta-graphs have achieved empirical success, they still have limitations in modeling the complex underlying relations to facilitate the graph neural networks, e.g., different schemas may overlap in semantics and lead to redundant computations. to address this issue, we leverage a recently introduced higher-order network schema known as the motif clique (abbreviated as m-clique). this schema offers greater expressiveness, enabling us to effectively construct the semantic neighborhood of nodes. we further propose a novel heterogeneous graph attention network with m cliques, named hamc, which employs a two-level attention mechanism (node-level and semantic-level) to learn node representations. the two-level attention measures the importance of neighbors and m-cliques for each node, respectively. extensive experiments demonstrate that the proposed hamc outperforms the state-of-theart methods in many heterogeneous network analytic tasks such as node classification and clustering. our code is available at https://github.com/wcx21/hamc-heterogeneous-graph-attention-network-with-motif-clique.",AB_0340
"generalized zero-shot learning (gzsl) aims to recognize objects from both seen and unseen categories by transferring semantic knowledge and merely utilizing seen class data for training. recent feature generation methods in the 2d image domain have made great progress. however, very little is known about its usefulness in 3d point cloud zero-shot learning. this work aims to facilitate research on 3d point cloud generalized zero-shot learning. different from previous works, we focus on synthesizing the more high-level discriminative point cloud features. to this end, we design a representation enhancement strategy to generate the features. specifically, we propose a contrastive generative network with recursive -loop, termed as cgrl, which can be leveraged to enlarge the inter-class distances and narrow the intra-class gaps. by applying the contrastive representations to the generative model in a recursive-loop form, it can provide the self-guidance for the generator recurrently, which can help yield more discriminative features and train a better classifier. to validate the effectiveness of the proposed method, extensive experiments are conducted on three benchmarks, including modelnet40, mcgill, and scanobjectnn. experimental evaluations demonstrate the superiority of our approach and it can outperform the state-of-the-arts by a large margin. code is available at https://github.com/photon-git/cgrl",AB_0340
"visible-infrared person re-identification (vi-reid) has been challenging due to the existence of large discrepan-cies between visible and infrared modalities. most pioneering approaches reduce intra-modality variations and inter-modality discrepancies by learning modality-shared features. however, an explicit modality-shared cue, i.e., body keypoints, has not been fully exploited in vi-reid. additionally, existing feature learning paradigms imposed constraints on either global features or partitioned feature stripes, which neglect the prediction consistency of global and part features. to address the above problems, we exploit pose estimation as an auxiliary learning task to assist vi-reid in an end-to-end framework. by jointly training these two tasks in a mutually beneficial manner, our model learns higher quality id-related features. on top of it, the learnings of global features and local features are seamlessly synchronized by hierarchical feature constraint (hfc), where the former supervises the latter using the knowledge distillation strategy. experimental results on two benchmark vi-reid datasets show that the proposed method consistently improves state-of-the-art methods by significant margins. specifically, our method achieves nearly 20% map improvements against the state-of-the-art method on the regdb dataset. our intriguing findings highlight the usage of auxiliary task learning in vi-reid. our source code is available at https://github.com/yoqim/pose_vireid.",AB_0340
"the inherent ambiguity in ground-truth annotations of 3d bounding boxes, caused by occlusions, signal missing, or manual annotation errors, can confuse deep 3d object detectors during training, thus deteriorating detection accuracy. however, existing methods overlook such issues to some extent and treat the labels ass deterministic. in this paper, we formulate the label uncertainty problem as the diversity of potentially plausible bounding boxes of objects. then, we propose glenet, a generative framework adapted from conditional variational autoencoders, to model the one-to-many relationship between a typical 3d object and its potential ground-truth bounding boxes with latent variables. the label uncertainty generated by glenet is a plug-and-play module and can be conveniently integrated into existing deep 3d detectors to build probabilistic detectors and supervise the learning of the localization uncertainty. besides, we propose an uncertainty-aware quality estimator architecture in probabilistic detectors to guide the training of the iou-branch with predicted localization uncertainty. we incorporate the proposed methods into various popular base 3d detectors and demonstrate significant and consistent performance gains on both kitti and waymo benchmark datasets. especially, the proposed glenet-vr outperforms all published lidar-based approaches by a large margin and achieves the top rank among single-modal methods on the challenging kitti test set. the source code and pre-trained models are publicly available at https://github.com/eaphan/glenet.",AB_0340
"infrared and visible image fusion aims to highlight the infrared target and preserve valuable texture details as much as possible. however, the infrared target needs to be more apparent in most image fusion methods. a large amount of infrared noise remains in the fusion results, significantly reducing the proportion of valuable texture details in the fusion results. how to highlight the salient of infrared targets, lower noise, and retain more valuable texture details in the fusion results still need to be solved. we propose an adaptive salient region analysis method based on superpixels (ssra) for infrared and visible fusion to solve this problem. this method uses salient region analysis based on superpixels to highlight the salience region effectively. we design a texture detail fusion method based on brightness analysis of the visible image to suppress noise and keep more meaningful texture detail information. the experimental results show that our proposed method performs better in subjective vision and quantitative evaluation than some advanced methods. in addition, we also demonstrate that ssra is capable of supporting high-level visual tasks well. our code is publicly available at: https://github.com/vcmhe/ssra.",AB_0340
"learning multi-label image recognition with incomplete annotation is gaining popularity due to its superior performance and significant labor savings when compared to training with fully labeled datasets. existing literature mainly focuses on label completion and co-occurrence learning while facing difficulties with the most common single-positive label manner. to tackle this problem, we present a semantic contrastive bootstrapping (scob) approach to gradually recover the cross-object relationships by introducing class activation as semantic guidance. with this learning guidance, we then propose a recurrent semantic masked transformer to extract iconic object-level representations and delve into the contrastive learning problems on multi-label classification tasks. we further propose a bootstrapping framework in an expectation-maximization fashion that iteratively optimizes the network parameters and refines semantic guidance to alleviate possible disturbance caused by wrong semantic guidance. extensive experimental results demonstrate that the proposed joint learning framework surpasses the state-of-the-art models by a large margin on four public multi-label image recognition benchmarks. codes can be found at https:// github.com/ icvteam/ scob.",AB_0340
"one-shot semantic segmentation approaches aim to learn a meta-learning framework from seen classes with annotated samples, which can be applied in novel classes with only one annotated sample. however, most existing works still face the challenge of reduced generalization capability on novel classes due to two reasons: utilizing only foreground and background prototypes generated from support samples may lead to semantic bias from the model's perspective, and negative support-query pairs may result in spatial inconsistency from the data's perspective. to alleviate the semantic bias problem, we propose a multi-view prototype learning paradigm to reduce the appearance discrepancy between support and query images. in addition to the classical foreground and background prototypes, the multi-view prototypes include support outline view, query foreground view, seen class object view and natural background view prototypes. these proposed prototypes provide more refined semantic support information. to reduce the impact of negative samples, we propose a novel inference paradigm (n-iteration inference) for producing pseudo labels of novel classes as augmented support samples. these samples are then applied in the proposed multi-view prototype method for one-shot semantic segmentation. experimental results show that we have achieved new state-of-the-art performance on the two standard datasets, pascal-5(i) and coco-20(i). furthermore, we apply the inference paradigm to other classical works in order to enhance the performance of one-shot semantic segmentation. our source code will be available on https://github.com/whl182/mvpnet.",AB_0340
