AB,NO
"the core difficulty of text-based person search is how to achieve fine-grained alignment of visual and linguistic modal data, so as to bridge the gap of modal heterogeneity. most existing works on this task focus on global and local features extraction and matching, ignoring the importance of relational information. this paper proposes a new text-based person search model, named cm-lrgnet, which extracts cross-modal local-relational-global features in an end-to-end manner, and performs fine-grained cross-modal alignment on the above three feature levels. concretely, we first split the convolutional feature maps to obtain local features of images, and adaptively extract textual local features. then a relation encoding module is proposed to implicitly learn the relational information implied in the images and texts. finally, a relation-aware graph attention network is designed to fuse the local and relational features to generate global representations for both images and text queries. extensive experimental results on benchmark dataset (cuhk-pedes) show that our approach can achieve state-of-the-art performance (64.18%, 82.97%, 89.85% in terms of top-1, top-5, and top-10 accuracies), by learning and aligning local-relational-global representations from different modalities. our code has been released in https://github.com/zhangweifeng1218/text-based-person-search.(c) 2023 elsevier b.v. all rights reserved.",AB_0377
"the person re-id has made great progress benefiting from the advance of deep neural networks and the attention mechanism in recent years. however, existing models and attentions for person re-id only focus on learning the robust features while neglecting the difference between features of pairs, which is the core of feature similarity matching tasks. to address this problem, we propose the diff attention idea and design diff attention module to guide the network to learn a more discriminative attention map based on the difference of feature vectors. taking the diff attention module, we develop diff attention framework to match various backbone re-id models and train them. to efficiently train diff attention framework, we also propose two training strategies: joint training and separate training. our framework and strategies have achieved excellent performance on market-1501, cuhk03, and msmt17 datasets. our code will be made publicly available at https://github.com/linxin98/reid-diffattention.",AB_0377
"deep neural networks have greatly facilitated the applications of semantic segmentation. however, most of the existing neural networks bring massive calculations with lots of model parameters for achieving a higher precision, which is unaffordable for resource-constrained edge devices. to achieve an appropriate trade-off between computing efficiency and segmentation accuracy, we proposed an effective lightweight attention-guided network (elanet) for real-time semantic segmentation based on an asymmetrical encoder-decoder framework in this work. in the encoding phase, we combined atrous convolution and depth-wise convolution to design two types of effective context guidance blocks to learn contextual semantic information. a refined feature fusion module with a dual attention-guided fusion (daf) unit was developed in the decoder to exploit different levels of features. without any pretraining, we estimated the performance of multi-attention elanet with extensive experiments on the cityscapes dataset with an input resolution of 512x1024, resulting in 75.4% miou and 83 fps inference speed with only 0.76 m parameters and 10.34 gflops on a single 3090 gpu. the code is publicly available at https://github.com/dgs666/elanet.",AB_0377
"head pose estimation is one of the essential tasks in computer vision, which predicts the euler angles of the head in an image. in recent years, cnn-based methods for head pose estimation have achieved excellent performance. their training relies on rgb images providing facial landmarks or depth images from rgbd cameras. however, labeling facial landmarks is complex for large angular head poses in rgb images, and rgbd cameras are unsuitable for outdoor scenes. we propose a simple and effective annotation method for the head pose in rgb images. the novelty method uses a 3d virtual human head to simulate the head pose in the rgb image. the euler angle can be calculated from the change in coordinates of the 3d virtual head. we then create a dataset using our annotation method: 2dheadpose dataset, which contains a rich set of attributes, dimensions, and angles. finally, we propose gaussian label smoothing to suppress annotation noises and reflect inter-class relationships. a baseline approach is established using gaussian label smoothing. experiments demonstrate that our annotation method, datasets, and gaussian label smoothing are very effective. our baseline approach surpasses most current state-of-the-art methods. the annotation tool, dataset, and source code are publicly available at https://github.com/youngnuaa/2dheadpose.(c) 2022 elsevier ltd. all rights reserved.",AB_0377
"light field (lf) cameras suffer from a fundamental trade-off between spatial and angular resolutions. additionally, due to the significant amount of data that needs to be recorded, the lytro illum, a modern lf camera, can only capture three frames per second. in this paper, we consider space-time super-resolution (sr) for lf videos, aiming at generating high-resolution and high-frame-rate lf videos from low-resolution and low-frame-rate observations. extending existing space-time video sr methods to this task directly will meet two key challenges: 1) how to re-organize sub-aperture images (sais) efficiently and effectively given highly redundant lf videos, and 2) how to aggregate complementary information between multiple sais and frames considering the coherence in lf videos. to address the above challenges, we propose a novel framework for space-time super-resolving lf videos for the first time. first, we propose a novel multi-scale dilated sai re-organization strategy for re-organizing sais into auxiliary view stacks with decreasing resolution as the chebyshev distance in the angular dimension increases. in particular, the auxiliary view stack with original resolution preserves essential visual details, while the down-scaled view stacks capture long-range contextual information. second, we propose the multi-scale aggregated feature extractor and the angular-assisted feature interpolation module to utilize and aggregate information from the spatial, angular, and temporal dimensions in lf videos. the former aggregates similar contents from different sais and frames for subsequent reconstruction in a disparity-free manner at the feature level, whereas the latter interpolates intermediate frames temporally by implicitly aggregating geometric information. compared to other potential approaches, experimental results demonstrate that the reconstructed lf videos generated by our framework achieve higher reconstruction quality and better preserve the lf parallax structure and temporal consistency. the implementation code is available at https://github.com/zeyuxiao1997/lfstvsr.",AB_0377
"light field (lf) cameras record both intensity and directions of light rays, and encode 3d scenes into 4d lf images. recently, many convolutional neural networks (cnns) have been proposed for various lf image processing tasks. however, it is challenging for cnns to effectively process lf images since the spatial and angular information are highly inter-twined with varying disparities. in this paper, we propose a generic mechanism to disentangle these coupled information for lf image processing. specifically, we first design a class of domain-specific convolutions to disentangle lfs from different dimensions, and then leverage these disentangled features by designing task-specific modules. our disentangling mechanism can well incorporate the lf structure prior and effectively handle 4d lf data. based on the proposed mechanism, we develop three networks (i.e., distgssr, distgasr and distgdisp) for spatial super-resolution, angular super-resolution and disparity estimation. experimental results show that our networks achieve state-of-the-art performance on all these three tasks, which demonstrates the effectiveness, efficiency, and generality of our disentangling mechanism. project page: https://yingqianwang.github.io/distglf/.",AB_0377
"in this paper, we present vision permutator, a conceptually simple and data efficient mlp-like architecture for visual recognition. by realizing the importance of the positional information carried by 2d feature representations, unlike recent mlp-like models that encode the spatial information along the flattened spatial dimensions, vision permutator separately encodes the feature representations along the height and width dimensions with linear projections. this allows vision permutator to capture long-range dependencies and meanwhile avoid the attention building process in transformers. the outputs are then aggregated in a mutually complementing manner to form expressive representations. we show that our vision permutators are formidable competitors to convolutional neural networks (cnns) and vision transformers. without the dependence on spatial convolutions or attention mechanisms, vision permutator achieves 81.5% top-1 accuracy on imagenet without extra large-scale training data (e.g., imagenet-22k) using only 25m learnable parameters, which is much better than most cnns and vision transformers under the same model size constraint. when scaling up to 88m, it attains 83.2% top-1 accuracy, greatly improving the performance of recent state-of-the-art mlp-like networks for visual recognition. we hope this work could encourage research on rethinking the way of encoding spatial information and facilitate the development of mlp-like models. pytorch/mindspore/jittor code is available at https://github.com/andrew-qibin/visionpermutator.",AB_0377
"self-supervised space-time correspondence learning utilizing unlabeled videos holds great potential in computer vision. most existing methods rely on contrastive learning with mining negative samples or adapting reconstruction from the image domain, which requires dense affinity across multiple frames or optical flow constraints. moreover, video correspondence prediction models need to uncover more inherent properties of the video, such as structural information. in this work, we propose higraph+, a sophisticated space-time correspondence framework based on learnable graph kernels. by treating videos as a spatial-temporal graph, the learning objective of higraph+ is issued in a self-supervised manner, predicting the unobserved hidden graph via graph kernel methods. first, we learn the structural consistency of sub-graphs in graph-level correspondence learning. furthermore, we introduce a spatio-temporal hidden graph loss through contrastive learning that facilitates learning temporal coherence across frames of sub-graphs and spatial diversity within the same frame. therefore, we can predict long-term correspondences and drive the hidden graph to acquire distinct local structural representations. then, we learn a refined representation across frames on the node-level via a dense graph kernel. the structural and temporal consistency of the graph forms the self-supervision of model training. higraph+ achieves excellent performance and demonstrates robustness in benchmark tests involving object, semantic part, keypoint, and instance labeling propagation tasks. our algorithm implementations have been made publicly available at https://github.com/zyqin19/higraph.",AB_0377
"multi-view subspace clustering is an important topic in cluster analysis. its aim is to utilize the complementary information conveyed by multiple views of objects to be clustered. recently, view-shared anchor learning based multi-view clustering methods have been developed to speed up the learning of common data representation. although widely applied to largescale scenarios, most of the existing approaches are still faced with two limitations. first, they do not pay sufficient consideration on the negative impact caused by certain noisy views with unclear clustering structures. second, many of them only focus on the multi-view consistency, yet are incapable of capturing the cross-view diversity. as a result, the learned complementary features may be inaccurate and adversely affect clustering performance. to solve these two challenging issues, we propose a fast self-guided multi-view subspace clustering (fsmsc) algorithm which skillfully integrates the view-shared anchor learning and global-guided-local self-guidance learning into a unified model. such an integration is inspired by the observation that the view with clean clustering structures will play a more crucial role in grouping the clusters when the features of all views are concatenated. specifically, we first learn a locally-consistent data representation shared by all views in the local learning module, then we learn a globally-discriminative data representation from multi-view concatenated features in the global learning module. afterwards, a feature selection matrix constrained by the & ell;(2,1)- norm is designed to construct a guidance from global learning to local learning. in this way, the multi-view consistent and diverse information can be simultaneously utilized and the negative impact caused by noisy views can be overcame to some extent. extensive experiments on different datasets demonstrate the effectiveness of our proposed fast self-guided learning model, and its promising performance compared to both, the state-of-the-art non-deep and deep multi-view clustering algorithms. the code of this paper is available at https://github.com/chenzhe207/fsmsc.",AB_0377
"existing graph clustering networks heavily rely on a predefined yet fixed graph, which can lead to failures when the initial graph fails to accurately capture the data topology structure of the embedding space. in order to address this issue, we propose a novel clustering network called embedding-induced graph refinement clustering network (egrc-net), which effectively utilizes the learned embedding to adaptively refine the initial graph and enhance the clustering performance. to begin, we leverage both semantic and topological information by employing a vanilla auto-encoder and a graph convolution network, respectively, to learn a latent feature representation. subsequently, we utilize the local geometric structure within the feature embedding space to construct an adjacency matrix for the graph. this adjacency matrix is dynamically fused with the initial one using our proposed fusion architecture. to train the network in an unsupervised manner, we minimize the jeffreys divergence between multiple derived distributions. additionally, we introduce an improved approximate personalized propagation of neural predictions to replace the standard graph convolution network, enabling egrc-net to scale effectively. through extensive experiments conducted on nine widely-used benchmark datasets, we demonstrate that our proposed methods consistently outperform several state-of-the-art approaches. notably, egrc-net achieves an improvement of more than 11.99% in adjusted rand index (ari) over the best baseline on the dblp dataset. furthermore, our scalable approach exhibits a 10.73% gain in ari while reducing memory usage by 33.73% and decreasing running time by 19.71%. the code for egrc-net will be made publicly available at https://github.com/zhihaopeng-cityu/egrc-net.",AB_0377
