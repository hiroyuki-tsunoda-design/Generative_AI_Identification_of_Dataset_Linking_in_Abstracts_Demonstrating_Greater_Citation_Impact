AB,NO
"to minimize the impact of age variation on face recognition, age-invariant face recognition (aifr) extracts identity-related discriminative features by minimizing the correlation between identity- and age-related features while face age synthesis (fas) eliminates age variation by converting the faces in different age groups to the same group. however, aifr lacks visual results for model interpretation and fas compromises downstream recognition due to artifacts. therefore, we propose a unified, multi-task framework to jointly handle these two tasks, termed mtlface, which can learn the age-invariant identity-related representation for face recognition while achieving pleasing face synthesis for model interpretation. specifically, we propose an attention-based feature decomposition to decompose the mixed face features into two uncorrelated components-identity- and age-related features-in a spatially constrained way. unlike the conventional one-hot encoding that achieves group-level fas, we propose a novel identity conditional module to achieve identity-level fas, which can improve the age smoothness of synthesized faces through a weight-sharing strategy. benefiting from the proposed multi-task framework, we then leverage those high-quality synthesized faces from fas to further boost aifr via a novel selective fine-tuning strategy. furthermore, to advance both aifr and fas, we collect and release a large cross-age face dataset with age and gender annotations, and a new benchmark specifically designed for tracing long-missing children. extensive experimental results on five benchmark cross-age datasets demonstrate that mtlface yields superior performance than state-of-the-art methods for both aifr and fas. we further validate mtlface on two popular general face recognition datasets, obtaining competitive performance on face recognition in the wild. the source code and datasets are available at http://hzzone.github.io/mtlface.",AB_0354
"point cloud segmentation is a fundamental task in 3d. despite recent progress on point cloud segmentation with the power of deep networks, current learning methods based on the clean label assumptions may fail with noisy labels. yet, class labels are often mislabeled at both instance-level and boundary-level in real-world datasets. in this work, we take the lead in solving the instance-level label noise by proposing a point noise-adaptive learning (pnal) framework. compared to noise-robust methods on image tasks, our framework is noise-rate blind, to cope with the spatially variant noise rate specific to point clouds. specifically, we propose a point-wise confidence selection to obtain reliable labels from the historical predictions of each point. a cluster-wise label correction is proposed with a voting strategy to generate the best possible label by considering the neighbor correlations. to handle boundary-level label noise, we also propose a variant pnal-boundary with a progressive boundary label cleaning strategy. extensive experiments demonstrate its effectiveness on both synthetic and real-world noisy datasets. even with 60% symmetric noise and high-level boundary noise, our framework significantly outperforms its baselines, and is comparable to the upper bound trained on completely clean data. moreover, we cleaned the popular real-world dataset scannetv2 for rigorous experiment. our code and data is available at https://github.com/ pleaseconnectwifi/pnal.",AB_0354
"we present a novel method for local image feature matching. instead of performing image feature detection, description, and matching sequentially, we propose to first establish pixel-wise dense matches at a coarse level and later refine the good matches at a fine level. in contrast to dense methods that use a cost volume to search correspondences, we use self and cross attention layers in transformer to obtain feature descriptors that are conditioned on both images. the global receptive field provided by transformer enables our method to produce dense matches in low-texture areas, where feature detectors usually struggle to produce repeatable interest points. the experiments on indoor and outdoor datasets show that loftr outperforms state-of-the-art methods by a large margin. we further adapt loftr to modern sfm systems and illustrate its application in multiple-view geometry. the proposed method demonstrates superior performance in image matching challenge 2021 and ranks first on two public benchmarks of visual localization among the published methods. the code is available at https://zju3dv.github.io/loftr.",AB_0354
"in this paper, we introduce a new framework for unsupervised deep homography estimation. our contributions are 3 folds. first, unlike previous methods that regress 4 offsets for a homography, we propose a homography flow representation, which can be estimated by a weighted sum of 8 pre-defined homography flow bases. second, considering a homography contains 8 degree-of-freedoms (dofs) that is much less than the rank of the network features, we propose a low rank representation (lrr) block that reduces the feature rank, so that features corresponding to the dominant motions are retained while others are rejected. last, we propose a feature identity loss (fil) to enforce the learned image feature warp-equivariant, meaning that the result should be identical if the order of warp operation and feature extraction is swapped. with this constraint, the unsupervised optimization can be more effective and the learned features are more stable. with global-to-local homography flow refinement, we also naturally generalize the proposed method to local mesh-grid homography estimation, which can go beyond the constraint of a single homography. extensive experiments are conducted to demonstrate the effectiveness of all the newly proposed components, and results show that our approach outperforms the state-of-the-art on the homography benchmark dataset both qualitatively and quantitatively. code is available at https://github.com/megvii-research/baseshomo.",AB_0354
"due to the wavelength-dependent light absorption and scattering, the raw underwater images are usually inevitably degraded. underwater image enhancement (uie) is of great importance for underwater observation and operation. data-driven methods, such as deep learning-based uie approaches, tend to be more applicable to real underwater scenarios. however, the training of deep models is limited by the extreme scarcity of underwater images with enhancement references, resulting in their poor performance in dynamic and diverse underwater scenes. as an alternative, enhancement reference achieved by volunteer voting alleviate the sample shortage to some extent. since such artificially acquired references are not veritable ground truth, they are far from complete and accurate to provide correct and rich supervision for the enhancement model training. beyond training with single reference, we propose the first comparative learning framework for uie problem, namely cluie-net, to learn from multiple candidates of enhancement reference. this new strategy also supports semi-supervised learning mode. besides, we propose a regional quality-superiority discriminative network (rqsd-net) as an embedded quality discriminator for the cluie-net. comprehensive experiments demonstrate the effectiveness of rqsd-net and the comparative learning strategy for uie problem. the code, models and new dataset rqsd-ui are available at: https://justwj.github.io/cluie-net.html/.",AB_0354
"accurate whole-body multi-person pose estimation and tracking is an important yet challenging topic in computer vision. to capture the subtle actions of humans for complex behavior analysis, whole-body pose estimation including the face, body, hand and foot is essential over conventional body-only pose estimation. in this article, we present alphapose, a system that can perform accurate whole-body pose estimation and tracking jointly while running in realtime. to this end, we propose several new techniques: symmetric integral keypoint regression (sikr) for fast and fine localization, parametric pose non-maximum-suppression (p-nms) for eliminating redundant human detections and pose aware identity embedding for jointly pose estimation and tracking. during training, we resort to part-guided proposal generator (pgpg) and multi-domain knowledge distillation to further improve the accuracy. our method is able to localize whole-body keypoints accurately and tracks humans simultaneously given inaccurate bounding boxes and redundant detections. we show a significant improvement over current state-of-the-art methods in both speed and accuracy on coco-wholebody, coco, posetrack, and our proposed halpe-fullbody pose estimation dataset. our model, source codes and dataset are made publicly available at https://github.com/mvig-sjtu/alphapose.",AB_0354
"scene text spotting is of great importance to the computer vision community due to its wide variety of applications. recent methods attempt to introduce linguistic knowledge for challenging recognition rather than pure visual classification. however, how to effectively model the linguistic rules in end-to-end deep networks remains a research challenge. in this paper, we argue that the limited capacity of language models comes from 1) implicit language modeling; 2) unidirectional feature representation; and 3) language model with noise input. correspondingly, we propose an autonomous, bidirectional and iterative abinet++ for scene text spotting. first, the autonomous suggests enforcing explicitly language modeling by decoupling the recognizer into vision model and language model and blocking gradient flow between both models. second, a novel bidirectional cloze network (bcn) as the language model is proposed based on bidirectional feature representation. third, we propose an execution manner of iterative correction for the language model which can effectively alleviate the impact of noise input. additionally, based on an ensemble of the iterative predictions, a self-training method is developed which can learn from unlabeled images effectively. finally, to polish abinet++ in long text recognition, we propose to aggregate horizontal features by embedding transformer units inside a u-net, and design a position and content attention module which integrates character order and content to attend to character features precisely. abinet++ achieves state-of-the-art performance on both scene text recognition and scene text spotting benchmarks, which consistently demonstrates the superiority of our method in various environments especially on low-quality images. besides, extensive experiments including in english and chinese also prove that, a text spotter that incorporates our language modeling method can significantly improve its performance both in accuracy and speed compared with commonly used attention-based recognizers. code is available at https://github.com/fangshancheng/abinet-pp.",AB_0354
"detection transformer (detr) and deformable detr have been proposed to eliminate the need for many hand-designed components in object detection while demonstrating good performance as previous complex hand-crafted detectors. however, their performance on video object detection (vod) has not been well explored. in this paper, we present transvod, the first end-to-end video object detection system based on simple yet effective spatial-temporal transformer architectures. the first goal of this paper is to streamline the pipeline of current vod, effectively removing the need for many hand-crafted components for feature aggregation, e.g., optical flow model, relation networks. besides, benefited from the object query design in detr, our method does not need postprocessing methods such as seq-nms. in particular, we present a temporal transformer to aggregate both the spatial object queries and the feature memories of each frame. our temporal transformer consists of two components: temporal query encoder (tqe) to fuse object queries, and temporal deformable transformer decoder (tdtd) to obtain current frame detection results. these designs boost the strong baseline deformable detr by a significant margin (3 %-4 % map) on the imagenet vid dataset. transvod yields comparable performances on the benchmark of imagenet vid. then, we present two improved versions of transvod including transvod++ and transvod lite. the former fuses object-level information into object query via dynamic convolution while the latter models the entire video clips as the output to speed up the inference time. we give detailed analysis of all three models in the experiment part. in particular, our proposed transvod++ sets a new state-of-the-art record in terms of accuracy on imagenet vid with 90.0 % map. our proposed transvod lite also achieves the best speed and accuracy trade-off with 83.7 % map while running at around 30 fps on a single v100 gpu device. code and models are available at https://github.com/sjtu-luhe/transvod.",AB_0354
"we introduce camrl, the first curriculum-based asymmetric multi-task learning (amtl) algorithm for dealing with multiple reinforcement learning (rl) tasks altogether. to mitigate the negative influence of customizing the one-off training order in curriculumbased amtl, camrl switches its training mode between parallel single-task rl and asymmetric multi-task rl (mtrl), according to an indicator regarding the training time, the overall performance, and the performance gap among tasks. to leverage the multi-sourced prior knowledge flexibly and to reduce negative transfer in amtl, we customize a composite loss with multiple differentiable ranking functions and optimize the loss through alternating optimization and the frank-wolfe algorithm. the uncertainty-based automatic adjustment of hyper-parameters is also applied to eliminate the need of laborious hyper-parameter analysis during optimization. by optimizing the composite loss, camrl predicts the next training task and continuously revisits the transfer matrix and network weights. we have conducted experiments on a wide range of benchmarks in multi-task rl, covering gym-minigrid, meta-world, atari video games, vision-based pybullet tasks, and rlbench, to show the improvements of camrl over the corresponding single-task rl algorithm and state-of-the-art mtrl algorithms. the code is available at: https://github.com/huanghanchi/camrl.",AB_0354
"existing deep clustering methods rely on either contrastive or non-contrastive representation learning for downstream clustering task. contrastive-based methods thanks to negative pairs learn uniform representations for clustering, in which negative pairs, however, may inevitably lead to the class collision issue and consequently compromise the clustering performance. non-contrastive-based methods, on the other hand, avoid class collision issue, but the resulting non-uniform representations may cause the collapse of clustering. to enjoy the strengths of both worlds, this paper presents a novel end-to-end deep clustering method with prototype scattering and positive sampling, termed propos. specifically, we first maximize the distance between prototypical representations, named prototype scattering loss, which improves the uniformity of representations. second, we align one augmented view of instance with the sampled neighbors of another view-assumed to be truly positive pair in the embedding space-to improve the within-cluster compactness, termed positive sampling alignment. the strengths of propos are avoidable class collision issue, uniform representations, well-separated clusters, and within-cluster compactness. by optimizing propos in an end-to-end expectation-maximization framework, extensive experimental results demonstrate that propos achieves competing performance on moderate-scale clustering benchmark datasets and establishes new state-of-the-art performance on large-scale datasets. source code is available at https://github.com/hzzone/propos.",AB_0354
