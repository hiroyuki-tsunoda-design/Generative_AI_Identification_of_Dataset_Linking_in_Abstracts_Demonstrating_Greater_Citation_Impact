AB,NO
"three-dimensional (3d) multiobject tracking (mot) is an essential perception task for autonomous vehicles (avs). studies have indicated that multimodal data fusion can provide more stable and efficient perception information to avs than a single sensor. therefore, this paper proposes a new spatiotemporal adaptive attention 3d (3dstaa) tracker, which attempts to improve the tracking performance of the end-to-end 3d mot by adaptively correlating spatiotemporal data. the novelty of this paper includes the following. (1) different from nonintelligent fusion methods, this paper uses an efficiently adaptive spatial-guided fusion (sgfus) module for multimodal feature fusion. as a result, the 3d structural information obtained from point cloud data can provide additional spatial information as complementary information to the 2d texture information extracted from the image data, collaboratively facilitating and refining the perception information representation in the margin area. (2) this paper develops a spatiotemporal object-unique attention (stoua) module that calculates the relational degree of each perceived object between two adjacent frames through attentional encoding. at the same time, an adaptive weighting strategy is used to further study the spatiotemporal correlation of unique objects, reducing the similarity among various objects and the differences across the same object. experiments tested using the kitti tracking benchmark show that the 3dstaa tracker is highly competitive in both inference time and tracking performance compared with state-of-the-art (sota) methods. our corresponding code will be released on the https://github.com/xf-zh.(c) 2023 published by elsevier b.v.",AB_0367
"text-based image captioning (textcap) aims to remedy the shortcomings of existing image captioning tasks that ignore text content when describing images. instead, it requires models to recognize and describe images from both visual and textual content to achieve a deeper level of comprehension of the images. however, existing methods tend to use numerous complex network architectures to improve performance, which still fails to adequately model the relationship between vision and text on the one side, while on the other side this leads to long running times, high memory consumption, and other unfavorable deployment problems. to solve the above issues, we have developed a lightweight captioning method with a collaborative mechanism, lcm-captioner, which balances high efficiency with high performance. first, we propose a feature-lightening transformation for the textcap task, named textlight, which is able to learn rich multimodal representations while mapping features to lower dimensions, thereby reducing memory costs. next, we present a collaborative attention module for visual and text information, vtcam, to facilitate the semantic alignment of multimodal information to uncover important visual objects and textual content. finally, the conducted extensive experiments on the textcaps dataset demonstrate the effectiveness of our method. code is available at https://github.com/denghy258/lcm-captioner. (c) 2023 elsevier ltd. all rights reserved.",AB_0367
"regional dropout strategies have demonstrated to be very effective in improving both the performance and the generalization capability of deep learning models. however, when such strategies are performed in a totally random manner, the background noise and label mismatch problems arise. to tackle such problems, existing approaches typically focus on regions with the highest distinctiveness. yet, there are two main drawbacks of existing approaches: (i) many existing region-based augmentation methods can only use rectangular regions, resulting in the loss of object contour information; (ii) deterministic se-lection of the most discriminative regions leads to poor diversification in data augmentation. in fact, a trade-off is needed between diversification and concentration, which can decrease the undesirable noise.in this paper, we propose a novel object-centric contour-aware cutmix data augmentation strategy with arbitrary-shape and size superpixel supports, which is hereafter referred to as occamix for short. it not only captures the most discriminative regions, but also effectively preserves the contour details of the objects. moreover, it enables the search of natural object parts of different sizes. extensive experiments on a large number of benchmark datasets show that occamix significantly outperforms state-of-the-art cutmix based data augmentation methods in classification tasks. the source codes and trained models are available at https://github.com/danielaplusplus/occamix .(c) 2023 the author(s). published by elsevier ltd. this is an open access article under the cc by-nc-nd license (  )",AB_0367
"local feature extraction is a key link in realizing computer vision tasks. although the related research has made great progress, the balance of network efficiency and accuracy has always existed. to address these issues, we propose a lightweight and efficient multi-task feature point detection network. first, an enhanced block (e-block) is built based on the asymmetric convolution structure to fully mine the feature information, and the parameters of the e-block are fused by the method of structural re-parameterization to improve the network operation efficiency. then, we design an enhanced module (e-model) through e -block to improve the feature point detection ability of the network. in particular, e-model utilizes a lightweight shuffle attention (sa) mechanism to reduce redundant feature extraction. in addition, we also design a fast version of the network by exploiting the characteristics of grouped convolution and multi-scale aggregation of pyramid convolution. experiments on the hpatches dataset and kitti dataset show that the proposed network has satisfactory feature extraction ability while reducing the volume. https://github .com /spiritashes /lefpd -net .git.(c) 2023 elsevier inc. all rights reserved.",AB_0367
"the contextual information is critical for various computer vision tasks, previous works commonly design plug-and-play modules and structural losses to effectively extract and aggregate the global context. these methods utilize fine-label to optimize the model but ignore that fine-trained features are also pre-cious training resources, which can introduce preferable distribution to hard pixels (i.e., misclassified pix-els). inspired by contrastive learning in unsupervised paradigm, we apply the contrastive loss in a supervised manner and re-design the loss function to cast off the stereotype of unsupervised learning (e.g., imbalance of positives and negatives, confusion of anchors computing). to this end, we propose positive -negative equal contrastive loss (pne loss), which increases the latent impact of positive embed-ding on the anchor and treats the positive as well as negative sample pairs equally. the pne loss can be directly plugged right into existing semantic segmentation frameworks and leads to excellent perfor-mance with neglectable extra computational costs. we utilize a number of classic segmentation methods (e.g., deeplabv3, hrnetv2, ocrnet, upernet) and backbone (e.g., resnet, hrnet, swin transformer) to conduct comprehensive experiments and achieve state-of-the-art performance on three benchmark data -sets (e.g., cityscapes, coco-stuff and ade20k). our code will be publicly available at https://github.com/ jingw193/pne_loss. (c) 2023 elsevier b.v. all rights reserved.",AB_0367
"recently, image-text matching has been intensively explored to bridge vision and language. previous methods explore an inter-modality relationship between an image-text pair from the single-view feature. however, it is difficult to discover all the abundant information based on a single inter-modality relation-ship. in this paper, a novel multi-view inter-modality representation with progressive fusion (mirpf) is developed to explore inter-modality relationships from multi-view features. the multi-view strategy provides more complementary and global semantic clues than single-view approaches. in particular, the multi-view inter-modality representation network is constructed to generate multiple inter -modality representations, which provide diverse views to discover the latent image-text relationships. furthermore, the progressive fusion module is performed to fuse inter-modality features stepwise, which fully uses the inherent complementary between different views. extensive experiments on flickr30k and mscoco verify the superiority of mirpf compared with several existing approaches. the code is available at: https://github.com/jasscia18/mirpf. (c) 2023 published by elsevier b.v.",AB_0367
"discriminative subspace learning is an important problem in machine learning, which aims to find the maximum separable decision subspace. traditional euclidean-based methods usually use fisher discriminant criterion for finding an optimal linear mapping from a high-dimensional data space to a lowerdimensional subspace, which hardly guarantee a quadratic rate of global convergence and suffers from the singularity problem. here, we propose the manifold optimization-based discriminant analysis (moda) which is constructed by using the latent subspace alignment and the geometry of objective function with orthogonality constraint. moda is solved by using riemannian version of trust-region algorithm. experimental results on various image datasets and electroencephalogram (eeg) datasets show that moda achieves the best separability and is significantly superior to the competing algorithms. especially for the time series of eeg signals, the accuracy of moda is 20-30% higher than existing algorithms. the code for moda is available at https://github.com/ncclabsustech/moda-algorithm . (c) 2023 elsevier ltd. all rights reserved.",AB_0367
"deep clustering has attracted increasing attention in recent years due to its capability of joint representation learning and clustering via deep neural networks. in its latest developments, the contrastive learning has emerged as an effective technique to substantially enhance the deep clustering performance. however, the existing contrastive learning based deep clustering algorithms mostly focus on some carefullydesigned augmentations (often with limited transformations to preserve the structure), referred to as weak augmentations, but cannot go beyond the weak augmentations to explore the more opportunities in stronger augmentations (with more aggressive transformations or even severe distortions). in this paper, we present an end-to-end deep clustering approach termed s trongly a ugmented c ontrastive c lustering (sacc), which extends the conventional two-augmentation-view paradigm to multiple views and jointly leverages strong and weak augmentations for strengthened deep clustering. particularly, we utilize a backbone network with triply-shared weights, where a strongly augmented view and two weakly augmented views are incorporated. based on the representations produced by the backbone, the weak-weak view pair and the strong-weak view pairs are simultaneously exploited for the instance-level contrastive learning (via an instance projector) and the cluster-level contrastive learning (via a cluster projector), which, together with the backbone, can be jointly optimized in a purely unsupervised manner. experimental results on five challenging image datasets have shown the superiority of our sacc approach over the state-of-the-art. the code is available at https://github.com/dengxiaozhi/sacc .(c) 2023 elsevier ltd. all rights reserved.",AB_0367
"many complex systems in the real world can be characterized as attributed networks. to mine the poten-tial information in these networks, deep embedded clustering, which obtains node representations and clusters simultaneously, has been given much attention in recent years. under the assumption of consis-tency for data in different views, the cluster structure of network topology and that of node attributes should be consistent for an attributed network. however, many existing methods ignore this property, even though they separately encode node representations from network topology and node attributes and cluster nodes on representation vectors learned from one of the views. therefore, in this study, we propose an end-to-end deep embedded clustering model for attributed networks. it utilizes graph autoen-coder and node attribute autoencoder to learn node representations and cluster assignments. in addition, a distribution consistency constraint is introduced to maintain the latent consistency of cluster distri-butions in two views. extensive experiments on several datasets demonstrate that the proposed model achieves significantly better or competitive performance compared with the state-of-the-art methods. the source code can be found at https://github.com/zhengymm/dcp.(c) 2023 elsevier ltd. all rights reserved.",AB_0367
"this paper proposes an efficient optimization algorithm based on the physical phenomenon of rime-ice, called the rime. the rime algorithm implements the exploration and exploitation behaviors in the optimiza-tion methods by simulating the soft-rime and hard-rime growth process of rime-ice and constructing a soft -rime search strategy and a hard-rime puncture mechanism. meanwhile, the greedy selection mechanism in the algorithm is improved, and the population is updated in the stage of selecting the optimal solution to enhance the exploitation capability of the rime. in the experimental, this paper conducts qualitative analysis experiments on the rime to clarify the characteristics of the algorithm in the process of finding the optimal solution. the performance of rime is then tested on a total of 42 functions in the classic ieee cec2017 and the latest ieee cec2022 test sets. the proposed algorithm is compared with 10 well-established algorithms and 10 latest improved algorithms to verify its performance advantage. in addition, this paper designs exper-iments for the parametric analysis of rime to discuss the potential of the algorithm in running different parameters and handling different problems. finally, this paper applies rime to five practical engineering problems to verify its effectiveness and superiority in real-world problems. the statistical and comparison results show that the rime is a strong and competitive algorithm. the source code of the rime1 algorithm and associated files are publicly accessible at https://aliasgharheidari.com/rime.html.(c) 2023 elsevier b.v. all rights reserved.",AB_0367
