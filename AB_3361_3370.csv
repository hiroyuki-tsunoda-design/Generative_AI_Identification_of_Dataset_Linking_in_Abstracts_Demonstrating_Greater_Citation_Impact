AB,NO
"vision-based practical applications, such as consumer photography and automated driving systems, greatly rely on enhancing the visibility of images captured in night-time environments. for this reason, various image enhancement algorithms (ehas) have been proposed. however, little attention has been given to the quality evaluation of enhanced night-time images. in this paper, we conduct the first dedicated exploration of the subjective and objective quality evaluation of enhanced night-time images. first, we build an enhanced night-time image quality (ehnq) database, which is the largest of its kind so far. it includes 1,500 enhanced images generated from 100 real night-time images using 15 different ehas. subsequently, we perform a subjective quality evaluation and obtain subjective quality scores on the ehnq database. thereafter, we present an objective blind quality index for enhanced night-time images (behn). enhanced night-time images usually suffer from inappropriate brightness and contrast, deformed structure, and unnatural colorfulness. in behn, we capture perceptual features that are highly relevant to these three types of corruptions, and we design an ensemble training strategy to map the extracted features into the quality score. finally, we conduct extensive experiments on ehnq and eaqa databases. the experimental and analysis results validate the performance of the proposed behn compared with the state-of-the-art approaches. our ehnq database is publicly available for download at https://sites.google.com/site/xiangtaooo/.",AB_0337
"referring video object segmentation (rvos) aims to segment the text-depicted object from video sequences. with excellent capabilities in long-range modelling and information interaction, transformers have been increasingly applied in existing rvos architectures. to better leverage multimodal data, most efforts focus on the interaction between visual and textual features. however, they ignore the syntactic structures of the text during the interaction, where all textual components are intertwined, resulting in ambiguous vision-language alignment. in this paper, we improve the multimodal interaction by decoupling the interweave. specifically, we train a lightweight subject perceptron, which extracts the subject part from the input text. then, the subject and text features are fed into two parallel branches to interact with visual features. this enables us to perform subject-aware and context aware interactions, respectively, thus encouraging more explicit and discriminative feature embedding and alignment. moreover, we find the decoupled architecture also facilitates incorporating the vision-language pre-trained alignment into rvos, further improving the segmentation performance. experimental results on all rvos benchmark datasets demonstrate the superiority of our proposed method over the state-of-the-arts. the code of our method is available at: https://github.com/gaomingqi/dmformer.",AB_0337
"modality alignment can maintain the consistency of semantics in multi-modal emotion recognition tasks, ensuring that features from different modalities accurately represent the emotion-related information in an encoding space. however, current alignment models either focus only on the local fusion of different modal representations or lack a mining process for unimodal specificity information. we design a semantic alignment network based on multi-spatial learning (sams) for multi-modal emotion recognition, which achieves local and global alignment between modalities using high-level emotion representations of different modalities as supervisory signals. sams builds a multi-spatial learning framework for each modality, and constructs a self-modal interaction module under this framework based on cross-modal semantic learning. sams provides two learning spaces for each modality, one to detect the affective information for a specific modality, and the other to learn semantic knowledge from other modalities. subsequently, the features of these two spaces are aligned in temporal and utterance levels by homologous encoding and different target constraints. based on the alignment characteristics of these two spaces, a self-modal interaction is built to investigate the fusion representation by exploring the global correlation between the alignment features in unimodal multi-spatial learning. in experiments, our proposed model yields consistent improvements on two standard multi-modal benchmarks, and outperforms state-of-the-art approaches. the code of our sams is available at: https://github.com/xiaomi1024/code_sams.",AB_0337
"drones have been widely used in a variety of applications, e.g., aerial photography and military security, because of their high maneuverability and broad views compared with fixed cameras. multi-drone tracking systems can provide rich information about targets by collecting complementary video clips from different views, especially when targets are occluded or disappear in some views. however, it is challenging to handle cross-drone information interaction and multi-drone information fusion in multi-drone visual tracking. recently, transformer has shown significant advantages in automatically modeling the correlation between templates and search regions for visual tracking. to leverage its potential in multi-drone tracking, we propose a novel cross-drone transformer network (transmdot) for visual object tracking tasks. the self-attention mechanism is used to automatically capture the correlation between multiple templates and the corresponding search region to achieve multi-drone feature fusion. during the tracking process, a cross-drone mapping mechanism is proposed by using the surrounding information of the drone with promising tracking status as reference, assisting drones that lost targets to re-calibrate, which implements real-time cross-drone information interaction. as the existing multi-drone evaluation metrics only consider spatial information while ignore temporal information, we further present a system perception index (spfi) that combines both temporal and spatial information to evaluate the tracking status of multiple drones. experiments on the mdot dataset prove that transmdot greatly surpasses the state-of-the-art methods in both single-drone performance and multi-drone system fusion performance. our code will be available on https://github.com/cgjacklin/transmdot.",AB_0337
"graph representation is a critical technology in the field of knowledge engineering and knowledgebased applications since most knowledge bases are represented in the graph structure. nowadays, contrastive learning has become a prominent way for graph representation by contrasting positive- positive and positive-negative node pairs between two augmentation graphs. it has achieved new state-of-the-art in the field of self-supervised graph representation. however, existing contrastive graph representation methods mainly focus on modifying (normally removing some edges/nodes) the original graph structure to generate the augmentation graph for the contrastive. it inevitably changes the original graph structures, meaning the generated augmentation graph is no longer equivalent to the original graph. this harms the performance of the representation in many structure-sensitive graphs such as protein graphs, chemical graphs, molecular graphs, etc. moreover, there is only one positive-positive node pair but relatively massive positive-negative node pairs in the self-supervised graph contrastive learning. this can lead to the same class, or very similar samples are considered negative samples. to this end, in this work, we propose a virtual masking augmentation (vma) to generate an augmentation graph without changing any structures from the original graph. meanwhile, a node augmentation method is proposed to augment the positive node pairs by discovering the most similar nodes in the same graph. then, two different augmentation graphs are generated and put into a contrastive learning model to learn the graph representation. extensive experiments on massive datasets demonstrate that our method achieves new state-of-the-art results on self-supervised graph representation. the source code of the proposed method is available at https://github.com/ duanhaorancc/cgra.& copy; 2023 the author(s). published by elsevier ltd. this is an open access article under the cc by-nc-nd license ().",AB_0337
"as a representative self-supervised method, contrastive learning has achieved great successes in unsupervised training of representations. it trains an encoder by distinguishing positive samples from negative ones given query anchors. these positive and negative samples play critical roles in defining the objective to learn the discriminative encoder, avoiding it from learning trivial features. while existing methods heuristically choose these samples, we present a principled method where both positive and negative samples are directly learnable end-to-end with the encoder. we show that the positive and negative samples can be cooperatively and adversarially learned by minimizing and maximizing the contrastive loss, respectively. this yields cooperative positives and adversarial negatives with respect to the encoder, which are updated to continuously track the learned representation of the query anchors over mini-batches. the proposed method achieves 71.3% and 75.3% in top-1 accuracy respectively over 200 and 800 epochs of pre-training resnet-50 backbone on imagenet1k without tricks such as multi-crop or stronger augmentations. with multi-crop, it can be further boosted into 75.7%. the source code and pre-trained model are released in https://github.com/maple-research-lab/caco.",AB_0337
"vision transformer (vit) has shown great potential for various visual tasks due to its ability to model long-range dependency. however, vit requires a large amount of computing resource to compute the global self-attention. in this work, we propose a ladder self-attention block with multiple branches and a progressive shift mechanism to develop a light-weight transformer backbone that requires less computing resources (e.g., a relatively small number of parameters and flops), termed progressive shift ladder transformer (pslt). first, the ladder self-attention block reduces the computational cost by modelling local self-attention in each branch. in the meanwhile, the progressive shift mechanism is proposed to enlarge the receptive field in the ladder self-attention block by modelling diverse local self-attention for each branch and interacting among these branches. second, the input feature of the ladder self-attention block is split equally along the channel dimension for each branch, which considerably reduces the computational cost in the ladder self-attention block (with nearly 1/3 the amount of parameters and flops), and the outputs of these branches are then collaborated by a pixel-adaptive fusion. therefore, the ladder self-attention block with a relatively small number of parameters and flops is capable of modelling long-range interactions. based on the ladder self-attention block, pslt performs well on several vision tasks, including image classification, objection detection and person re-identification. on the imagenet-1 k dataset, pslt achieves a top-1 accuracy of 79.9% with 9.2 m parameters and 1.9 g flops, which is comparable to several existing models with more than 20 m parameters and 4 g flops. code is available at https://isee-ai.cn/wugaojie/pslt.html.",AB_0337
"in this paper, we present a new approach for model acceleration by exploiting spatial sparsity in visual data. we observe that the final prediction in vision transformers is only based on a subset of the most informative regions, which is sufficient for accurate image recognition. based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input to accelerate vision transformers. specifically, we devise a lightweight prediction module to estimate the importance of each token given the current features. the module is added to different layers to prune redundant tokens hierarchically. while the framework is inspired by our observation of the sparse attention in vision transformers, we find that the idea of adaptive and asymmetric computation can be a general solution for accelerating various architectures. we extend our method to hierarchical models including cnns and hierarchical vision transformers as well as more complex dense prediction tasks. to handle structured feature maps, we formulate a generic dynamic spatial sparsification framework with progressive sparsification and asymmetric computation for different spatial locations. by applying lightweight fast paths to less informative features and expressive slow paths to important locations, we can maintain the complete structure of feature maps while significantly reducing the overall computations. extensive experiments on diverse modern architectures and different visual tasks demonstrate the effectiveness of our proposed framework. by hierarchically pruning 66% of the input tokens, our method greatly reduces 31% similar to 35% flops and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. by introducing asymmetric computation, a similar acceleration can be achieved on modern cnns and swin transformers. moreover, our method achieves promising results on more complex tasks including semantic segmentation and object detection. our results clearly demonstrate that dynamic spatial sparsification offers a new and more effective dimension for model acceleration. code is available at https://github.com/raoyongming/dynamicvit.",AB_0337
"local features detection and description are widely used in many vision applications with high industrial and commercial demands. with large-scale applications, these tasks raise high expectations for both the accuracy and speed of local features. most existing studies on local features learning focus on the local descriptions of individual keypoints, which neglect their relationships established from global spatial awareness. in this paper, we present awdesc with a consistent attention mechanism (coam) that opens up the possibility for local descriptors to embrace image-level spatial awareness in both the training and matching stages. for local features detection, we adopt local features detection with feature pyramid to obtain more stable and accurate keypoints localization. for local features description, we provide two versions of awdesc to cope with different accuracy and speed requirements. on the one hand, we introduce context augmentation to address the inherent locality of convolutional neural networks by injecting non-local context information, so that local descriptors can look wider to describe better. specifically, well-designed adaptive global context augmented module (agca) and diverse surrounding context augmented module (dsca) are proposed to construct robust local descriptors with context information from global to surrounding. on the other hand, we design an extremely lightweight backbone network coupled with the proposed special knowledge distillation strategy to achieve the best trade-off in accuracy and speed. what is more, we perform thorough experiments on image matching, homography estimation, visual localization, and 3d reconstruction tasks, and the results demonstrate that our method surpasses the current state-of-the-art local descriptors. code is available at: https://github.com/vignywang/awdesc.",AB_0337
"recent advances in self-attention and pure multi-layer perceptrons (mlp) models for vision have shown great potential in achieving promising performance with fewer inductive biases. these models are generally based on learning interaction among spatial locations from raw data. the complexity of self-attention and mlp grows quadratically as the image size increases, which makes these models hard to scale up when high-resolution features are required. in this paper, we present the global filter network (gfnet), a conceptually simple yet computationally efficient architecture, that learns long-term spatial dependencies in the frequency domain with log-linear complexity. our architecture replaces the self-attention layer in vision transformers with three key operations: a 2d discrete fourier transform, an element-wise multiplication between frequency-domain features and learnable global filters, and a 2d inverse fourier transform. based on this basic design, we develop a series of isotropic models with a transformer-style simple architecture and cnn-style hierarchical models with better performance. isotropic gfnet models exhibit favorable accuracy/complexity trade-offs compared to recent vision transformers and pure mlp models. hierarchical gfnet models can inherit successful designs in cnns and be easily scaled up with larger model sizes and more training data, showing strong performance on both image classification (e.g., 85.0% top-1 accuracy on imagenet-1 k without any extra data or supervision, and 87.4% accuracy with imagenet-21 k pre-training) and dense prediction tasks (e.g., 54.3 miou on ade20 k val). our results demonstrate that gfnet can be a very competitive alternative to transformer-based models and cnns in terms of efficiency, generalization ability and robustness. code is available at https://github.com/raoyongming/gfnet.",AB_0337
