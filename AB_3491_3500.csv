AB,NO
"as a technically challenging topic, visual storytelling aims at generating an imaginary and coherent story with narrative multi-sentences from a group of relevant images. existing methods often generate direct and rigid descriptions of apparent image-based contents, because they are not capable of exploring implicit information beyond images. hence, these schemes could not capture consistent dependencies from holistic representation, impairing the generation of reasonable and fluent stories. to address these problems, a novel knowledge-enriched attention network with group-wise semantic model is proposed. three main novel components are designed and supported by substantial experiments to reveal practical advantages. first, a knowledge-enriched attention network is designed to extract implicit concepts from external knowledge system, and these concepts are followed by a cascade cross-modal attention mechanism to characterize imaginative and concrete representations. second, a group-wise semantic module with second-order pooling is developed to explore the globally consistent guidance. third, a unified one-stage story generation model with encoder-decoder structure is proposed to simultaneously train and infer the knowledge-enriched attention network, group-wise semantic module and multi-modal story generation decoder in an end-to-end fashion. substantial experiments on the visual storytelling datasets with both objective and subjective evaluation metrics demonstrate the superior performance of the proposed scheme as compared with other state-of-the-art methods. the source code of this work can be found in https://mic.tongji.edu.cn.",AB_0350
"the newly proposed localized simple multiple kernel k-means (simplemkkm) provides an elegant clustering framework which sufficiently considers the potential variation among samples. although achieving superior clustering performance in some applications, we observe that it is required to pre-specify an extra hyperparameter, which determines the size of the localization. this greatly limits its availability in practical applications since there is a little guideline to set a suitable hyperparameter in clustering tasks. to overcome this issue, we firstly parameterize a neighborhood mask matrix as a quadratic combination of a set of pre-computed base neighborhood mask matrices, which corresponds to a group of hyperparameters. we then propose to jointly learn the optimal coefficient of these neighborhood mask matrices together with the clustering tasks. by this way, we obtain the proposed hyperparameter-free localized simplemkkm, which corresponds to a more intractable minimization-minimization-maximization optimization problem. we rewrite the resultant optimization as a minimization of an optimal value function, prove its differentiability, and develop a gradient based algorithm to solve it. furthermore, we theoretically prove that the obtained optimum is the global one. comprehensive experimental study on several benchmark datasets verifies its effectiveness, comparing with several state-of-the-art counterparts in the recent literature. the source code for hyperparameter-free localized simplemkkm is available at https://github.com/xinwangliu/simplemkkmcodes/.",AB_0350
"semi-supervised semantic segmentation aims to learn a semantic segmentation model via limited labeled images and adequate unlabeled images. the key to this task is generating reliable pseudo labels for unlabeled images. existing methods mainly focus on producing reliable pseudo labels based on the confidence scores of unlabeled images while largely ignoring the use of labeled images with accurate annotations. in this paper, we propose a cross-image semantic consistency guided rectifying (cisc-r) approach for semi-supervised semantic segmentation, which explicitly leverages the labeled images to rectify the generated pseudo labels. our cisc-r is inspired by the fact that images belonging to the same class have a high pixel-level correspondence. specifically, given an unlabeled image and its initial pseudo labels, we first query a guiding labeled image that shares the same semantic information with the unlabeled image. then, we estimate the pixel-level similarity between the unlabeled image and the queried labeled image to form a cisc map, which guides us to achieve a reliable pixel-level rectification for the pseudo labels. extensive experiments on the pascalvoc 2012, cityscapes, and coco datasets demonstrate that the proposed cisc-r can significantly improve the quality of the pseudo labels and outperform the state-of-the-art methods. code is available at https://github.com/luffy03/cisc-r.",AB_0350
"inserting an svd meta-layer into neural networks is prone to make the covariance ill-conditioned, which could harm the model in the training stability and generalization abilities. in this article, we systematically study how to improve the covariance conditioning by enforcing orthogonality to the pre-svd layer. existing orthogonal treatments on the weights are first investigated. however, these techniques can improve the conditioning but would hurt the performance. to avoid such a side effect, we propose the nearest orthogonal gradient (nog) and optimal learning rate (olr). the effectiveness of our methods is validated in two applications: decorrelated batch normalization (bn) and global covariance pooling (gcp). extensive experiments on visual recognition demonstrate that our methods can simultaneously improve covariance conditioning and generalization. the combinations with orthogonal weight can further boost the performance. moreover, we show that our orthogonality techniques can benefit generative models for better latent disentanglement through a series of experiments on various benchmarks. code is available at: https://github. com/kingjamessong/orthoimprovecond.",AB_0350
"tiny machine learning (tinyml), executing ai workloads on resource and power strictly restricted systems, is an important and challenging topic. this brief firstly presents an extremely tiny backbone to construct high efficiency cnn models for various visual tasks. then, a specially designed neural co-processor (ncp) is interconnected with mcu to build an ultra-low power tinyml system, which stores all features and weights on chip and completely removes both of latency and power consumption in off-chip memory access. moreover, an application specific instruction-set is further presented for realizing agile development and rapid deployment. extensive experiments demonstrate that the proposed tinyml system based on our tiny model, ncp and instruction set yields considerable accuracy and achieves a record ultra-low power of 160mw while implementing object detection and recognition at 30fps. the demo video is available on https://www.youtube.com/watch?v=mizpxtj-9ey.",AB_0350
"domain adaptive semantic segmentation attempts to make satisfactory dense predictions on an unlabeled target domain by utilizing the supervised model trained on a labeled source domain. one popular solution is self-training, which retrains the model with pseudo labels on target instances. plenty of approaches tend to alleviate noisy pseudo labels, however, they ignore the intrinsic connection of the training data, i.e., intra-class compactness and inter-class dispersion between pixel representations across and within domains. in consequence, they struggle to handle cross-domain semantic variations and fail to build a well-structured embedding space, leading to less discrimination and poor generalization. in this work, we propose semantic-guided pixel contrast (sepico), a novel one-stage adaptation framework that highlights the semantic concepts of individual pixels to promote learning of class-discriminative and class-balanced pixel representations across domains, eventually boosting the performance of self-training methods. specifically, to explore proper semantic concepts, we first investigate a centroid-aware pixel contrast that employs the category centroids of the entire source domain or a single source image to guide the learning of discriminative features. considering the possible lack of category diversity in semantic concepts, we then blaze a trail of distributional perspective to involve a sufficient quantity of instances, namely distribution-aware pixel contrast, inwhich we approximate the true distribution of each semantic category from the statistics of labeled source data. moreover, such an optimization objective can derive a closed-form upper bound by implicitly involving an infinite number of (dis)similar pairs, making it computationally efficient. extensive experiments show that sepico not only helps stabilize training but also yields discriminative representations, making significant progress on both synthetic-to-real and daytime-to-nighttime adaptation scenarios. the code and models are available at https://github.com/bitda/sepico.",AB_0350
"we introduce a new image segmentation task, called entity segmentation (es), which aims to segment all visual entities ( objects and stuffs) in an image without predicting their semantic labels. by removing the need of class label prediction, the models trained for such task can focus more on improving segmentation quality. it has many practical applications such as image manipulation and editing where the quality of segmentation masks is crucial but class labels are less important. we conduct the first-ever study to investigate the feasibility of convolutional center-based representation to segment things and stuffs in a unified manner, and show that such representation fits exceptionally well in the context of es. more specifically, we propose a condinst-like fully-convolutional architecture with two novel modules specifically designed to exploit the classagnostic and non-overlapping requirements of es. experiments show that the models designed and trained for es significantly outperforms popular class-specific panoptic segmentation models in terms of segmentation quality. moreover, an es model can be easily trained on a combination of multiple datasets without the need to resolve label conflicts in dataset merging, and the model trained for es on one or more datasets can generalize very well to other test datasets of unseen domains. the code has been released at https://github.com/dvlab-research/entity.",AB_0350
"salient object detection (sod) of images refers to simulating the attention mechanism of human vision to capture the most attractive objects in an image. current sod mainly relies on rgb images captured by optical cameras. however, existing optical cameras are not comparable to the human visual system, especially in poorly illumination scenes. our human visual system is able to resolve scenes well in low light conditions, while optical cameras can barely image without enough illumination. to make machine vision closer to the imaging of the human eye, we propose to use thermal infrared (t) images to compensate rgb images and build a variable illumination rgbt dataset named vi-rgbt1500 for sod. this dataset is collected under three different illumination conditions including sufficient illumination, uneven illumination and insufficient illumination to fully demonstrate the superiority of the rgbt image combination. furthermore, we propose a multiple graph affinity interactive (mgai) network to validate the proposed dataset. our network structure is simple using only the mgai to fuse the features of different modalities. meanwhile, the mgai model highlights valuable information during the interaction, which facilitates feature representation under variable illumination. the proposed vi-rgbt1500 dataset and three publicly available rgbt sod datasets are used for the comparison experiments, and the results with the state-of-the-art methods prove that our vi-rgbt1500 dataset is valuable and the performance of the mgai network is competitive. the vi-rgbt1500 dataset and the mgai network are available at: https://github.com/huanglm-me/vi-rgbt1500.",AB_0350
"this article presents a generic probabilistic framework for estimating the statistical dependency and finding the anatomical correspondences among an arbitrary number of medical images. the method builds on a novel formulation of the n-dimensional joint intensity distribution by representing the common anatomy as latent variables and estimating the appearance model with nonparametric estimators. through connection to maximum likelihood and the expectation-maximization algorithm, an information-theoretic metric called x-metric and a co-registration algorithm named x-coreg are induced, allowing groupwise registration of the n observed images with computational complexity of o(n). moreover, the method naturally extends for a weakly-supervised scenario where anatomical labels of certain images are provided. this leads to a combined-computing framework implemented with deep learning, which performs registration and segmentation simultaneously and collaboratively in an end-to-end fashion. extensive experiments were conducted to demonstrate the versatility and applicability of our model, including multimodal groupwise registration, motion correction for dynamic contrast enhanced magnetic resonance images, and deep combined computing for multimodal medical images. results show the superiority of our method in various applications in terms of both accuracy and efficiency, highlighting the advantage of the proposed representation of the imaging process. code is available from https://zmiclab.github.io/projects.html.",AB_0350
"video snapshot compressive imaging (sci) captures multiple sequential video frames by a single measurement using the idea of computational imaging. the underlying principle is to modulate high-speed frames through different masks and these modulated frames are summed to a single measurement captured by a low-speed 2d sensor (dubbed optical encoder); following this, algorithms are employed to reconstruct the desired high-speed frames (dubbed software decoder) if needed. in this article, we consider the reconstruction algorithm in video sci, i.e., recovering a series of video frames from a compressed measurement. specifically, we propose a spatial-temporal transformer (stformer) to exploit the correlation in both spatial and temporal domains. stformer network is composed of a token generation block, a video reconstruction block, and these two blocks are connected by a series of stformer blocks. each stformer block consists of a spatial self-attention branch, a temporal self-attention branch and the outputs of these two branches are integrated by a fusion network. extensive results on both simulated and real data demonstrate the state-of-the-art performance of stformer. the code and models are publicly available at https://github.com/ucaswangls/stformer.",AB_0350
