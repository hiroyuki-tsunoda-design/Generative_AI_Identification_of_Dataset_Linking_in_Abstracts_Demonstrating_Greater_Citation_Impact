AB,NO
"motivated by the intuition that the critical step of localizing a 2d image in the corresponding 3d point cloud is establishing 2d-3d correspondence between them, we propose the first feature-based dense correspondence framework for addressing the challenging problem of 2d image-to-3d point cloud registration, dubbed corri2p. corri2p is mainly composed of three modules, i.e., feature embedding, symmetric overlapping region detection, and pose estimation through the established correspondence. specifically, given a pair of a 2d image and a 3d point cloud, we first transform them into high-dimensional feature spaces and feed the resulting features into a symmetric overlapping region detector to determine the region where the image and point cloud overlap. then we use the features of the overlapping regions to establish dense 2d-3d correspondence, on which epnp within ransac is performed to estimate the camera pose, i.e., translation and rotation matrices. experimental results on kitti and nuscenes datasets show that our corri2p outperforms state-of-the-art image-to-point cloud registration methods significantly. the code will be publicly available at https://github.com/rsy6318/corri2p.",AB_0370
"the explosive growth of image data facilitates the fast development of image processing and computer vision methods for emerging visual applications, meanwhile introducing novel distortions to processed images. this poses a grand challenge to existing blind image quality assessment (biqa) models, which are weak at adapting to subpopulation shift. recent work suggests training biqa methods on the combination of all available human-rated iqa datasets. however, this type of approach is not scalable to a large number of datasets and is cumbersome to incorporate a newly created dataset as well. in this paper, we formulate continual learning for biqa, where a model learns continually from a stream of iqa datasets, building on what was learned from previously seen data. we first identify five desiderata in the continual setting with three criteria to quantify the prediction accuracy, plasticity, and stability, respectively. we then propose a simple yet effective continual learning method for biqa. specifically, based on a shared backbone network, we add a prediction head for a new dataset and enforce a regularizer to allow all prediction heads to evolve with new data while being resistant to catastrophic forgetting of old data. we compute the overall quality score by a weighted summation of predictions from all heads. extensive experiments demonstrate the promise of the proposed continual learning method in comparison to standard training techniques for biqa, with and without experience replay. we made the code publicly available at https://github.com/zwx8981/biqa_cl.",AB_0370
"existing unsupervised outlier detection (od) solutions face a grave challenge with surging visual data like images. although deep neural networks (dnns) prove successful for visual data, deep od remains difficult due to od's unsupervised nature. this paper proposes a novel framework named e-3 outlier that can perform effective and end-to-end deep outlier removal. its core idea is to introduce self-supervision into deep od. specifically, our major solution is to adopt a discriminative learning paradigm that creates multiple pseudo classes from given unlabeled data by various data operations, which enables us to apply prevalent discriminative dnns (e.g., resnet) to the unsupervised od problem. then, with theoretical and empirical demonstration, we argue that inlier priority, a property that encourages dnn to prioritize inliers during self-supervised learning, makes it possible to perform end-to-end od. meanwhile, unlike frequently-used outlierness measures (e.g., density, proximity) in previous od methods, we explore network uncertainty and validate it as a highly effective outlierness measure, while two practical score refinement strategies are also designed to improve od performance. finally, in addition to the discriminative learning paradigm above, we also explore the solutions that exploit other learning paradigms (i.e., generative learning and contrastive learning) to introduce self-supervision for e(3)outlier. such extendibility not only brings further performance gain on relatively difficult datasets, but also enables e(3)outlier to be applied to other od applications like video abnormal event detection. extensive experiments demonstrate that e(3)outlier can considerably outperform stateof-the-art counterparts by 10% - 30% auroc. demo codes are available at https://github.com/demonzyj56/e3outlier.",AB_0370
"wasserstein generative adversarial network (wgan) has attracted great attention due to its solid mathematical background, i.e., to minimize the wasserstein distance between the generated distribution and the distribution of interest. inwgan, the wasserstein distance is quantitatively evaluated by the discriminator, also known as the critic. the vanillawgan trained the critic with the simple lipschitz condition, which was later shown less effective for modeling complex distributions, like the distribution of natural images. we try to improve the wgan training by introducing pairwise constraint on the critic, oriented to image restoration tasks. in principle, pairwise constraint is to suggest the critic assign a higher rating to the original (real) image than to the restored (generated) image, as long as such a pair of images are available. we show that such pairwise constraint may be implemented by rectifying the gradients in wgan training, which leads to the proposed rectifiedwasserstein generative adversarial network (rewagan). in addition, we build interesting connections between rewagan and the perception-distortion tradeoff. we verify rewagan on two representative image restoration tasks: single image super-resolution (4x and 8x) and compression artifact reduction, where our rewagan not only beats the vanillawgan consistently, but also outperforms the state-of-the-art perceptual quality-oriented methods significantly. our code and models are publicly available at https://github.com/mahaichuan/rewagan.",AB_0370
"although current salient object detection (sod) works have achieved significant progress, they are limited when it comes to the integrity of the predicted salient regions. we define the concept of integrity at both a micro and macro level. specifically, at the micro level, the model should highlight all parts that belong to a certain salient object. meanwhile, at the macro level, the model needs to discover all salient objects in a given image. to facilitate integrity learning for sod, we design a novel integrity cognition network (icon), which explores three important components for learning strong integrity features. 1) unlike existing models, which focus more on feature discriminability, we introduce a diverse feature aggregation (dfa) component to aggregate features with various receptive fields (i.e., kernel shape and context) and increase feature diversity. such diversity is the foundation for mining the integral salient objects. 2) based on the dfa features, we introduce an integrity channel enhancement (ice) component with the goal of enhancing feature channels that highlight the integral salient objects, while suppressing the other distracting ones. 3) after extracting the enhanced features, the par t-whole verification (pwv) method is employed to determine whether the part and whole object features have strong agreement. such par t-whole agreements can further improve the micro-level integrity for each salient object. to demonstrate the effectiveness of our icon, comprehensive experiments are conducted on seven challenging benchmarks. our icon outperforms the baseline methods in terms of a wide range of metrics. notably, our icon achieves similar to 10% relative improvement over the previous best model in terms of average false negative ratio (fnr), on six datasets. codes and results are available at: https:// github.com/mczhuge/icon.",AB_0370
"single-image dehazing is a challenging task in several machine-vision applications. methods based on physical models and prior knowledge fail under certain conditions, resulting in defects such as color distortion. transformer-based methods have a strong representation ability owing to their self-attention mechanism that can effectively obtain global information. however, this approach is computationally expensive, and its weak inductive bias capability increases the risk of overfitting on small-sample datasets. to address these problems, in this study, we propose a novel dehazeformer guided by physical priors, named swintd-net, which is trained according to supervised and self -supervised learning, and combines the advantages of physical priors and transformers. the proposed dehazeformer learns features guided by physical priors, which improves the generalization ability of the network and enables it to achieve good restoration effects on both synthetic and real-world hazy images. in addition, we propose a more appropriate prior input to better use physical priors, and we design a multi-scale dark-light enhancement algorithm for image restoration post-processing, which can improve the visual perception quality for human observers while performing some local enhancements. extensive experiments illustrate that the proposed method outperforms state-of-the-art methods. the code and pre-trained models are available to academics so that they can reproduce our results and test them (https://github.com/hocking-cloud/swintd_net).(c) 2023 elsevier b.v. all rights reserved.",AB_0370
"one-to-multiple medical image segmentation aims to directly test a segmentation model trained with the medical images of a one-domain site on those of a multiple-domain site, suffering from segmentation performance degradation on multiple domains. this process avoids additional annota-tions and helps improve the application value of the model. however, no successful one-to-multiple unsupervised domain adaptation (o2m-uda) work has been reported in one-to-multiple medical image segmentation due to its inherent challenges: distribution differences among multiple target domains (among-target differences) caused by different scanning equipment and distribution differ-ences between one source domain and multiple target domains (source-target differences). in this paper, we propose an o2m-uda framework called dynamic domain adaptation (dyda), for one -to-multiple medical image segmentation, which has two innovations: (1) dynamic credible sample strategy (dcss) dynamically extracts credible samples from the target site and iteratively updates their number, thus iteratively expanding the generalization boundary of the model and minimizing the among-target differences; (2) hybrid uncertainty learning (hul) reduces the voxel-level and domain -level uncertainty simultaneously, thus minimizing the source-target differences from the detail and entire perspective concurrently. experiments on two one-to-multiple medical image segmentation tasks have been conducted to demonstrate the performance of the proposed dyda. the proposed dyda achieved competitive segmentation results and high adaptation with an average of 83.8% and 48.1% dice for the two tasks, respectively, which has improved by 21.7% and 9.2% compared with no adaptation, respectively. the code developed in this study code can be downloaded at https://github.com/zoeyjiang/dyda. (c) 2023 elsevier b.v. all rights reserved.",AB_0370
"sentence embedding, which aims to learn an effective representation of the sentence, is beneficial for downstream tasks. by utilizing contrastive learning, most recent sentence embedding methods have achieved promising results. however, these methods adopt simple data augmentation strategies to obtain variants of the sentence, limiting the representation ability of sentence embedding. in addition, these methods simply adopt the original framework of contrastive learning developed for image representation, which is not suitable for learning sentence embedding. to address these issues, we propose a method dubbed unsupervised contrastive learning of sentence embedding with prompt (clsep), aiming to provide effective sentence embedding by utilizing the prompt mechanism. meanwhile, we propose a novel data augmentation strategy for text data named partial word vector augmentation (pwva), which augments the data in the word embedding space, retaining more semantic information. finally, we introduce supervised contrastive learning of sentence embedding (suclse) and verify the effectiveness of the pwva on the natural language inference (nli) task. extensive experiments are conducted on the sts dataset, demonstrating that the proposed clsep and supcse are superior to the previous best methods, by utilizing the proposed pwva strategy. the code is available at https://github.com/qianandfei/clsep-contrastive-learning-of-sentence-embedding-with-prompt. (c) 2023 elsevier b.v. all rights reserved.",AB_0370
"pansharpening aims to sharpen a low-resolution multispectral (ms) image through a high-resolution single-channel panchromatic (pan) image to obtain a high-resolution multi-spectral (hrms) image. how-ever, low correlation between the pan and ms images, as well as the inaccurate detail injection for each band of ms image are the key problems causing spectral and spatial distortions in pansharpening. to ad-dress these issues, a new pansharpening method based on the intensity mixture and band-adaptive detail fusion is proposed. to obtain a mixed-intensity image ( t ) that has a high correlation with the ms image and maintain the gradient information of the pan image, the intensity mixture model is constructed by establishing the intensity and gradient constraints between t and the source images. as it is hard to ob-tain a proper degradation filter in the model, a filter estimation algorithm is designed by the distribution alignment. to inject the details that match the point spread function of the sensor, a band-adaptive detail fusion algorithm is presented to fuse the details extracted from t with those from the ms image for each band. furthermore, as there are far fewer details in the ms image than in t , a detail enhancement algo-rithm is proposed to enhance the details proportionally. the final hrms image is obtained by injecting the fused details into the upsampled ms image. extensive experiments show that the proposed method can efficiently achieve the best results in fusion quality compared to state-of-the-art methods. the code is availabe at https://github.com/yotick/imbd .(c) 2023 elsevier ltd. all rights reserved.",AB_0370
"combinatorial optimization problems such as traveling salesman problem (tsp) have a wide range of real-world applications in transportation, logistics, manufacturing. it has always been a difficult problem to solve large-scale tsp problems quickly because of memory usage limitations. recent research shows that the transformer model is a promising approach. however, the transformer has several severe problems that prevent it from quickly solving tsp combinatorial optimization problems, such as quadratic time complexity, especially quadratic space complexity, and the inherent limitations of the encoder and decoder itself. to address these issues, we developed a memory-efficient transformer-based network model for tsp combinatorial optimization problems, termed tspformer, with two distinctive characteristics: (1) a sampled scaled dot-product attention mechanism with o(l log(l)) (l is the length of input sequences) time and space complexity, which is the most different between our work and other works. (2) due to the reduced space complexity, gpu/cpu memory usage is significantly reduced. extensive experiments demonstrate that tspformer significantly outperforms existing methods and provides a new solution to the tsp combinatorial optimization problems. our pytorch code will be publicly available on github https://github.com/yhnju/tspformer.(c) 2023 elsevier ltd. all rights reserved.",AB_0370
