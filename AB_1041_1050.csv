AB,NO
"background: co-localized sets of genes that encode specialized functions are common across microbial genomes and occur in genomes of larger eukaryotes as well. important examples include biosynthetic gene clusters (bgcs) that produce specialized metabolites with medicinal, agricultural, and industrial value (e.g. antimicrobials). comparative analysis of bgcs can aid in the discovery of novel metabolites by highlighting distribution and identifying variants in public genomes. unfortunately, genecluster-level homology detection remains inaccessible, time- consuming and difficult to interpret. results: the comparative gene cluster analysis toolbox (cagecat) is a rapid and userfriendly platform to mitigate difficulties in comparative analysis of whole gene clusters. the software provides homology searches and downstream analyses without the need for command-line or programming expertise. by leveraging remote blast databases, which always provide up-to-date results, cagecat can yield relevant matches that aid in the comparison, taxonomic distribution, or evolution of an unknown query. the service is extensible and interoperable and implements the cblaster and clinker pipelines to perform homology search, filtering, gene neighbourhood estimation, and dynamic visualisation of resulting variant bgcs. with the visualisation module, publication-quality figures can be customized directly from a web-browser, which greatly accelerates their interpretation via informative overlays to identify conserved genes in a bgc query. conclusion: overall, cagecat is an extensible software that can be interfaced via a standard web-browser for whole region homology searches and comparison on continually updated genomes from ncbi. the public web server and installable docker image are open source and freely available without registration at: https://cagecat. bioinformatics.nl.",AB_0105
"with the rapid development of deep neural networks, the demand for large-scale accurately labeled datasets is growing rapidly. however, human-labeled datasets often produce misleading information due to mistakes in manual labeling. most previous works fail to control well the overfitting of the trained model to noisy labels. in this paper, we propose a novel two-stage learning framework for learning with noisy labels, called the two-stage training method with contradictory loss and co-teaching based on meta-learning (tcc-net). first, a novel robust loss function called contradictory loss is designed for pre-training, which is proved to be sufficiently robust both in the experimental results and theoretical foundation. during the training stage, we co-teach two pretrained networks based on meta-learning without any auxiliary clean subset as meta-data. unlike other co-teaching methods, we introduce two multilayer perceptron to assist in weighting the selected samples, meaning each network updates itself with weighted-selected samples by its peer network and self-perceptron. experimental results on corrupted datasets, such as cifar10, cifar100, animal10n, and clothing1m, demonstrate that tcc-net is superior to other state-of-the-art methods on shallower layers. specifically, we achieve 12.59% improvement on synthetic cifar10 with 80% symmetric noise and 0.27% on the real-world animal10n dataset. code is released at https://github.com/qiangqiangxia/tcc-net.",AB_0105
"background: the health effects of heat are well documented; however, limited information is available regarding the health risks of hot nights. hot nights have become more common, increasing at a faster rate than hot days, making it urgent to understand the characteristics of the hot night risk.objectives: we estimated the effects of hot nights on the cause-and location-specific mortality in a nationwide assessment over 43 y (1973-2015) using a unified analytical framework in the 47 prefectures of japan.methods: hot nights were defined as days with a) minimum temperature & ge;25 & deg;c (hn25) and b) minimum temperature & ge;95th percentile (hn95th) for the prefecture. we conducted a time-series analysis using a two-stage approach during the hot night occurrence season (april-november). for each prefecture, we estimated associations between hot nights and mortality controlling for potential confounders including daily mean temperature. we then used a random-effects meta-analytic model to estimate the pooled cumulative association.results: overall, 24,721,226 deaths were included in this study. nationally, all-cause mortality increased by 9%-10% [hn25 relative risk (rr) = 1.09, 95% confidence interval (ci): 1.08, 1.10; hn95th rr = 1.10, 95% ci: 1.09, 1.11] during hot nights in comparison with nonhot nights. all 11 cause-specific mortalities were strongly associated with hot nights, and the corresponding associations appeared to be acute and lasted a few weeks, depending on the cause of death. the strength of the association between hot nights and mortality varied among prefectures. we found a higher mortality risk from hot nights in early summer in comparison with the late summer in all regions.conclusions: our findings support the evidence of mortality impacts from hot nights in excess of that explicable by daily mean temperature and have implications useful for establishing public health policy and research efforts estimating the health effects of climate change. https://doi.org/ 10.1289/ehp11444",AB_0105
"this paper proposes a density-based topology optimization method for natural convection problems using the lattice boltzmann method (lbm). as the lbm can be developed as a completely explicit scheme, its attractive features over the traditional ones, such as the finite element method, are (1) suitability for solving unsteady flow problems and (2) scalability for large-scale parallel computing. we develop an lbm code for solving unsteady natural convection problems and provide its sensitivity analysis based on the so-called adjoint lattice boltzmann method. notably, the adjoint equation is derived from the discrete particle velocity boltzmann equation and can be solved similarly to the original lbm concerning unsteady natural convection problems. we first show that the proposed method can produce similar results to the previous work in a steady-state natural convection problem. we then demonstrate the efficacy of the proposed method through 2d numerical examples concerning unsteady natural convection. as a large-scale problem, we tackle a 3d unsteady natural convection problem on a parallel supercomputer. all the developed codes written in c++ are available at https://github.com/panfactory/panslbm2.git.",AB_0105
"while natural systems often present collective intelligence that allows them to self-organize and adapt to changes, the equivalent is missing in most artificial systems. we explore the possibility of such a system in the context of cooperative 2d push manipulations using mobile robots. although conventional works demonstrate potential solutions for the problem in restricted settings, they have computational and learning difficulties. more importantly, these systems do not possess the ability to adapt when facing environmental changes. in this work, we show that by distilling a planner derived from a differentiable soft-body physics simulator into an attention-based neural network, our multi-robot push manipulation system achieves better performance than baselines. in addition, our system also generalizes to configurations not seen during training and is able to adapt toward task completions when external turbulence and environmental changes are applied. supplementary videos can be found on our project website: https://sites.google.com/view/ ciom/home.",AB_0105
"in this paper, we come up with a simple yet effective approach for instance segmentation on 3d point cloud with strong robustness. previous top-performing methods for this task adopt a bottom-up strategy, which often involves various inefficient operations or complex pipelines, such as grouping over-segmented components, introducing heuristic post-processing steps, and designing complex loss functions. as a result, the inevitable variations of the instances sizes make it vulnerable and sensitive to the values of pre-defined hyper-parameters. to this end, we instead propose a novel pipeline that applies dynamic convolution to generate instance-aware parameters in response to the characteristics of the instances. the representation capability of the parameters is greatly improved by gathering homogeneous points that have identical semantic categories and close votes for the geometric centroids. instances are then decoded via several simple convolution layers, where the parameters are generated depending on the input. in addition, to introduce a large context and maintain limited computational overheads, a light-weight transformer is built upon the bottleneck layer to capture the long-range dependencies. with the only post-processing step, non-maximum suppression (nms), we demonstrate a simpler and more robust approach that achieves promising performance on various datasets: scannetv2, s3dis, and partnet. the consistent improvements on both voxel- and point-based architectures imply the effectiveness of the proposed method. code is available at: https://git.io/dyco3d.",AB_0105
"genomic selection (gs) is expected to accelerate plant and animal breeding. during the last decade, genome-wide polymorphism data have increased, which has raised concerns about storage cost and computational time. several individual studies have attempted to compress the genome data and predict phenotypes. however, compression models lack adequate quality of data after compression, and prediction models are time consuming and use original data to predict the phenotype. therefore, a combined application of compression and genomic prediction modeling using deep learning could resolve these limitations. a deep learning compression-based genomic prediction (deepcgp) model that can compress genome-wide polymorphism data and predict phenotypes of a target trait from compressed information was proposed. the deepcgp model contained two parts: (i) an autoencoder model based on deep neural networks to compress genome-wide polymorphism data, and (ii) regression models based on random forests (rf), genomic best linear unbiased prediction (gblup), and bayesian variable selection (bayesb) to predict phenotypes from compressed information. two datasets with genome-wide marker genotypes and target trait phenotypes in rice were applied. the deepcgp model obtained up to 99% prediction accuracy to the maximum for a trait after 98% compression. bayesb required extensive computational time among the three methods, and showed the highest accuracy; however, bayesb could only be used with compressed data. overall, deepcgp outperformed state-of-the-art methods in terms of both compression and prediction. our code and data are available at https://github.com/tanzilamohita/deepcgp.",AB_0105
"in recent years, thanks to the inherent powerful feature representation and learning abilities of the convolutional neural network (cnn), deep cnn-steered single image super-resolution approaches have achieved remarkable performance improvements. however, these methods are often accompanied by large consumption of computing and memory resources, which is difficult to be adopted in real-world application scenes. to handle this issue, we design an efficient feature de-redundancy and self-calibration super-resolution network (fdscsr). in particular, a feature de-redundancy and self-calibration block (fdscb) is proposed to reduce the repetitive feature information extracted by the model and further enhance the efficiency of the model. then, based on fdscb, a local feature fusion module is presented to elaborately utilize and fuse the feature information extracted by each fdscb. abundant experiments on benchmarks have demonstrated that our fdscsr achieves superior performance with relatively less computational consumption and storage resource than other state-of-the-art approaches. the code is available at https://github.com/iviplab/fdscsr.",AB_0105
"background: asthma is a heterogeneous disease with multiple phenotypes that are useful in precision medicine. as the population ages, the elderly asthma (ea, aged > 65 years) population is growing, and ea is now a major health problem worldwide.objective: to characterize ea and identify its phenotypes.methods: in adult patients with asthma (aged > 18 years) who had been diagnosed with having asthma at least 1 year before study enrollment, 1925 were included in the nhom-asthma (registered in umin-ctr; umin000027776), and the data were used for this study, jfge-asthma (registered in umin-ctr; umin000036912). data from ea and non-ea (nea) groups were compared, and ward's minimum-variance hierarchical clustering method and principal component analysis were performed.results: ea was characterized by older asthma onset, longer asthma duration and smoking history, more comorbidities, lower pulmonary function, less atopic, lower adherence, and more hospital admissions because of asthma. in contrast, the number of eosinophils, total immunoglobulin e level, oral corticosteroid use, and asthma control questionnaire scores were equivalent between ea and nea. there were 3 distinct phenotypes in ea, which are as follows: ea1: youngest, late onset, short duration, mild; ea2: early onset, long duration, atopic, low lung function, moderate; and ea3: oldest, eosinophilic, overweight, low lung function, most severe. the classification factors of the ea phenotypes included the age of onset and asthma control questionnaire-6. similarities were observed between ea and nea phenotypes after principal component analysis.conclusion: the ea in japan may be unique because of the population's high longevity. characterization of ea phenotypes from the present cohort indicated the need for distinct precision medicine for ea.trial registration: jfge-asthma registered in umin-ctr (https://www.umin.ac.jp/ctr/); umin000036912.& copy; 2023 american college of allergy, asthma & immunology. published by elsevier inc. all rights reserved.",AB_0105
"this article investigates a new challenging problem called defensive few-shot learning in order to learn a robust few-shot model against adversarial attacks. simply applying the existing adversarial defense methods to few-shot learning cannot effectively solve this problem. this is because the commonly assumed sample-level distribution consistency between the training and test sets can no longer be met in the few-shot setting. to address this situation, we develop a general defensive few-shot learning (dfsl) framework to answer the following two key questions: (1) how to transfer adversarial defense knowledge from one sample distribution to another? (2) how to narrow the distribution gap between clean and adversarial examples under the few-shot setting? to answer the first question, we propose an episode-based adversarial training mechanism by assuming a task-level distribution consistency to better transfer the adversarial defense knowledge. as for the second question, within each few-shot task, we design two kinds of distribution consistency criteria to narrow the distribution gap between clean and adversarial examples from the feature-wise and prediction-wise perspectives, respectively. extensive experiments demonstrate that the proposed framework can effectively make the existing few-shot models robust against adversarial attacks. code is available at https://github.com/wenbinlee/defensivefsl.git.",AB_0105
