AB,NO
"several optimization-based population search methods have been proposed; they use various operators that permit exploring the search space. these methods typically suffer from local search (ls) problems and are unbalanced between exploration and exploitation. consequently, recent researchers sought to modify the algorithms to avoid search problems using local search techniques to intensify the exploitation when is necessary. this paper proposes a novel single-based local search optimization algorithm called the non-monopolize search (no). the no is a single-solution metaphor-free algorithm, and its operators are designed based to explore and exploit along the iterative process. the no works only with a candidate solution, and the operators modify the dimension to move the current solution along the search space. the no is an effective ls method that combines the benefits of exploration with exploitation. different from other ls, the no can escape from suboptimal solutions thanks to the randomness incorporated into its operators. this is the main advantage of the no. experiments are conducted on standard benchmark functions to validate the performance of the proposed non-monopolize search optimization technique. the results are compared with other well-known methods, and the proposed no got better results. moreover, the proposed no can be considered a powerful alternative to improve the optimization algorithms' performance and help avoid local search problems. source codes of no are publicly available at https://www.mathworks.com/matlabcentral/fileexchange/156154-the-non-monopolize-search-no.",AB_0327
"as a critical part of the 3d human pose estimation (hpe), establishing the 2d-to-3d lifting mapping is limited by depth ambiguity. most current works generally lack the quantitative analysis of the relative depth expression and the depth ambiguity error expression in lifting mapping, resulting in low prediction efficiency and poor interpretability. to this end, this paper mines and leverages prior geometric knowledge of these expressions based on the pinhole imaging principle, decoupling the 2d-to-3d lifting mapping and simplifying the model training. specifically, this paper proposes a prior geometric knowledge oriented pose estimation model with two-branch transformer architectures, explicitly introducing high-dimensional prior geometric features to improve model efficiency and interpretability. it converts the regression of spatial coordinates into the prediction of spatial direction vectors between joints to generate multiple feasible solutions further alleviate the depth ambiguity. moreover, this paper raises a novel non-learning-based absolute depth estimation algorithm based on prior geometric relationship decoupling from relative depth expression for the first time. it establishes multiple independent depth mapping from non-root nodes to the root node to calculate the absolute depth candidate, which is parameter-free, plug-and-play, and interpretable. experiments show that the proposed pose estimation model achieves state-of-the-art performance on human 3.6m and mpi-inf-3dhp benchmarks with lower parameters and faster inference speed, and the proposed absolute depth estimation algorithm achieves similar performance to traditional methods without any network parameters. the source code are available at https://github.com/humengxian/gkonet.",AB_0327
"the salient object detection of optical remote sensing images (orsi-sod) is an important research direction in orsi processing, which has achieved promising results in the last few years. many recent works heavily rely on feature learning of regions for the improvement of the detection accuracy, while neglecting the concrete role of edge and skeleton information in the calculation. in this work, we propose a two-stage edge and skeleton guidance network (esgnet) for orsi-sod in a coarse-to-fine way, and further demonstrate that the fused features of edge and skeleton are essential for orsi-sod. in the first stage, we construct the spatial graph attention (sga) module for saliency features to generate an initial saliency map, and apply the spatial self-optimization (sso) to enhance edge and skeleton features. the multi-level interactive fusion (mif) module is used for the adequate integration of edge and skeleton features into saliency features. in the second stage, with the aim to accomplish better prediction of salient object localization and shape, the feature enhancement integration operation is introduced to recover object details from the learned edge and skeleton features. extensive experiments on three public orsi-sod datasets demonstrate that our esgnet achieves competitive performance with the state-of-the-art methods and also confirms the importance of edge and skeleton information for orsi-sod. meanwhile, generalizability experiments on natural image datasets show that our method is competent for many types of sod tasks. the code and results of our method are available at https://github.com/aoao0206/esgnet.",AB_0327
"image aesthetics assessment (iaa) has attracted growing interest in recent years but is still challenging due to its highly abstract nature. nowadays, more and more people tend to comment images shared on the social networks, which can provide rich aesthetics-aware semantic information from different aspects. therefore, user comments of an image can be exploited as supplementary information for enhancing aesthetic representation learning. previous researches have demonstrated that aesthetic attributes make significant effect on image aesthetic quality and humans' aesthetic perception. typically, people are used to give comments on an image from the perspective of aesthetic attributes, based on which the aesthetic quality of images can be inferred. motivated by this, this paper presents an attribute-assisted multimodal memory network (amm-net) for image aesthetics assessment, which utilizes aesthetic attributes to model the interactions between visual and textual modalities. specifically, we design two memory networks to capture the attribute-aware information most related to the image and associated comments respectively. further, with multiple memory hops, attribute semantics shared by the two modalities are refined and cross-modal interactions are enhanced progressively. finally, more discriminative aesthetic representations can be obtained for iaa. the experimental results and comparisons on two public multimodal iaa datasets demonstrate the superiority of the proposed model over the state-of-the-art methods. the source code is available at https://github.com/zhutong0219/amm-net.",AB_0327
"few-shot fine-grained image classification has attracted considerable attention in recent years for its realistic setting to imitate how humans conduct recognition tasks. metric-based few-shot classifiers have achieved high accuracies. however, their metric function usually requires two arguments of vectors, while transforming or reshaping three-dimensional feature maps to vectors can result in loss of spatial information. image reconstruction is thus involved to retain more appearance details: the test images are reconstructed by different classes and then classified to the one with the smallest reconstruction error. however, discriminative local information, vital to distinguish sub-categories in fine-grained images with high similarities, is not well elaborated when only the base features from a usual embedding module are adopted for reconstruction. hence, we propose the novel local content-enriched cross-reconstruction network (lccrn) for few-shot fine-grained classification. in lccrn, we design two new modules: the local content-enriched module (lcem) to learn the discriminative local features, and the cross-reconstruction module (crm) to fully engage the local features with the appearance details obtained from a separate embedding module. the classification score is calculated based on the weighted sum of reconstruction errors of the cross-reconstruction tasks, with weights learnt from the training process. extensive experiments on four fine-grained datasets showcase the superior classification performance of lccrn compared with the state-of-the-art few-shot classification methods. codes are available at: https://github.com/lutsong/lccrn.",AB_0327
"light field salient object detection (lf sod) aims to segment the visually distinctive objects out of surroundings. since light field images provide a multi-focus stack (many focal slices in different depth levels) and an all-focus image for the same scene, they record comprehensive but redundant information. existing methods exploit the useful cue by long short-term memory with attention mechanism, 3d convolution, and graph learning. however, the importance of intra-slice and inter-slice in the focal stack is not well investigated. in the paper, we propose a learnable weight descriptor to simultaneously exploit different weights in slice, spatial region, and channel dimensions, and therefore propose an lf sod method based on the learnable descriptor. the method extracts slice features and all-focus features from a weight-shared backbone and another backbone, respectively. a transformer decoder is used to learn the weight descriptor which both emphasizes the importance of each slice (inter-slice) and discriminates the spatial and channel importance of each slice (intra-slice). the learnt descriptor serves as the weight to make slice features attend to important slices, regions, and channels. furthermore, we propose the hierarchical multi-modal fusion which aggregates high-layer features by modelling the long-range dependency to fully excavate common salient semantics and combines low-layer features by spatial constraint to eliminate the blurring effect of slice features. the experimental result exceeds the state-of-the-art methods at least 25% in terms of mean absolute error evaluation metric. it demonstrates a significant improvement in lf sod performance via the designed learnable weight descriptor. https://github.com/liuzywen/lftransnet",AB_0327
"most cutting-edge video saliency prediction models rely on spatiotemporal features extracted by 3d convolutions due to its local contextual cues acquirement ability. however, the shortage of 3d convolutions is that it cannot effectively capture long-term spatiotemporal dependencies in videos. to address this limitation, we propose a novel transformer-based multi-scale feature integration network (tmfi-net) for video saliency prediction, where the proposed tmfi-net consists of a semantic-guided encoder and a hierarchical decoder. firstly, embarking on the transformer-based multi-level spatiotemporal features, the semantic-guided encoder enhances the features by inserting the high-level feature into each level feature via a top-down pathway and a longitudinal connection, which endows the multi-level spatiotemporal features with rich contextual information. in this way, the features are steered to give more concerns to saliency regions. secondly, the hierarchical decoder employs a multi-dimensional attention (ma) module to elevate features along channel, temporal, and spatial dimensions jointly. successively, the hierarchical decoder deploys a progressive decoding block to conduct an initial saliency prediction, which provides a coarse localization of saliency regions. lastly, considering the complementarity of different saliency predictions, we integrate all initial saliency prediction results into the final saliency map. comprehensive experimental results on four video saliency datasets firmly demonstrate that our model achieves superior performance when compared with the state-of-the-art video saliency models. the code is available at https://github.com/wusonghe/tmfi-net.",AB_0327
"sentiment analysis has broad application prospects in the field of social opinion mining. the openness and invisibility of the internet makes users' expression styles more diverse and thus results in the blooming of complicated contexts in which different unimodal data have inconsistent sentiment tendencies. however, most sentiment analysis algorithms only focus on designing multimodal fusion methods without preserving the individual semantics of each unimodal data. to avoid misunderstandings caused by ambiguity and sarcasm in complicated contexts, we propose a multimodal mutual attention-based sentiment analysis (mmsa) framework adapted to complicated contexts, which consists of three levels of subtasks to preserve the unimodal unique semantics and enhance the common semantics, to mine the association between unique semantics and common semantics and to balance decisions from unique and common semantics. in the framework, a multiperspective and hierarchical fusion (mhf) module is developed to fully fuse multimodal data, in which different modalities are mutually constrained and the fusion order is adjusted in the next step to enhance cross-modal complementarity. to balance the data, we calculate the loss by applying different weights to positive and negative samples. the experimental results on the ch-sims multimodal dataset show that our method outperforms existing multimodal sentiment analysis algorithms.the code of this work is available at https://gitee.com/viviziqing/mmsacode.",AB_0327
"deep neural network-based image compression has been extensively studied. however, the model robustness which is crucial to practical application is largely overlooked. we propose to examine the robustness of prevailing learned image compression models by injecting negligible adversarial perturbation into the original source image. severe distortion in decoded reconstruction reveals the general vulnerability in existing methods regardless of their settings (e.g., network architecture, loss function, quality scale). a variety of defense strategies including geometric self-ensemble based pre-processing, and adversarial training, are investigated against the adversarial attack to improve the model's robustness. later the defense efficiency is further exemplified in real-life image recompression case studies. overall, our methodology is simple, effective, and generalizable, making it attractive for developing robust learned image compression solutions. all materials are made publicly accessible at https://njuvision.github.io/robustnic for reproducible research.",AB_0327
"partial query based image retrieval (pqir) enables a search engine to perform an interactive retrieval given by only an initial query and actively provide alternative feedbacks for a user to refine a set of retrieval results. it alleviates the deficiency in practice interactive image retrieval that requires the user to laboriously provide detailed feedbacks, and enables the retrieval on-the-fly with the incomplete initial query. although significant progress has been made, existing works remain have challenge in actively providing more discriminative feedbacks. to address this challenge, we propose a novel attributes&objects-based consensus extraction and representation (aocer) framework. specifically, we formulate a simple but effective attribute&object feedback (aof) paradigm, which employs both attributes and objects as intermediate feedbacks to carry out multiple rounds of interaction. to mine the intrinsic associations among concepts and enhance their feature representations, we further propose an interventional consensus representation learning (icrl) module, which mainly constructs an interventional concept graph to yield the interventional consensus representation (icr). in addition, a dual-head feedback sampler (dhfs) is developed to sample objects and attributes for conducting the next round retrieval. extensive experiments demonstrate the superiority of the proposed framework. our source code will be released at https://github.com/zhangy0822/aocer.",AB_0327
