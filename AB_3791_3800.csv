AB,NO
"for the long-term person re-identification (reid) task, pedestrians are likely to change clothes, which poses a key challenge in overcoming drastic appearance variations caused by these cloth changes. however, analyzing how cloth changes influence identity-invariant representation learning is difficult. in this context, varying cloth-changed samples are not adaptively utilized, and their effects on the resulting features are overshadowed. to address these limitations, this paper aims to estimate the effect of cloth-changing patterns at both the image and feature levels, presenting a dual-level adaptive weighting (dlaw) solution. specifically, at the image level, we propose an adaptive mining strategy to locate the cloth-changed regions for each identity. this strategy highlights the informative areas that have undergone changes, enhancing robustness against cloth variations. at the feature level, we estimate the degree of cloth-changing by modeling the correlation of part-level features and re-weighting identity-invariant feature components. this further eliminates the effects of cloth variations at the semantic body part level. extensive experiments demonstrate that our method achieves promising performance on several cloth-changing datasets. code and models are available at https://github.com/fountaindream/dlaw.",AB_0380
"visual grounding, aiming to align image regions with textual queries, is a fundamental task for cross-modal learning. we study the weakly supervised visual grounding, where only image-text pairs at a coarse-grained level are available. due to the lack of fine-grained correspondence information, existing approaches often encounter matching ambiguity. to overcome this challenge, we introduce the cycle consistency constraint into region-phrase pairs, which strengthens correlated pairs and weakens unrelated pairs. this cycle pairing makes use of the bidirectional association between image regions and text phrases to alleviate matching ambiguity. furthermore, we propose a parallel grounding framework, where backbone networks and subsequent relation modules extract individual and contextual representations to calculate context-free and context-aware similarities between regions and phrases separately. those two representations characterize visual/linguistic individual concepts and inter-relationships, respectively, and then complement each other to achieve cross-modal alignment. the whole framework is trained by minimizing an image-text contrastive loss and a cycle consistency loss. during inference, the above two similarities are fused to give the final region-phrase matching score. experiments on five popular datasets about visual grounding demonstrate a noticeable improvement in our method. the source code is available at https://github.com/evergrow/wsvg.",AB_0380
"lately, video-language pre-training and text-video retrieval have attracted significant attention with the explosion of multimedia data on the internet. however, existing approaches for video-language pre-training typically limit the exploitation of the hierarchical semantic information in videos, such as frame semantic information and global video semantic information. in this work, we present an end-to-end pre-training network with hierarchical matching and momentum contrast named hmmc. the key idea is to explore the hierarchical semantic information in videos via multilevel semantic matching between videos and texts. this design is motivated by the observation that if a video semantically matches a text (can be a title, tag or caption), the frames in this video usually have semantic connections with the text and show higher similarity than frames in other videos. hierarchical matching is mainly realized by two proxy tasks: video-text matching (vtm) and frame-text matching (ftm). another proxy task: frame adjacency matching (fam) is proposed to enhance the single visual modality representations while training from scratch. furthermore, momentum contrast framework was introduced into hmmc to form a multimodal momentum contrast framework, enabling hmmc to incorporate more negative samples for contrastive learning which contributes to the generalization of representations. we also collected a large-scale chinese video-language dataset (over 763k unique videos) named chvtt to explore the multilevel semantic connections between videos and texts. experimental results on two major text-video retrieval benchmark datasets demonstrate the advantages of our methods. we release our code at https://github.com/cheetah003/hmmc.",AB_0380
"human action recognition plays a driving engine of many human-computer interaction applications. most current researches focus on improving the model generalization by integrating multiple homogeneous modalities, including rgb images, human poses, and optical flows. furthermore, contextual interactions and out-of-context sign languages have been validated to depend on scene category and human per se. those attempts to integrate appearance features and human poses have shown positive results. however, with human poses' spatial errors and temporal ambiguities, existing methods are subject to poor scalability, limited robustness, and sub-optimal models. in this paper, inspired by the assumption that different modalities may maintain temporal consistency and spatial complementarity, we present a novel bi-directional co-temporal and cross-spatial attention fusion model (b2c-afm). our model is characterized by the asynchronous fusion strategy of multi-modal features along temporal and spatial dimensions. besides, the novel explicit motion-oriented pose representations called limb flow fields (lff) are explored to alleviate the temporal ambiguity regarding human poses. experiments on publicly available datasets validate our contributions. abundant ablation studies experimentally show that b2c-afm achieves robust performance across seen and unseen human actions. the codes are available at https://github.com/gftww/b2c.git.",AB_0380
"although adversarial examples pose a serious threat to deep neural networks, most transferable adversarial attacks are ineffective against black-box defense models. this may lead to the mistaken belief that adversarial examples are not truly threatening. in this paper, we propose a novel transferable attack that can defeat a wide range of black-box defenses and highlight their security limitations. we identify two intrinsic reasons why current attacks may fail, namely data-dependency and network-overfitting. they provide a different perspective on improving the transferability of attacks. to mitigate the data-dependency effect, we propose the data erosion method. it involves finding special augmentation data that behave similarly in both vanilla models and defenses, to help attackers fool robustified models with higher chances. in addition, we introduce the network erosion method to overcome the network-overfitting dilemma. the idea is conceptually simple: it extends a single surrogate model to an ensemble structure with high diversity, resulting in more transferable adversarial examples. two proposed methods can be integrated to further enhance the transferability, referred to as erosion attack (ea). we evaluate the proposed ea under different defenses that empirical results demonstrate the superiority of ea over existing transferable attacks and reveal the underlying threat to current robust models. the source code is publicly available at https://github.com/mesunhlf/ea.",AB_0380
"considering the spectral properties of images, we propose a new self-attention mechanism with highly reduced computational complexity, up to a linear rate. to better preserve edges while promoting similarity within objects, we propose individualized processes over different frequency bands. in particular, we study a case where the process is merely over low-frequency components. by ablation study, we show that low frequency self-attention can achieve very close or better performance relative to full frequency even without retraining the network. accordingly, we design and embed novel plug-and-play modules to the head of a cnn network that we refer to as fsanet. the frequency self-attention 1) requires only a few low frequency coefficients as input, 2) can be mathematically equivalent to spatial domain self-attention with linear structures, 3) simplifies token mapping (1x1 convolution) stage and token mixing stage simultaneously. we show that frequency self-attention requires 87.29% similar to 90.04% less memory, 96.13% similar to 98.07% less flops, and 97.56% similar to 98.18% in run time than the regular self-attention. compared to other resnet101-based self-attention networks, fsanet achieves a new state-of-the-art result (83.0% miou) on cityscape test dataset and competitive results on ade20k and vocaug. fsanet can also enhance mask r-cnn for instance segmentation on coco. in addition, utilizing the proposed module, segformer can be boosted on a series of models with different scales, and segformer-b5 can be improved even without retraining. code is accessible at https://github.com/zfycsu/fsanet.",AB_0380
"person re-identification (re-id) aims to match the same person across different cameras. however, most existing re-id methods assume that people wear the same clothes in different views, which limit their performance in identifying target pedestrians who change clothes. cloth-changing re-id is a quite challenging problem as clothes occupying a large number of pixels in an image becomes invalid or even misleads information. to tackle this problem, we propose a novel multi-biometric unified network (mbunet) for learning the robustness of cloth-changing re-id model by exploiting clothing-independent cues. specifically, we first introduce a multi-biological feature branch to extract a variety of biological features, such as the head, neck, and shoulders to resist cloth-changing. then, a differential feature attention module (dfam) is embedded in this branch, which can extract discriminative fine-grained biological features. besides, we design a differential recombination on max pooling (drmp) strategy and simultaneously apply a direction-adaptive graph convolutional layer to mine more robust global and pose features. finally, we propose a lightweight domain adaptation module (ldam) that combines the attention mechanism before and after the waveblock to capture and enhance transferable features across scenarios. to further improve the performance of the model, we also integrate map optimization into the objective function of our model for joint training to solve the discrete optimization problem of map. extensive experiments on five cloth-changing re-id datasets demonstrate the advantages of our proposed mbunet. the code is available at https://github.com/liyeabc/mbunet.",AB_0380
"sketch is a well-researched topic in the vision community by now. sketch semantic segmentation in particular, serves as a fundamental step towards finer-level sketch interpretation. recent works use various means of extracting discriminative features from sketches and have achieved considerable improvements on segmentation accuracy. common approaches for this include attending to the sketch-image as a whole, its stroke-level representation or the sequence information embedded in it. however, they mostly focus on only a part of such multi-facet information. in this paper, we for the first time demonstrate that there is complementary information to be explored across all these three facets of sketch data, and that segmentation performance consequently benefits as a result of such exploration of sketch-specific information. specifically, we propose the sketch-segformer, a transformer-based framework for sketch semantic segmentation that inherently treats sketches as stroke sequences other than pixel-maps. in particular, sketch-segformer introduces two types of self-attention modules having similar structures that work with different receptive fields (i.e., whole sketch or individual stroke). the order embedding is then further synergized with spatial embeddings learned from the entire sketch as well as localized stroke-level information. extensive experiments show that our sketch-specific design is not only able to obtain state-of-the-art performance on traditional figurative sketches (such as spg, sketchseg-150k datasets), but also performs well on creative sketches that do not conform to conventional object semantics (creativesketch dataset) thanks for our usage of multi-facet sketch information. ablation studies, visualizations, and invariance tests further justifies our design choice and the effectiveness of sketch-segformer. codes are available at https://github.com/pris-cv/sketch-sf.",AB_0380
"as a crucial application in privacy protection, scene text removal (str) has received amounts of attention in recent years. however, existing approaches coarsely erasing texts from images ignore two important properties: the background texture integrity (bi) and the text erasure exhaustivity (ee). these two properties directly determine the erasure performance, and how to maintain them in a single network is the core problem for str task. in this paper, we attribute the lack of bi and ee properties to the implicit erasure guidance and imbalanced multi-stage erasure respectively. to improve these two properties, we propose a new progressively region-based scene text eraser (pert). there are three key contributions in our study. first, a novel explicit erasure guidance is proposed to enhance the bi property. different from implicit erasure guidance modifying all the pixels in the entire image, our explicit one accurately performs stroke-level modification with only bounding-box level annotations. second, a new balanced multi-stage erasure is constructed to improve the ee property. by balancing the learning difficulty and network structure among progressive stages, each stage takes an equal step towards the text-erased image to ensure the erasure exhaustivity. third, we propose two new evaluation metrics called bi-metric and ee-metric, which make up the shortcomings of current evaluation tools in analyzing bi and ee properties. compared with previous methods, pert outperforms them by a large margin in both bi-metric (?6.13%) and ee-metric (?1.9%), obtaining sota results with high speed (71 fps) and at least 25% lower parameter complexity. code will be available at https://github.com/wangyuxin87/pert.",AB_0380
"video frame interpolation (vfi) aims to synthesize an intermediate frame between two consecutive frames. state-of-the-art approaches usually adopt a two-step solution, which includes 1) generating locally-warped pixels by calculating the optical flow based on pre-defined motion patterns (e.g., uniform motion, symmetric motion), 2) blending the warped pixels to form a full frame through deep neural synthesis networks. however, for various complicated motions (e.g., non-uniform motion, turn around), such improper assumptions about pre-defined motion patterns introduce the inconsistent warping from the two consecutive frames. this leads to the warped features for new frames are usually not aligned, yielding distortion and blur, especially when large and complex motions occur. to solve this issue, in this paper we propose a novel trajectory-aware transformer for video frame interpolation (ttvfi). in particular, we formulate the warped features with inconsistent motions as query tokens, and formulate relevant regions in a motion trajectory from two original consecutive frames into keys and values. self-attention is learned on relevant tokens along the trajectory to blend the pristine features into intermediate frames through end-to-end training. experimental results demonstrate that our method outperforms other state-of-the-art methods in four widely-used vfi benchmarks. both code and pre-trained models will be released at https://github.com/chengxuliu/ttvfi.",AB_0380
