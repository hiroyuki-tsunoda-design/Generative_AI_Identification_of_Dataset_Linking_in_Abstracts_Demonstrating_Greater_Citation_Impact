AB,NO
"existing localization methods that intensively leverage the environment-specific received signal strength (rss) or channel state information (csi) of wireless signals are rather accurate in certain environments. however, these methods, whether based on pure statistical signal processing or data-driven approaches, often struggle to generalize to new environments, which results in considerable time and effort being wasted. to address this challenge, we propose metaloc, which is the first fingerprinting-based localization framework that leverages the model-agnostic meta-learning (maml). specifically, built on a deep neural network with strong representation capabilities, metaloc is trained on historical data sourced from well-calibrated environments, employing a two-loop optimization mechanism to obtain the meta-parameters. these meta-parameters act as the initialization for quick adaptation in new environments, reducing the need for much human effort. the framework introduces two paradigms for the optimization of meta-parameters: a centralized paradigm that simplifies the process by sharing data from all historical environments, and a distributed paradigm that maintains data privacy by training meta-parameters for each specific environment separately. furthermore, the advanced distributed paradigm modifies the vanilla maml loss function to ensure that the reduction of loss occurs in a consistent direction across various training domains, thus facilitating faster convergence during training. our experiments on both synthetic and real datasets demonstrate that metaloc outperforms baseline methods in terms of localization accuracy, robustness, and cost-effectiveness. the code and datasets used in this study are publicly available at: https://github.com/wu-dongze/metaloc.",AB_0328
"recently, semantic segmentation based on rgb and thermal infrared (tir) images has become a research hotspot because of its stability in the weak light environment. however, most of the current methods ignore the differences between the two modalities of data and do not use semantic information in multi-modal fusion. in this paper, we propose a novel semantic-guided fusion network (sgfnet) for rgb-thermal semantic segmentation, which makes full use of semantic information in the multi-modal fusion. our sgfnet consists of an asymmetric encoder with tir branch and rgb branch and a decoder. we concentrate on enhancing the multi-modal feature representation in the encoder with a pattern of fusion and enhancement. specifically, considering that tir images are stable under weak light conditions, we first propose a semantic guidance head to extract semantic information in the tir branch. in the rgb branch, we propose a multi-modal coordination and distillation unit to fuse multi-modal features first. then, we propose a cross-level and semantic-guided enhancement unit to enhance the fused features with cross-level information and semantic information. we arrange these two units at all stages of the rgb branch to generate features with strong representation abilities at different levels. for the decoder, to obtain large receptive fields and fine edges, we improve the lawin aspp decoder by introducing edge information extracted from the low-level features, proposing the edge-aware lawin aspp decoder. with our encoder and decoder working together, our sgfnet can identify objects accurately and segment objects finely. extensive experiments on the mfnet dataset demonstrate the superior performance of the proposed sgfnet compared with state-of-the-art methods. the code and results of our method are available at https://github.com/kw717/sgfnet.",AB_0328
"in recent years, with the rapid development of face editing and generation, more and more fake videos are circulating on social media, which has caused extreme public concerns. existing face forgery detection methods based on frequency domain find that the gan forged images have obvious grid-like visual artifacts in the frequency spectrum. but for synthesized videos, these methods only confine to a single frame and pay little attention to the most discriminative part and temporal frequency clue among different frames. to take full advantage of the rich information in video sequences, this paper performs video forgery detection on both spatial and temporal frequency domains and proposes a discrete cosine transform-based forgery clue augmentation network (fcan-dct) to achieve a more comprehensive spectrum spatial-temporal feature representation. fcan-dct totally consists of a backbone network and two branches: compact feature extraction (cfe) module and frequency temporal attention (fta) module. we conduct thorough experimental assessments on three visible light (vis) based datasets (i.e.,, faceforensics++, celeb-df (v2), wilddeepfake), and our self-built video forgery dataset deepfakenir, which is the first video forgery dataset on near-infrared (nir) modality. the experimental results demonstrate the effectiveness and robustness of our method for detecting forgery videos in both vis and nir scenarios.deepfakenir and code are available at https://github.com/aep-wyk/deepfakenir.",AB_0328
"fusion of images acquired using different sensors generates a single output with enhanced information for high-level visual perception applications. the transformer architecture has demonstrated its powerful ability to obtain important global contextual dependencies for multi-modal image fusion tasks. however, transformer-based image fusion methods face many critical issues, such as incurring huge computational burdens, limited ability to learn local features, and the difficulty of handling images of arbitrary sizes. to address the above limits, we proposed a novel laplacian pyramid hybrid (laph) network to combine the advantages of cnn and transformer architectures for multi-modal image fusion tasks. with the divide-and-conquer philosophy, we first build a light-weight cnn-based branch, performing effective extraction and fusion of texture/edge features via central difference convolutions, to process the high-resolution components with abundant details encoded in the lower pyramid levels of the laplacian pyramid. then, we design a transformer-based branch to process the low-resolution base components, learning long-range dependencies of global-contextual features without incurring extensive computational loads. here, we design a multi-scale recurrent modulation mechanism to integrate the edge/texture features from the cnn branch as guidance to progressively refine the feature extraction and fusion on low-frequency components. finally, we propose a new multi-scale spatial consistency loss term based on the neighbor contrast in source images, generating fused images with more natural and realistic appearances. extensive experiments on two different multi-modal image fusion tasks verify the superiority of our method. the source codes are made publicly available at https://github.com/rgttadv/laph.",AB_0328
"spiking neural networks (snns) have shown advantages in computation and energy efficiency over traditional artificial neural networks (anns) thanks to their event-driven representations. snns also replace weight multiplications in anns with additions, which are more energy-efficient and less computationally intensive. however, it remains a challenge to train deep snns due to the discrete spiking function. a popular approach to circumvent this challenge is ann-to-snn conversion. however, due to the quantization error and accumulating error, it often requires lots of time steps (high inference latency) to achieve high performance, which negates snn's advantages. to this end, this paper proposes fast-snn that achieves high performance with low latency. we demonstrate the equivalent mapping between temporal quantization in snns and spatial quantization in anns, based on which the minimization of the quantization error is transferred to quantized ann training. with the minimization of the quantization error, we show that the sequential error is the primary cause of the accumulating error, which is addressed by introducing a signed if neuron model and a layer-wise fine-tuning mechanism. our method achieves state-of-the-art performance and low latency on various computer vision tasks, including image classification, object detection, and semantic segmentation. codes are available at: https://github.com/yangfan-hu/fast-snn.",AB_0328
"this paper addresses the problem of shadow detection and shadow removal from a single image. despite awareness of utilizing both local and global contexts, previous works only aggregate features level by level in a coarse-to-fine manner. to overcome this problem, we present rmlanet, a novel random multi-level attention network. to be specific, we first design a shuffled multi-level feature aggregation module to fuse the multi-level features and the guiding features using the self-attention mechanism. nevertheless, the computational complexity of dense self-attention is unaffordable when processing high-resolution inputs. we argue that dense attention between any pixel pair is unnecessary due to the local consistency in images. then we further propose a sparse attention mechanism to reduce the number of attention pairs, which greatly reduces the computational complexity. through extensive experiments on four shadow detection and three shadow removal benchmark datasets, our proposed rmlanet achieves superior performance over current state-of-the-art approaches for both shadow detection and shadow removal. codes are publicly available at https://github.com/leipingjie/rmlanet.",AB_0328
"the processing and recognition of geoscience images have wide applications. most of existing researches focus on understanding the high-quality geoscience images by assuming that all the images are clear. however, in many real-world cases, the geoscience images might contain occlusions during the image acquisition. this problem actually implies the image inpainting problem in computer vision and multimedia. as far as we know, all the existing image inpainting algorithms learn to repair the occluded regions for a better visualization quality, they are excellent for natural images but not good enough for geoscience images, and they never consider the following gescience task when developing inpainting methods. this paper aims to repair the occluded regions for a better geoscience task performance and advanced visualization quality simultaneously, without changing the current deployed deep learning based geoscience models. because of the complex context of geoscience images, we propose a coarse-to-fine encoder-decoder network with the help of designed coarse-to-fine adversarial context discriminators to reconstruct the occluded image regions. due to the limited data of geoscience images, we propose a maskmix based data augmentation method, which augments inpainting masks instead of augmenting original images, to exploit the limited geoscience image data. the experimental results on three public geoscience datasets for remote sensing scene recognition, cross-view geolocation and semantic segmentation tasks respectively show the effectiveness and accuracy of the proposed method. the code is available at: https://github.com/hms97/task-driven-inpainting.",AB_0328
"combining color (rgb) images with thermal images can facilitate semantic segmentation of poorly lit urban scenes. however, for rgb-thermal (rgb-t) semantic segmentation, most existing models address cross-modal feature fusion by focusing only on exploring the samples while neglecting the connections between different samples. additionally, although the importance of boundary, binary, and semantic information is considered in the decoding process, the differences and complementarities between different morphological features are usually neglected. in this paper, we propose a novel rgb-t semantic segmentation network, called mmsmcnet, based on modal memory fusion and morphological multiscale assistance to address the aforementioned problems. for this network, in the encoding part, we used segformer for feature extraction of bimodal inputs. next, our modal memory sharing module implements staged learning and memory sharing of sample information across modal multiscales. furthermore, we constructed a decoding union unit comprising three decoding units in a layer-by-layer progression that can extract two different morphological features according to the information category and realize the complementary utilization of multiscale cross-modal fusion information. each unit contains a contour positioning module based on detail information, a skeleton positioning module with deep features as the primary input, and a morphological complementary module for mutual reinforcement of the first two types of information and construction of semantic information. based on this, we constructed a new supervision strategy, that is, a multi-unit-based complementary supervision strategy. extensive experiments using two standard datasets showed that mmsmcnet outperformed related state-of-the-art methods. the code is available at: https://github.com/2021nihao/mmsmcnet.",AB_0328
"endoscopic ultrasound (eus) has emerged as a pivotal tool for the screening and diagnosis of submucosal tumors (smts). however, the inherently low-quality and highly variable image content presents substantial obstacles to the automation of smt diagnosis. deep learning, with its adaptive feature extraction capabilities, offers a potential solution, yet its implementation often requires a vast quantity of high-quality data - a challenging prerequisite in clinical settings. to address this conundrum, this paper proposes a novel data-efficient visual analytics method that integrates human feedback into the model lifecycle, thereby augmenting the practical utility of data. the methodology leverages a two-stage deep learning algorithm, which encompasses self-supervised pre-training and an attention-based network. comprehensive experimental validation reveals that the proposed approach facilitates the model in deciphering the hierarchical structure information within high-noise eus images. moreover, when allied with human-machine interaction, it enhances data utilization, thereby elevating the accuracy and reliability of diagnostic outcomes. the code is available at https://github.com/zehebi29/la-ranet.git.",AB_0328
"meaning representation (amr) parsing aims to translate sentences to semantic amr graphs and has recently been empowered by pre-trained transformer models (e.g., bart). we argue that explicitly encoding syntactic knowledge is beneficial for amr parsing, since the amr graph of a sentence has similar substructures to those of its corresponding syntactic dependency tree. however, the effect of integrating syntactic dependency knowledge into pre-trained transformer-based amr parsers, as well as how to better infuse them, remains unclear. therefore, we conduct a systematic study to explore the utility of incorporating dependency trees into pre-trained transformers for amr parsing. specifically, we propose and compare two distinct syntax-infused amr parsers for injecting dependency structures: the syntax-aware self-attention (sasa) network, which extends the self-attention mechanism of the transformer encoder with syntax-aware constraints, and the syntax-aware graph attention (sagat) network, which augments the pre-trained transformer by encoding the syntax with a graph attention network. extensive experiments conducted on different benchmarks demonstrate that both proposed syntax-infused amr parsers achieve remarkable and explainable improvements. specifically, sasa significantly outperforms the strong baseline by up to 1.2% and 0.9% smatch scores on the amr2.0 and amr3.0 datasets, respectively. furthermore, the proposed models alleviate the performance degradation caused by long-distance dependencies (ldds), particularly in complex and long sentences. in addition, our proposed models achieve a new state-of-the-art performance level on out-of-distribution and low-resource benchmarks. the source code is available at https://github.com/hi-anonymous/syntax-aware_amr.",AB_0328
