AB,NO
"zero-shot learning is an approach where models generalize to unseen tasks without direct training on them. we introduce the unified multiple-choice (unimc) framework, which is format-independent, compatible with various formats, and applicable to tasks like text classification and sentiment analysis. furthermore, we design a two-stage tuning method, initially training on multiple-choice formats to develop format-agnostic capabilities, and subsequently enabling direct predictions on unseen tasks for zero-shot learning. our methodology avoids issues in large-scale models like flan, enhancing generalization and reducing parameters. in experiments, unimc shows state-of-the-art (sota) performance across out-of-domain and in-domain benchmarks, with only 235m parameters, far fewer than previous methods. moreover, the unimc-chinese model excels beyond human performance on benchmarks like eprstmt and chid-fc, underscoring its generalization capacity across languages. additionally, ablation experiments demonstrate the effectiveness of our design. the code and model weights are available at https://github.com/idea-ccnl/fengshenbang-lm/tree/main/fengshen/examples/unimc.",AB_0120
"convolutional neural network-based single image super-resolution (sisr) involves numerous parameters and high computational expenses to ensure improved performance, limiting its applicability in resource-constrained devices such as mobile phones. knowledge distillation (kd), which transfers useful knowledge from a teacher network to a student network, has been investigated as a method to make networks more efficient in terms of performance. to this end, feature distillation (fd) has been utilized in kd to minimize the euclidean distance-based loss of feature maps between teacher and student networks. however, this technique does not adequately consider the effective and meaningful delivery of knowledge from the teacher to the student network to improve the latter's performance under given network capacity constraints. in this study, we propose a feature-domain adaptive contrastive distillation (facd) method to train lightweight student sisr networks efficiently. we highlight the limitations of existing fd methods in terms of euclidean distance-based loss, and propose a feature-domain contrastive loss, which causes student networks to learn richer information from the teacher's representation in the feature domain. we also implement adaptive distillation that performs distillation selectively depending on the conditions of the training patches. experimental results demonstrated that the proposed facd scheme improves student enhanced deep residual networks and residual channel attention networks not only in terms of the peak signal-to-noise ratio (psnr) on all benchmark datasets and scales but also in terms of subjective image quality, compared to the conventional fd approaches. in particular, facd achieved an average psnr improvement of 0.07 db over conventional fd in both networks. code will be release at https://github.com/hcmoon0613/facd.",AB_0120
"deep learning techniques have become popular in land cover change detection (lccd) with remote sensing images (rsis). however, many existing networks mostly concentrate on learning deep features but without considering the effect of different features' attention and fusion strategy on detection performance. in this article, a novel hierarchical attention feature fusion (haff)-based network for lccd with rsis is proposed. in the proposed haff-based network, novel multiscale convolution fusion filters (mcffs) explore the global semantic feature of the interested targets from multiperspective ways. to achieve that objective, the proposed mcffs are composed by a well-known position attention module (pam) and a novel multiperspectives feature filter block (mffb) with different kernel sizes. in addition, a compound loss function was proposed for balancing the impact from the features at different levels in terms of backpropagation error. experiments conducted on six pairs of real rsis, including three pairs of homogeneous images and three pairs of heterogeneous images, confirmed the superiority of the proposed haff network over other cognate methods. moreover, the ablation experiments further confirmed the feasibility and superiority of the proposed mcffs, whereas quantitative observations indicated that competitive improvements are achieved by the proposed mcffs in terms of all the evaluation indicators. the code for the proposed approach will be available at https://github.com/imgscigroup/haff.",AB_0120
"various studies have been conducted on instance segmentation and made great strides over the past few years. most recently, instance-specific mask generation via dynamic kernel predictions has shown the significant performance improvement even without bounding boxes as well as anchors. however, this scheme still does not fully consider dynamic properties since the size of the receptive field is not enough to cover the spatially-meaningful range due to memory limitations. furthermore, the single-fused feature often fails to grasp complicated boundaries for objects of different sizes. in this article, we propose the dynamic residual filtering method with the laplacian pyramid, which separately restores the global layout and local boundaries of instance masks. specifically, we firstly apply the laplacian pyramid-based decomposition scheme to features encoded from the backbone and subsequently restore sub-band mask residuals from coarse to fine pyramid levels. to do this, we design spatially-aware convolution filters to progressively capture the residual form of mask features at each level of the laplacian pyramid while holding deformable receptive fields with dynamic offset information. this is fairly desirable since global and local properties of mask features can be accurately restored with keeping the spatial flexibility through the invertible process of the laplacian reconstruction. experimental results on the coco dataset demonstrate that our proposed method achieves the state-of-the-art performance, i.e., 42.7% ap. the code and model are publicly available at: https://github.com/tjqansthd/lapmask.",AB_0120
"several existing still image object detectors suffer from image deterioration in videos, such as motion blur, camera defocus, and partial occlusion. we present diffusionvid, a diffusion model-based video object detector that exploits spatio-temporal conditioning. inspired by the diffusion model, diffusionvid refines random noise boxes to obtain the original object boxes in a video sequence. to effectively refine the object boxes from the degraded images in the videos, we used three novel approaches: cascade refinement, dynamic coreset conditioning, and local batch refinement. the cascade refinement architecture progressively extracts information and refines boxes, whereas the dynamic coreset conditioning further improves the denoising quality using adaptive conditions based on the spatio-temporal coreset. local batch refinement significantly improves the inference speed by exploiting gpu parallelism. on the standard and widely used imagenet-vid benchmark, our diffusionvid with the resnet-101 and swin-base backbones achieves 86.9 map @ 46.6 fps and 92.4 map @ 27.0 fps, respectively, which is state-of-the-art performance. to the best of the authors' knowledge, this is the first video object detector based on a diffusion model. the code and models are available at https://github.com/sdroh1027/diffusionvid.",AB_0120
"constructing a high dynamic range (hdr) image from multi-exposure low dynamic range (ldr) images is challenging mainly due to two major problems. one is a large misalignment between the ldr images taken at different moments due to camera and object motions. the other is missing content in over- or under-exposed areas, which can be very large in the ldr images with long or short exposure times. these problems lead to ghosting artifacts or saturated regions in the reconstructed hdr image. in this paper, we propose a method using convolutional neural networks (cnns) to address these problems. specifically, each ldr image is fed to a two-branch cnn that extracts two kinds of features separately, named content features and global features. then, another cnn aligns and merges the features to create the hdr output. the content features are used to create spatial attention masks, which are used to align the features in the time domain with exposure-invariant attributes. further, fourier coefficient vectors of global features are utilized to modulate intermediate features in the frequency domain during the reconstruction process. experimental results show that our method achieves state-of-the-art hdr reconstruction performances on several benchmarks. our code is available at https://github.com/keunteklee/fdm-hdr.",AB_0120
"image scaling techniques such as super-resolution (sr) are useful for object detection, especially for detecting small objects. however, we found that scaling by an inappropriate factor tends to induce false-positive detections. this paper presents a region-dependent scale-proposal (rdsp) network that estimates the appropriate scale factors for each image region depending on its contextual information. in our detection framework, images are appropriately scaled by sr according to the estimations of the rdsp network, and fed into the scale-specific object detectors. while previous works have proposed models for scale proposal, our rdsp extracts regions where objects could potentially exist based on scene structure, regardless of whether actual objects are present, because small objects are often too small to determine their presence accurately. additionally, while existing approaches have fused object detection and sr in an end-to-end manner, scale proposals for sr are not provided or are performed independently. qualitative and quantitative experiments show that our rdsp network provides appropriate sr scales and improve detection accuracy on highly challenging dataset, captured by real car-mounted cameras with size-varied objects, including extremely small objects. our code is available at https://github.com/kakitamedia/rdsp.",AB_0120
"current methods for remote sensing image dehazing confront noteworthy computational intricacies and yield suboptimal dehazed outputs, thereby circumscribing their pragmatic applicability. to this end, we propose encoder-free multiaxis physics-aware fusion network (empf-net), a novel empf-net that exhibits both light-weighted characteristics and computational efficiency. in our pipeline, we contend that conventional u-shaped networks allocate substantial computational resources to encode haze-degraded features, which play a subordinate role in the reconstruction process. consequently, our encoder stages solely incorporate down-sampling operations. to improve the representation efficiency and enhance the generalization capabilities, we devise a multiaxis partial queried learning block (mpqlb) that primarily concentrates on learning dimension-wise queries, instead of relying solely on strictly correlated content of the input features. furthermore, we augment the reconstruction procedure by incorporating ground truth supervision into each stage via a supervised cross-scale transposed attention module (sctam). it calculates attention maps under the guidance of clean images, thereby suppressing less informative features to propagate to the subsequent level. in addition, to address the challenge of ineffective intralevel feature fusion, which result in insufficient elimination of haze-degraded information and have a negative impact on the quality of reconstructed images, we introduce a physics-aware intralevel fusion module (pifm). this module harnesses a physical inversion model to facilitate the intralevel feature interaction and alleviate the interference of dehazing-irrelevant information. our proposed empf-net is evaluated on 12 publicly available datasets, and the experimental results substantiate our superiority in terms of both metrical scores and visual quality, despite being equipped with a modest parameter count of 300k. our approach is readily accessible at https://github.com/chdwyb/empf-net.",AB_0120
"in general, 3d human-pose estimation requires high-performance computing resources. existing methods working on mobile devices trade off accuracy in return for increased efficiency, often making the estimation accuracy far from sufficient for developing serious applications. in this paper, we present a mobile 3d human-pose estimation model, achieving real-time performances with a well-designed balance between efficiency and accuracy. as the backbone, our model leverages the cutting-edge convnext architecture, renowned for its feature extraction capabilities. we enhance its performance through strategic architectural modifications and incorporation of depthwise separable convolutions in the upsampling module. the experiments made with the human3.6m dataset show that the accuracy delivered by our model is comparable to that of the state-of-the-art models, consuming significantly fewer computational resources. to showcase the practicality of our model, we present a prototype of an ar fitness application. built upon our 3d human pose estimation model, it helps trainees recreate trainers' poses from reference images. the effectiveness of the application is validated via experiments and evaluations. the source code can be found at: https://github.com/medialab-ku/convnextpose.",AB_0120
"assistive neuro-inspired rehabilitation devices are essential for people who have suffered a spinal cord injury (sci), stroke, or limb amputation in their activities of daily living. neuro-inspired rehabilitation devices typically use a single-modal biosignal with a conventional machine learning algorithm on an embedded edge device for gesture classification. although deep learning decoders provide high-accuracy gesture classification, the mismatch in the computational complexity and resource availability of edge devices has limited the deployment of real-time gesture inference on embedded devices. in this study, we describe an event-driven, edge-compatible deep neural network (dnn) capable of classifying gestures from a single or hybrid biosignal detected at the edge. the dnn-based decoders were deployed on a field-programmable gate array (fpga) to classify motor intent acquired from the biosensors for intuitive control of a 3-d-printed upper limb rehabilitation device. the study was validated with 33 subjects offline and on-device. offline average classification accuracy of 93.14% for single-modal electromyography (emg), emg-net, 50.42% for single-modal electroencephalography (eeg), eeg-net, and 93.35% for hybrid-modal biosignal (hybrid-net) using the 8-bit fixed-point quantization-aware method was obtained, while the real-time inference on the fpga resulted in 94.97%, 58.27%, and 92.73%, respectively. the emg biosensor shifted 5 cm to examine model degradation yielded 11.5% and 2.64% accuracy loss for the on-device emg-net and hybrid-net, respectively. the event-driven algorithm implemented performed with a reliability of 1, ensuring inference with voluntary gesture grasp. the study reports that hybrid biosignals outperformed single-modal eeg in gesture classification offline and on-device and single-modal emg in case of emg electrode shift. in addition, this article demonstrates an end-to-end approach that deploys a dnn decoder to an edge device for neuro-inspired control of the dexterous hand devoid of an internet-of-things (iot) connection. the data and code are available at the following repository: https://github.com/humanmachineinterface/gest-infer.",AB_0120
