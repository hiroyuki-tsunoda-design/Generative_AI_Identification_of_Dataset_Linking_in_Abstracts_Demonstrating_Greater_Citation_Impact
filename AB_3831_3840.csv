AB,NO
"night-time scene parsing (ntsp) is essential to many vision applications, especially for autonomous driving. most of the existing methods are proposed for day-time scene parsing. they rely on modeling pixel intensity-based spatial contextual cues under even illumination. hence, these methods do not perform well in night-time scenes as such spatial contextual cues are buried in the over-/under-exposed regions in night-time scenes. in this paper, we first conduct an image frequency-based statistical experiment to interpret the day-time and night-time scene discrepancies. we find that image frequency distributions differ significantly between day-time and night-time scenes, and understanding such frequency distributions is critical to ntsp problem. based on this, we propose to exploit the image frequency distributions for night-time scene parsing. first, we propose a learnable frequency encoder (lfe) to model the relationship between different frequency coefficients to measure all frequency components dynamically. second, we propose a spatial frequency fusion module (sff) that fuses both spatial and frequency information to guide the extraction of spatial context features. extensive experiments show that our method performs favorably against the state-of-the-art methods on the nightcity, nightcity+ and bdd100k-night datasets. in addition, we demonstrate that our method can be applied to existing day-time scene parsing methods and boost their performance on night-time scenes. the code is available at https://github.com/wangsen99/fdlnet.",AB_0384
"deep unfolding network (dun) that unfolds the optimization algorithm into a deep neural network has achieved great success in compressive sensing (cs) due to its good interpretability and high performance. each stage in dun corresponds to one iteration in optimization. at the test time, all the sampling images generally need to be processed by all stages, which comes at a price of computation burden and is also unnecessary for the images whose contents are easier to restore. in this paper, we focus on cs reconstruction and propose a novel dynamic path-controllable deep unfolding network (dpc-dun). dpc-dun with our designed path-controllable selector can dynamically select a rapid and appropriate route for each image and is slimmable by regulating different performance-complexity tradeoffs. extensive experiments show that our dpc-dun is highly flexible and can provide excellent performance and dynamic adjustment to get a suitable tradeoff, thus addressing the main requirements to become appealing in practice. codes are available at https://github.com/songjiechong/dpc-dun.",AB_0384
"image dehazing is a representative low-level vision task that estimates latent haze-free images from hazy images. in recent years, convolutional neural network-based methods have dominated image dehazing. however, vision transformers, which has recently made a breakthrough in high-level vision tasks, has not brought new dimensions to image dehazing. we start with the popular swin transformer and find that several of its key designs are unsuitable for image dehazing. to this end, we propose dehazeformer, which consists of various improvements, such as the modified normalization layer, activation function, and spatial information aggregation scheme. we train multiple variants of dehazeformer on various datasets to demonstrate its effectiveness. specifically, on the most frequently used sots indoor set, our small model outperforms ffa-net with only 25% #param and 5% computational cost. to the best of our knowledge, our large model is the first method with the psnr over 40 db on the sots indoor set, dramatically outperforming the previous state-of-the-art methods. we also collect a large-scale realistic remote sensing dehazing dataset for evaluating the method's capability to remove highly non-homogeneous haze. we share our code and dataset at https://github.com/idkiro/dehazeformer.",AB_0384
"recently, deep convolution neural networks (cnns) steered face super-resolution methods have achieved great progress in restoring degraded facial details by joint training with facial priors. however, these methods have some obvious limitations. on the one hand, multi-task joint learning requires additional marking on the dataset, and the introduced prior network will significantly increase the computational cost of the model. on the other hand, the limited receptive field of cnn will reduce the fidelity and naturalness of the reconstructed facial images, resulting in suboptimal reconstructed images. in this work, we propose an efficient cnn-transformer cooperation network (ctcnet) for face super-resolution tasks, which uses the multi-scale connected encoder-decoder architecture as the backbone. specifically, we first devise a novel local-global feature cooperation module (lgcm), which is composed of a facial structure attention unit (fsau) and a transformer block, to promote the consistency of local facial detail and global facial structure restoration simultaneously. then, we design an efficient feature refinement module (frm) to enhance the encoded features. finally, to further improve the restoration of fine facial details, we present a multi-scale feature fusion unit (mffu) to adaptively fuse the features from different stages in the encoder procedure. extensive evaluations on various datasets have assessed that the proposed ctcnet can outperform other state-of-the-art methods significantly. source code will be available at https://github.com/iviplab/ctcnet.",AB_0384
"recently deep learning-based image compression methods have achieved significant achievements and gradually outperformed traditional approaches including the latest standard versatile video coding (vvc) in both psnr and ms-ssim metrics. two key components of learned image compression are the entropy model of the latent representations and the encoding/decoding network architectures. various models have been proposed, such as autoregressive, softmax, logistic mixture, gaussian mixture, and laplacian. existing schemes only use one of these models. however, due to the vast diversity of images, it is not optimal to use one model for all images, even different regions within one image. in this paper, we propose a more flexible discretized gaussian-laplacian-logistic mixture model (gllmm) for the latent representations, which can adapt to different contents in different images and different regions of one image more accurately and efficiently, given the same complexity. besides, in the encoding/decoding network design part, we propose a concatenated residual blocks (crb), where multiple residual blocks are serially connected with additional shortcut connections. the crb can improve the learning ability of the network, which can further improve the compression performance. experimental results using the kodak, tecnick-100 and tecnick-40 datasets show that the proposed scheme outperforms all the leading learning-based methods and existing compression standards including vvc intra coding (4:4:4 and 4:2:0) in terms of the psnr and ms-ssim. the source code is available at https://github.com/fengyurenpingsheng.",AB_0384
"rgb-d saliency detection aims to fuse multi-modal cues to accurately localize salient regions. existing works often adopt attention modules for feature modeling, with few methods explicitly leveraging fine-grained details to merge with semantic cues. thus, despite the auxiliary depth information, it is still challenging for existing models to distinguish objects with similar appearances but at distinct camera distances. in this paper, from a new perspective, we propose a novel hierarchical depth awareness network (hidanet) for rgb-d saliency detection. our motivation comes from the observation that the multi-granularity properties of geometric priors correlate well with the neural network hierarchies. to realize multi-modal and multi-level fusion, we first use a granularity-based attention scheme to strengthen the discriminatory power of rgb and depth features separately. then we introduce a unified cross dual-attention module for multi-modal and multi-level fusion in a coarse-to-fine manner. the encoded multi-modal features are gradually aggregated into a shared decoder. further, we exploit a multi-scale loss to take full advantage of the hierarchical information. extensive experiments on challenging benchmark datasets demonstrate that our hidanet performs favorably over the state-of-the-art methods by large margins. the source code can be found in https://github.com/zongwei97/hidanet/.",AB_0384
"in this work, we propose a new deep image compression framework called complexity and bitrate adaptive network (cbanet) that aims to learn one single network to support variable bitrate coding under various computational complexity levels. in contrast to the existing state-of-the-art learning-based image compression frameworks that only consider the rate-distortion trade-off without introducing any constraint related to the computational complexity, our cbanet considers the complex rate-distortion-complexity trade-off when learning a single network to support multiple computational complexity levels and variable bitrates. since it is a non-trivial task to solve such a rate-distortion-complexity related optimization problem, we propose a two-step approach to decouple this complex optimization task into a complexity-distortion optimization sub-task and a rate-distortion optimization sub-task, and additionally propose a new network design strategy by introducing a complexity adaptive module (cam) and a bitrate adaptive module (bam) to respectively achieve the complexity-distortion and rate-distortion trade-offs. as a general approach, our network design strategy can be readily incorporated into different deep image compression methods to achieve complexity and bitrate adaptive image compression by using a single network. comprehensive experiments on two benchmark datasets demonstrate the effectiveness of our cbanet for deep image compression. code is released at https://github.com/jinyangguo/cbanet-release.",AB_0384
"in the past years, attention-based transformers have swept across the field of computer vision, starting a new stage of backbones in semantic segmentation. nevertheless, semantic segmentation under poor light conditions remains an open problem. moreover, most papers about semantic segmentation work on images produced by commodity frame-based cameras with a limited framerate, hindering their deployment to auto-driving systems that require instant perception and response at milliseconds. an event camera is a new sensor that generates event data at microseconds and can work in poor light conditions with a high dynamic range. it looks promising to leverage event cameras to enable perception where commodity cameras are incompetent, but algorithms for event data are far from mature. pioneering researchers stack event data as frames so that event-based segmentation is converted to frame-based segmentation, but characteristics of event data are not explored. noticing that event data naturally highlight moving objects, we propose a posterior attention module that adjusts the standard attention by the prior knowledge provided by event data. the posterior attention module can be readily plugged into many segmentation backbones. plugging the posterior attention module into a recently proposed segformer network, we get evsegformer (the event-based version of segformer) with state-of-the-art performance in two datasets (mvsec and ddd-17) collected for event-based segmentation. code is available at https://github.com/zexijia/evsegformer to facilitate research on event-based vision.",AB_0384
"beyond high accuracy, good interpretability is very critical to deploy a face forgery detection model for visual content analysis. in this paper, we propose learning patch-channel correspondence to facilitate interpretable face forgery detection. patch-channel correspondence aims to transform the latent features of a facial image into multi-channel interpretable features where each channel mainly encoders a corresponding facial patch. towards this end, our approach embeds a feature reorganization layer into a deep neural network and simultaneously optimizes classification task and correspondence task via alternate optimization. the correspondence task accepts multiple zero-padding facial patch images and represents them into channel-aware interpretable representations. the task is solved by step-wisely learning channel-wise decorrelation and patch-channel alignment. channel-wise decorrelation decouples latent features for class-specific discriminative channels to reduce feature complexity and channel correlation, while patch-channel alignment then models the pairwise correspondence between feature channels and facial patches. in this way, the learned model can automatically discover corresponding salient features associated to potential forgery regions during inference, providing discriminative localization of visualized evidences for face forgery detection while maintaining high detection accuracy. extensive experiments on popular benchmarks clearly demonstrate the effectiveness of the proposed approach in interpreting face forgery detection without sacrificing accuracy. the source code is available at https://github.com/jae35/iffd",AB_0384
"with the development of video network, image set classification (isc) has received a lot of attention and can be used for various practical applications, such as video based recognition, action recognition, and so on. although the existing isc methods have obtained promising performance, they often have extreme high complexity. due to the superiority in storage space and complexity cost, learning to hash becomes a powerful solution scheme. however, existing hashing methods often ignore complex structural information and hierarchical semantics of the original features. they usually adopt a single-layer hashing strategy to transform high-dimensional data into short-length binary codes in one step. this sudden drop of dimension could result in the loss of advantageous discriminative information. in addition, they do not take full advantage of intrinsic semantic knowledge from whole gallery sets. to tackle these problems, in this paper, we propose a novel hierarchical hashing learning (hhl) for isc. specifically, a coarse-to-fine hierarchical hashing scheme is proposed that utilizes a two-layer hash function to gradually refine the beneficial discriminative information in a layer-wise fashion. besides, to alleviate the effects of redundant and corrupted features, we impose the $\ell _{2,1}$ norm on the layer-wise hash function. moreover, we adopt a bidirectional semantic representation with the orthogonal constraint to keep intrinsic semantic information of all samples in whole image sets adequately. comprehensive experiments demonstrate hhl acquires significant improvements in accuracy and running time. we will release the demo code on https://github.com/sunyuan-cs.",AB_0384
