AB,NO
"representation learning provides an attractive opportunity to model the evolution of dynamic networks. however, the existing methods have two limitations: (1) most graph neural network-based methods fail to utilize the high-order proximity of nodes that captures the important properties of a network topology; (2) evolutionary dynamics-based methods are much fine-grained in modeling time information but neglect the coherence of dynamic networks, which leads to the model being susceptible to subtle noise. in this paper, we propose an enhanced hypergraph neural network framework for dynamic network embedding (hyperdne) to tackle these issues. specifically, we innovatively design a sequential hypergraph with dual-stream output to explore the group properties of nodes and edges, and a line graph neural network is added as an auxiliary enhancement scheme to further aggregate social influence from the degree of social convergence. then, we compute the final embedding through attentions along the node and hyperedge levels to fuse multi-level variations in the network structure. the experimental results on six real networks demonstrate significant gains for hyperdne over several state-of-the-art network embedding baselines. the dataset and source code of hyperdne are publicly available at https:// github.com/qhgz2013/hyperdne.",AB_0376
"it is meaningful but challenging to teach machines to recognize handwritten chinese characters. however, conventional approaches typically view handwritten chinese characters as either static images or tempo-ral trajectories, which may ignore the inherent geometric semantics of characters. instead, here we first propose to represent handwritten characters as skeleton graphs, explicitly considering the natural charac-teristics of characters (i.e., characters as graphs). furthermore, we propose a novel pyramid graph trans-former (pygt) to specifically process the graph-structured characters, which fully integrates the advan-tages of transformers and graph convolutional networks. specifically, our pygt can learn better graph fea-tures through (i) capturing the global information from all nodes with graph attention mechanism and (ii) modelling the explicit local adjacency structures of nodes with graph convolutions. furthermore, the pygt learns the multi-resolution features by constructing a progressive shrinking pyramid. compared with ex-isting approaches, it is more interpretable to recognize characters as geometric graphs. moreover, the pro-posed method is generic for both online and offline handwritten chinese character recognition (hccr), and it also can be feasibly extended to handwritten text recognition. extensive experiments empirically demonstrate the superiority of pygt over the prevalent approaches including 2d-cnn, rnn/1d-cnn, and vision transformer (vit) for hccr. the code is available at https://github.com/ganji15/pygt-hccr .& copy; 2023 elsevier ltd. all rights reserved.",AB_0376
"deep models trained on long-tailed datasets exhibit unsatisfactory performance on tail classes. existing methods usually modify the classification loss to increase the learning focus on tail classes, which unex-pectedly sacrifice the performance on head classes. in fact, this scheme leads to a contradiction between the two goals of long-tailed learning, i.e., learning generalizable representations and facilitating learning for tail classes. in this work, we explore knowledge distillation in long-tailed scenarios and propose a novel distillation framework, named balanced knowledge distillation (bkd), to disentangle the contradic-tion between the two goals and achieve both simultaneously. specifically, given a teacher model, we train the student model by minimizing the combination of an instance-balanced classification loss and a class-balanced distillation loss. the former benefits from the sample diversity and learns generalizable repre-sentation, while the latter considers the class priors and facilitates learning for tail classes. we conduct extensive experiments on several long-tailed benchmark datasets and demonstrate that the proposed bkd is an effective knowledge distillation framework in long-tailed scenarios, as well as a competitive method for long-tailed learning. our source code is available: https://github.com/ericzsy/ balancedknowledgedistillation.& copy; 2023 elsevier b.v. all rights reserved.",AB_0376
"the stochastic momentum method is a commonly used acceleration technique for solving large-scale stochastic optimization problems. current convergence results of stochastic momentum methods under non-convex stochastic settings mostly discuss convergence in terms of the random output and minimum output, which requires temporal and spatial statistics of historical data. on the other hand, the last-iterate convergence allows us to avoid storing or selecting past output iterates after each iteration, while maintaining rigour in convergence analysis. to this end, we address the convergence of the last iterate output (called last-iterate convergence) of the stochastic momentum methods for non-convex stochastic optimization problems, in a way which is conformal with traditional optimization theory. for generality, we prove the last-iterate convergence of the stochastic momentum methods under a unified framework, covering both stochastic heavy ball momentum and stochastic nesterov accelerated gradient momen-tum, whose momentum factors can be either constant or time-varying coefficients. finally, the last-iterate convergence of the stochastic momentum methods is verified on the benchmark mnist and cifar-10 datasets. the implementation of sum is available at: https://github.com/xudp100/sumhttps://github.com/xudp100/sum. ?? 2023 elsevier b.v. all rights reserved.",AB_0376
"in high-dynamic-range (hdr) image reconstruction, the background offset among multiple multi-exposure low-dynamic-range (ldr) images, wide-range movement of targets, and missing edge structure information in the over-/under-exposure region cause both ghosting and blurring artifacts. this study proposed a structure-embedded ghosting artifact suppression network (sgarn) to achieve detailed preservation and ghosting artifact suppression to address this issue. according to the different image feature maps' correlation in channels, a channel and multi-head joint attention network (cman) was designed to highlight the features conducive to high-quality hdr image reconstruction. a dense multi-scale information transfer network (dmitn) was designed to integrate the characteristics of different combinations of convolution kernels with different receptive fields. in addition, a structure-embedded network was designed to predict the edge structure to be compensated from the reference image. the predicted edge was integrated into the reconstructed hdr image. compared with state-of-the-art methods, the proposed method can achieve better visual performance and higher objective evaluation results on three public datasets. the source codes of the proposed method are available at https://github.com/lhf12278/sgarn.(c) 2023 elsevier b.v. all rights reserved.",AB_0376
"automatic 12-lead ecg classification is always in the spotlight and has achieved great progress owing to the application of deep learning. after reviewing previous works, we find that most multi-label ecg models can be uniformly characterized as a deep embedding method. this method employs only one embedding space to produce high-dimensional features for all diagnostic labels, which results in a label entanglement phenomenon and produces at least three defects. the most serious one is that as the number of labels goes up, the complexity of clustering in the penultimate layer grows exponentially. motivated by these flaws, we provide an intuitive insight and propose a label decoupling module. it can solve these defects by launching the samples of various labels into multiple spaces. then, we make a trainable mergence that combines the benefits of label decoupling and traditional label fusion to get the final prediction. in addition, we also introduce some metric learning techniques and further develop its large-margin version. it is important to note that our method is universal and can be applied with many state-of-the-art (sota) backbones for better performance in ecg classification. experiments on benchmark datasets demonstrate that our approach strengthens all tested backbones and achieves better performance than various sota techniques in this field. an implementation of the main codes is available at https://github.com/zhangshuojackpot/label-decoupling-module.(c) 2023 elsevier b.v. all rights reserved.",AB_0376
"although there is a long line of research on bidirectional image-text matching, the problem remains a challenge due to the well-known semantic gap between visual and textual modalities. popular solutions usually first detect the objects and then find the association between visual objects and the textual words to estimate the relevance; however, these methods only focus on the visual object features while ignoring the semantic attributions of the detected regions, which is an important clue in terms of bridging the semantic gap. to remedy this issue, we propose a generative multiattribution tag fusion method that further includes region attribution to alleviate the semantic gap. in particular, our method comprises three steps: the extraction of image features, the extraction of text features, and the matching of image and text by an attention mechanism. we first divide the image into blocks to obtain the region image features and region attribute labels. then, we fuse them to reduce the semantic gap between the image features and text features. second, bert and bi-gru are used to extract text features, and third, we use the attention mechanism to match each area in the image with each word in the text with the same meaning. the quantitative and qualitative results on the public datasets flickr30k and ms-coco demonstrate the effectiveness of our method. the source code is released on github https://github.com/smileslabsh/generative-label-fused-network.(c) 2023 elsevier b.v. all rights reserved.",AB_0376
"we tackle the problem of generalizing a predictor trained on a set of source domains to an unseen target domain, where the source and target domains are different but related to one another, i.e., the domain generalization problem. prior adversarial methods rely on solving the minimax problems to align in the neural network embedding space the components of the domains (i.e., a set of marginal distributions, a set of marginal distributions and multiple sets of class-conditional distributions). however, these methods introduce additional parameters (for each set of distributions) to the network predictor and are difficult to train. in this work, we propose to directly align the domains themselves via solving a minimax problem that can be decomposed and converted into a min one. particularly, we analytically solve the max problem with respect to (w.r.t.) the domain discriminators, and convert the minimax problem into a min one w.r.t. the embedding function. this is more advantageous since in the end our approach introduces no additional network parameters and simplifies the training procedure. we evaluate our approach on several multi-domain datasets and testify its superiority over the relevant methods. the source code is available at https://github.com/sentaochen/decomposed-adversarial-domain-generalization.(c) 2023 the author(s). published by elsevier b.v. this is an open access article under the cc by-nc-nd license ().",AB_0376
"the success of vision transformer (vit) in various computer vision tasks has promoted the ever-increasing prevalence of this convolution-free network. the fact that vit works on image patches makes it potentially relevant to the problem of jigsaw puzzle solving, which is a classical self-supervised task aiming at reordering shuffled sequential image patches back to their original form. solving jigsaw puzzle has been demonstrated to be helpful for diverse tasks using convolutional neural networks (cnns), such as feature representation learning, domain generalization and fine-grained classification. in this paper, we explore solving jigsaw puzzle as a self-supervised auxiliary loss in vit for image classification, named jigsaw-vit. we show two modifications that can make jigsaw-vit superior to standard vit: discarding positional embeddings and masking patches randomly. yet simple, we find that the proposed jigsaw-vit is able to improve on both generalization and robustness over the standard vit, which is usually rather a trade-off. numerical experiments verify that adding the jigsaw puzzle branch provides better generaliza-tion to vit on large-scale image classification on imagenet. moreover, such auxiliary loss also improves robustness against noisy labels on animal-10n, food-101n, and clothing1m, as well as adversarial exam-ples. our implementation is available at https://yingyichen-cyy.github.io/jigsaw- vit .(c) 2023 the authors. published by elsevier b.v. this is an open access article under the cc by-nc-nd license (  )",AB_0376
"in computer vision, some attribution methods for explaining cnns attempt to study how the intermediate features affect network prediction. however, they usually ignore the feature hierarchies among the intermediate features. this paper introduces a hierarchical decomposition framework to explain cnn's decision-making process in a top-down manner. specifically, we propose a gradient-based activation propagation (gap) module that can decompose any intermediate cnn decision to its lower layers and find the supporting features. then we utilize the gap module to iteratively decompose the network decision to the supporting evidence from different cnn layers. the proposed framework can generate a deep hierarchy of strongly associated supporting evidence for the network decision, which provides insight into the decision-making process. moreover, gap is effort-free for understanding cnn-based models without network architecture modification and extra training processes. experiments show the effectiveness of the proposed method. the data and source code will be publicly available at https://mmcheng.net/hdecomp/.",AB_0376
