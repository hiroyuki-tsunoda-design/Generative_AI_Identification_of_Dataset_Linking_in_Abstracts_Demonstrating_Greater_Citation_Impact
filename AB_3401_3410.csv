AB,NO
"most existing prevalent trackers draw support from deep learning networks to acquire more effective target features, while we found that the dominant discriminative trackers employ a plain convolution block to turn backbone features into target classification features suitable for tracking. in this paper, we propose a lightweight feature separation and fusion module to obtain more effective and efficient semantic features in an end-to-end manner. moreover, discriminative trackers entail collecting a certain number of new samples to online optimize the target classifier for retaining its discriminative ability, but it is not easy to pick out a more reliable sample for storage. therefore, a feasible target uncertain detection technique is devised to alleviate the tracking model corruption problem. in order to demonstrate the strong effectiveness and compatibility of our proposed approaches, we choose the excellent superdimp and tomp as baseline methods and conduct comprehensive experimental evaluations on seven public benchmarks. the results reveal that our methods perform superiorly against several state-of-the-art trackers on challenging benchmarks. the code and trained models will be available at https://github.com/hexdjx/vistrack.(c) 2023 elsevier b.v. all rights reserved.",AB_0341
"in recent years, methods based on discrete diffusion models have achieved state-of-the-art performances in voice generation [1,2]. in theory, the voice data can be transformed into the exact gaussian prior distributions only when the diffusion time tends to infinity. but in real applications, the gaussian prior distribution can only be achieved approximately in a limited time duration run by these diffusion-based methods, thus resulting in sub-optimal sound quality. in this paper, we present the schrowave to realize the continuous transformation from exact dirac's deltas to the target voice data distribution in finite time duration, conditioned on middle voice representation with different sizes. at the same time, in order to overcome the difficulty in calculating the score on the low-dimensional manifold of voice data during the generation process, we propose to use a two-stage diffusion and generation method, while each stage implemented by solving a conditional schrodinger bridge problem. our experiments on the public data set ljspeech show that the effect is significant in both objective and subjective evaluation, and achieve the new state-of-the-art mos of 4.53. audio samples are available at https://schrowave .github .io. & copy; 2023 elsevier inc. all rights reserved.",AB_0341
"we propose a fast stereo matching network based on temporal attention and 2d convolution (tanet). due to the high similarity of the disparity between consecutive frames in an image sequence, we propose a temporal attention (ta) module that uses the disparity map of the previous frame to guide the disparity search range in the current frame, thus significantly improving the efficiency of disparity calculation in the cost volume module. additionally, we propose a hierarchical cost construction and 2d convolution aggregation module that constructs a pyramid cost volume by fusing edge cues to establish detail constraints. this overcomes the problem of difficult convergence caused by information loss when replacing 3d convolution with 2d convolution. experimental results show that the ta module effectively optimizes the cost volume and, together with 2d convolution, improves the computational speed. compared with state-of-the-art algorithms, tanet achieves a speedup of nearly 4x, with a running time of 0.061s, and reduces the parameter count by nearly half while decreasing accuracy by 1.1%. code is available at https://github.com/y0uchenz/tanet.",AB_0341
"spiking neural networks are efficient computation models for low-power environments. spike-based bp algorithms and ann-to-snn (ann2snn) conversions are successful techniques for snn training. nevertheless, the spike-base bp training is slow and requires large memory costs, while ann2snn needs many inference steps to obtain good performance. in this paper, we propose an activation consistency coupled ann-snn (ac2as) framework to train the snn in a fast and memory-efficient way. the ac2as consists of two components: (a) a weight-shared architecture between ann and snn and (b) spiking mapping units. firstly, the architecture trains the weight-shared parameters on the ann branch, resulting in fast training and low memory costs for snn. secondly, the spiking mapping units are designed to ensure that the activation values of the ann are the spiking features. as a result, the activation consistency is guaranteed, and the classification error of the snn can be optimized by training the ann branch. besides, we design an adaptive threshold adjustment (ata) algorithm to decrease the firing of noisy spikes. experiment results show that our ac2as-based models perform well on the benchmark datasets (cifar10, cifar100, and tiny-imagenet). moreover, the ac2as achieves comparable accuracy under 0.625x time steps, 0.377x training time, 0.27x gpu memory costs, and 0.33x spike activities of the spike-based bp model. the code is available at https://github.com/tjxtt/ac2asnn.",AB_0341
"data augmentation has been an essential technique for improving the generalization ability of deep neural networks in image classification tasks. however, intensive changes in appearance and different degrees of occlusion in images are the key factors that severely affect the generalization ability of image classification models. therefore, in order to enhance the generalization performance and robustness of deep models, data augmentation approaches by providing models with more diverse training data in various scenarios are widely applied. although many existing data augmentation methods simulate occlusion in the augmented images to enhance the generalization of models, these methods randomly delete some areas in images without considering the semantic information of images. in this work, we propose a novel data augmentation method named advmask for image classification based on sparse adversarial attack techniques. advmask first identifies the key points that have the greatest influence on the classification results via a proposed end-to-end sparse adversarial attack module. during the data augmentation process, advmask efficiently generates diverse augmented data with structured occlusions based on the key points. by doing so, advmask can force deep models to seek other relevant content while the most discriminative content is hidden. extensive experimental results on various benchmark datasets and deep models demonstrate that our proposed method can effectively improve the generalization performance of deep models and significantly outperforms previous data augmentation methods. code for reproducing our results is available at https://github.com/jackbrocp/advmask.",AB_0341
"both semi-supervised learning and transfer learning aim at lowering the annotation burden for training models. however, such two tasks are usually studied separately, i.e. most semi-supervised learning algorithms train models from scratch while transfer learning assumes pre-trained models as the initialization. in this work, we focus on a previously-less-concerned setting that further reduces the annotation efforts through incorporating both semi-supervised and transfer learning, where specifically a pre-trained source model is used as the initialization of semi-supervised learning. as those powerful pre-trained models are ubiquitously available nowadays and can considerably benefit various down-streaming tasks, such a setting is relevant to real-world applications yet challenging to design effective algorithms.aiming at enabling transfer learning under semi-supervised settings, we propose a hierarchical self regularization mechanism to exploit unlabeled samples more effectively, where a novel self-regularizer has been introduced to incorporate both individual-level and population-level regularization terms. the former term employs self-distillation to regularize learned deep features for each individual sample, and the latter one enforces self-consistency on feature distributions between labeled and unlabeled samples. samples involved in both regularizers are weighted by an adaptive strategy, where self-regularization effects of both terms are adaptively controlled by the confidence of every sample. to validate our algorithm, exhaustive experiments have been conducted on diverse datasets such as cifar-10 for general object recognition, cub200-2011/mit-indoor-67 for fine-grained classification and mura for medical image classification. compared with state-of-the-art semi-supervised learning methods including pseudo label, mean teacher, mixmatch and fixmatch, our algorithm demonstrates two advantages: first of all, the proposed approach adopts a new point of view to tackle problems caused by inadequate supervision and achieves very competitive results; then, it is complementary to these state-of-the-art methods and thus can be combined with them to get additional improvements. furthermore, our method can also be applied to fully supervised transfer learning and self supervised learning. we have published our code at https://github.com/shi-labs/semi-supervised-transferlearning.",AB_0341
"rgb-t salient object detection aims to identify the most attractive object(s) in a scene using rgb and thermal data. for this task, on the one hand, how to excavate salient clues is crucial to improve the detection performance of the model. on the other hand, strengthening the representation of salient features is still a huge challenge. in order to solve the above issues, this paper proposes excavating and enhancing cnn features to boost the performance of rgb-t salient object detection (e(2)net). specifically, we first design a new joint attention module (jam) that jointly considers both dimensions of channel and pixel location to fully explore effective salient features by encoding contextual information as local features and simultaneously digging for useful clues from inside the channel. different from previous works of expanding the feeling field, to enhance the feature representation, we propose a feature enhancement module (fem), which realizes the parallel and independent learning of the four branches based on the channel splitting strategy, and greatly consolidates the feature expression ability. extensive experiments show that our proposed model outperforms the existing twenty-two excellent models on three challenging benchmark datasets. codes and results are available at: https://github.com/ranwanwu/e2net.",AB_0341
"video object segmentation automatically separates the interested objects from the background across a video sequence and was an active research area in recent years. the crucial challenge lies in investigating an effective architecture to fully exploit spatiotemporal correlation in a given video sequence for achieving accurate segmentation results. in this paper, we propose a novel semi-supervised transformer-based framework called target-guided spatiotemporal dual-stream transformers (tsdt) with two separate streams to enable effective spatiotemporal context propagation. technically, the temporal stream is used to aggregate rich temporal cues from past frames, while the spatial stream is trained to encode object location and appearance information stored in the current frame. to compress and integrate temporal features, a target guidance block (tgb) is designed to retrieve target information in the past video flow under the guidance of the current frame. the experimental results on video object segmentation benchmarks demonstrate the feasibility and effectiveness of the proposed framework. codes and trained models are available at https://github.com/zhouweii234/tsdtvos.",AB_0341
"stochastic gradient descent (sgd), one of the most popular neural network optimization algorithms, has a solid theoretical foundation as well as good generalization performance. however vanilla sgd performs catastrophically in binary neural networks (bnns). many studies have identified this phenomenon without explaining its causes in depth. in this paper, we try to experimentally understand the possible reasons for this and significantly improve the performance of vanilla sgd in bnns training by adjusting the training strategy to be comparable to adam. we subsequently propose a new optimization method for training deep neural networks (dnns) with binary weights. in the proposed sgd with adaptive threshold, referred to as sgdat, we suppress the frequency of weights flipping by thresholds and adjust the threshold of each parameter according to the number of flipping to further reduce the network noise, stabilize the network training, and improve the network generalization ability. also, we present a complete ablation study of the hyperparameters space, as well as experimentally analyze the impact of using adaptive thresholds. furthermore, we conduct image classification experiments over the cifar10, cifar100 and tinyimagenet datasets using binarynet and resnet-18 network structure. the experiments show that sgdat outperforms other binary optimizers. code is available at: https://github.com/ gushan/sgdat.",AB_0341
"scene understanding in a production workshop is an important technology to improve its intelligence level, semantic segmentation of production workshop objects is an effective method for realizing scene understanding. since the varieties of information of production workshop, making full use of the complementary information of rgb image and depth image can effectively improve the semantic segmentation accuracy of production workshop objects. aiming at solving the multi-scale and real-time problems of segmenting the production workshop objects, this paper proposes cross-modal transformer (cmformer), a transformer-based cross-modal semantic segmentation model. its key feature correction and feature fusion parts are composed of the multi-scale channel attention correction(ms-cac) module and the global feature aggregation(gfa) module. by improving multi head self-attention(mhsa) in transformer, we design cross-modal multi-head self-attention(cm-mhsa) to build long-range interaction between rgb image and depth image, and further design the ms-cac module and the gfa module on the basis of the cm-mhsa module, to achieve cross-modal information interaction in the channel and spatial dimensions. among them, the ms-cac module enriches the multi-scale features of each channel and achieve more accurate channel attention correction between the two modals; the gfa module interacts with rgb feature and depth feature in the spatial dimension and fuses global and local features at the same time. in the experiments on the nyu depth v2 dataset, the cmformer reached 68.00% mpa(mean pixel accuracy) and 55.75% miou(mean intersection over union), achieves the state-of-the-art results. while in the experiments on the scene objects for production workshop dataset(sop), the cmformer achieves 96.74% mpa, 92.98% miou and 43 fps(frames per second), which has high precision and good real-time performance. code is available at: https://github.com/futureiai/cmformer",AB_0341
