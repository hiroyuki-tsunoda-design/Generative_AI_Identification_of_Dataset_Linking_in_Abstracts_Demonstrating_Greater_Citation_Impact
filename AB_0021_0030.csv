AB,NO
"model-free reinforcement learning algorithms based on entropy regularized have achieved good performance in control tasks. those algorithms consider using the entropy-regularized term for the policy to learn a stochastic policy. this work provides a new perspective that aims to explicitly learn a representation of intrinsic information in state transition to obtain a multimodal stochastic policy, for dealing with the tradeoff between exploration and exploitation. we study a class of markov decision processes (mdps) with divergence maximization, called divergence mdps. the goal of the divergence mdps is to find an optimal stochastic policy that maximizes the sum of both the expected discounted total rewards and a divergence term, where the divergence function learns the implicit information of state transition. thus, it can provide better-off stochastic policies to improve both in robustness and performance in a high-dimension continuous setting. under this framework, the optimality equations can be obtained, and then a divergence actor-critic algorithm is developed based on the divergence policy iteration method to address large-scale continuous problems. the experimental results, compared to other methods, show that our approach achieved better performance and robustness in the complex environment particularly. the code of divac can be found in https://github.com/yzyvl/divac.",AB_0003
"deep neural networks often suffer from poor performance or even training failure due to the ill-conditioned problem, the vanishing/exploding gradient problem, and the saddle point problem. in this article, a novel method by acting the gradient activation function (gaf) on the gradient is proposed to handle these challenges. intuitively, the gaf enlarges the tiny gradients and restricts the large gradient. theoretically, this article gives conditions that the gaf needs to meet and, on this basis, proves that the gaf alleviates the problems mentioned above. in addition, this article proves that the convergence rate of sgd with the gaf is faster than that without the gaf under some assumptions. furthermore, experiments on cifar, imagenet, and pascal visual object classes confirm the gaf's effectiveness. the experimental results also demonstrate that the proposed method is able to be adopted in various deep neural networks to improve their performance. the source code is publicly available at https://github.com/longjin-lab/activated-gradients-for-deep-neural-networks.",AB_0003
"multiagent reinforcement learning methods, such as vdn, qmix, and qtran, that adopt centralized training with decentralized execution (ctde) framework have shown promising results in cooperation and competition. however, in some multiagent scenarios, the number of agents and the size of the action set actually vary over time. we call these unshaped scenarios, and the methods mentioned above fail in performing satisfyingly. in this article, we propose a new method, called unshaped networks for multiagent systems (unmas), which adapts to the number and size changes in multiagent systems. we propose the self-weighting mixing network to factorize the joint action-value. its adaption to the change in agent number is attributed to the nonlinear mapping from each-agent q value to the joint action-value with individual weights. besides, in order to address the change in an action set, each agent constructs an individual action-value network that is composed of two streams to evaluate the constant environment-oriented subset and the varying unit-oriented subset. we evaluate unmas on various starcraft ii micromanagement scenarios and compare the results with several state-of-the-art marl algorithms. the superiority of unmas is demonstrated by its highest winning rates especially on the most difficult scenario 3s5z_vs_3s6z. the agents learn to perform effectively cooperative behaviors, while other marl algorithms fail. animated demonstrations and source code are provided in https://sites.google.com/view/unmas.",AB_0003
"long-term visual place recognition (vpr) is challenging as the environment is subject to drastic appearance changes across different temporal resolutions, such as time of the day, month, and season. a wide variety of existing methods address the problem by means of feature disentangling or image style transfer but ignore the structural information that often remains stable even under environmental condition changes. to overcome this limitation, this article presents a novel structure-aware feature disentanglement network (sfdnet) based on knowledge transfer and adversarial learning. explicitly, probabilistic knowledge transfer (pkt) is employed to transfer knowledge obtained from the canny edge detector to the structure encoder. an appearance teacher module is then designed to ensure that the learning of appearance encoder does not only rely on metric learning. the generated content features with structural information are used to measure the similarity of images. we finally evaluate the proposed approach and compare it to state-of-the-art place recognition methods using six datasets with extreme environmental changes. experimental results demonstrate the effectiveness and improvements achieved using the proposed framework. source code and some trained models will be available at http://www.tianshu.org.cn.",AB_0003
"this article presents a general sampling method, called granular-ball sampling (gbs), for classification problems by introducing the idea of granular computing. the gbs method uses some adaptively generated hyperballs to cover the data space, and the points on the hyperballs constitute the sampled data. gbs is the first sampling method that not only reduces the data size but also improves the data quality in noisy label classification. in addition, because the gbs method can be used to exactly describe the boundary, it can obtain almost the same classification accuracy as the results on the original datasets, and it can obtain an obviously higher classification accuracy than random sampling. therefore, for the data reduction classification task, gbs is a general method that is not especially restricted by any specific classifier or dataset. moreover, the gbs can be effectively used as an undersampling method for imbalanced classification. it has a time complexity that is close to o(n), so it can accelerate most classifiers. these advantages make gbs powerful for improving the performance of classifiers. all codes have been released in the open source gbs library at http://www.cquptshuyinxia.com/gbs.html.",AB_0003
"most existing light field saliency detection methods have achieved great success by exploiting unique light field data-focus information in focal slices. however, they process light field data in a slicewise way, leading to suboptimal results because the relative contribution of different regions in focal slices is ignored. how we can comprehensively explore and integrate focused saliency regions that would positively contribute to accurate saliency detection. answering this question inspires us to develop a new insight. in this article, we propose a patch-aware network to explore light field data in a regionwise way. first, we excavate focused salient regions with a proposed multisource learning module (mslm), which generates a filtering strategy for integration followed by three guidances based on saliency, boundary, and position. second, we design a sharpness recognition module (srm) to refine and update this strategy and perform feature integration. with our proposed mslm and srm, we can obtain more accurate and complete saliency maps. comprehensive experiments on three benchmark datasets prove that our proposed method achieves competitive performance over 2-d, 3-d, and 4-d salient object detection methods. the code and results of our method are available at https://github.com/oiplab-dut/ieee-tcyb-panet.",AB_0003
"despite the remarkable successes of convolutional neural networks (cnns) in computer vision, it is time-consuming and error-prone to manually design a cnn. among various neural architecture search (nas) methods that are motivated to automate designs of high-performance cnns, the differentiable nas and population-based nas are attracting increasing interests due to their unique characters. to benefit from the merits while overcoming the deficiencies of both, this work proposes a novel nas method, relativenas. as the key to efficient search, relativenas performs joint learning between fast learners (i.e., decoded networks with relatively lower loss value) and slow learners in a pairwise manner. moreover, since relativenas only requires low-fidelity performance estimation to distinguish each pair of fast learner and slow learner, it saves certain computation costs for training the candidate architectures. the proposed relativenas brings several unique advantages: 1) it achieves state-of-the-art performances on imagenet with top-1 error rate of 24.88%, that is, outperforming darts and amoebanet-b by 1.82% and 1.12%, respectively; 2) it spends only 9 h with a single 1080ti gpu to obtain the discovered cells, that is, 3.75x and 7875x faster than darts and amoebanet, respectively; and 3) it provides that the discovered cells obtained on cifar-10 can be directly transferred to object detection, semantic segmentation, and keypoint detection, yielding competitive results of 73.1% map on pascal voc, 78.7% miou on cityscapes, and 68.5% ap on mscoco, respectively. the implementation of relativenas is available at https://github.com/emi-group/relativenas.",AB_0003
"existing domain adaptation approaches often try to reduce distribution difference between source and target domains and respect domain-specific discriminative structures by some distribution [e.g., maximum mean discrepancy (mmd)] and discriminative distances (e.g., intra-class and inter-class distances). however, they usually consider these losses together and trade off their relative importance by estimating parameters empirically. it is still under insufficient exploration so far to deeply study their relationships to each other so that we cannot manipulate them correctly and the model's performance degrades. to this end, this article theoretically proves two essential facts: 1) minimizing mmd equals to jointly minimizing their data variance with some implicit weights but, respectively, maximizing the source and target intra-class distances so that feature discriminability degrades and 2) the relationship between intra-class and inter-class distances is as one falls and another rises. based on this, we propose a novel discriminative mmd with two parallel strategies to correctly restrain the degradation of feature discriminability or the expansion of intra-class distance; specifically: 1) we directly impose a tradeoff parameter on the intra-class distance that is implicit in the mmd according to 1) and 2) we reformulate the inter-class distance with special weights that are analogical to those implicit ones in the mmd and maximizing it can also lead to the intra-class distance falling according to 2). notably, we do not consider the two strategies in one model due to 2). the experiments on several benchmark datasets not only prove the validity of our revealed theoretical results but also demonstrate that the proposed approach could perform better than some compared state-of-art methods substantially. our preliminary matlab code will be available at https://github.com/wwlovetransfer/.",AB_0003
"in transfer learning model, the source domain samples and target domain samples usually share the same class labels but have different distributions. in general, the existing transfer learning algorithms ignore the interclass differences and intraclass similarities across domains. to address these problems, this article proposes a transfer learning algorithm based on discriminative fisher embedding and adaptive maximum mean discrepancy (ammd) constraints, called discriminative fisher embedding dictionary transfer learning (dfedtl). first, combining the label information of source domain and part of target domain, we construct the discriminative fisher embedding model to preserve the interclass differences and intraclass similarities of training samples in transfer learning. second, an ammd model is constructed using atoms and profiles, which can adaptively minimize the distribution differences between source domain and target domain. the proposed method has three advantages: 1) using the fisher criterion, we construct the discriminative fisher embedding model between source domain samples and target domain samples, which encourages the samples from the same class to have similar coding coefficients; 2) instead of using the training samples to design the maximum mean discrepancy (mmd), we construct the ammd model based on the relationship between the dictionary atoms and profiles; thus, the source domain samples can be adaptive to the target domain samples; and 3) the dictionary learning is based on the combination of source and target samples which can avoid the classification error caused by the difference among samples and reduce the tedious and expensive data annotation. a large number of experiments on five public image classification datasets show that the proposed method obtains better classification performance than some state-of-the-art dictionary and transfer learning methods. the code has been available at https://github.com/shilinrui/dfedtl.",AB_0003
"this paper proposes a novel architecture, termed multiscale principle of relevant information (mpri), to learn discriminative spectral-spatial features for hyperspectral image classification. mpri inherits the merits of the principle of relevant information (pri) to effectively extract multiscale information embedded in the given data, and also takes advantage of the multilayer structure to learn representations in a coarse-to-fine manner. specifically, mpri performs spectral-spatial pixel characterization (using pri) and feature dimensionality reduction (using regularized linear discriminant analysis) iteratively and successively. extensive experiments on three benchmark data sets demonstrate that mpri outperforms existing state-of-the-art methods (including deep learning based ones) qualitatively and quantitatively, especially in the scenario of limited training samples. code of mpri is available at http://bit.ly/mpri_hsi.",AB_0003
