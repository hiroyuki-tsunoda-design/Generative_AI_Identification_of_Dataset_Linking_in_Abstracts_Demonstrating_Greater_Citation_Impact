AB,NO
"contextual information is vital in visual understanding problems, such as semantic segmentation and object detection. we propose a criss-cross network (ccnet) for obtaining full-image contextual information in a very effective and efficient way. concretely, for each pixel, a novel criss-cross attention module harvests the contextual information of all the pixels on its criss-cross path. by taking a further recurrent operation, each pixel can finally capture the full-image dependencies. besides, a category consistent loss is proposed to enforce the criss-cross attention module to produce more discriminative features. overall, ccnet is with the following merits: 1) gpu memory friendly. compared with the non-local block, the proposed recurrent criss-cross attention module requires 11x less gpu memory usage. 2) high computational efficiency. the recurrent criss-cross attention significantly reduces flops by about 85 percent of the non-local block. 3) the state-of-the-art performance. we conduct extensive experiments on semantic segmentation benchmarks including cityscapes, ade20k, human parsing benchmark lip, instance segmentation benchmark coco, video segmentation benchmark camvid. in particular, our ccnet achieves the miou scores of 81.9, 45.76 and 55.47 percent on the cityscapes test set, the ade20k validation set and the lip validation set respectively, which are the new state-of-the-art results. the source codes are available at https://github.com/speedinghzl/ccnethttps://github.com/speedinghzl/ccnet",AB_0102
"accurately predicting pedestrian trajectories requires a human-like socio-physical understanding of movement, nearby pedestrians, and obstacles. however, traditional methods struggle to generate multiple trajectories in the same situation based on socio-physical understanding and are compu-tationally intensive, making real-time application difficult. to overcome these limitations, we propose spu-bert, a fast multi-trajectory prediction model that incorporates two non-recursive berts for multi-goal prediction (mgp) and trajectory-to-goal prediction (tgp). first, mgp predicts multiple goals through generative models, followed by tgp generating trajectories that approach the predicted goals. spu-bert can simultaneously understand movement, social interaction, and scene context from trajectories and semantic maps using a single transformer encoder, providing explainable results as evidence of socio-physical understanding. in experiments, spu-bert accurately predicted future trajectories (with 0.19 m and 7.54 pixels of ade20 for the eth/ucy datasets and sdd) with over 100 times faster computation (0.132 s) than the state-of-the-art method. the code is available at https://github.com/kina4147/spubert. & copy; 2023 published by elsevier b.v.",AB_0102
"deep neural networks have achieved outstanding performance over various tasks, but they have a critical issue: over-confident predictions even for completely unknown samples. many studies have been proposed to successfully filter out these unknown samples, but they only considered narrow and specific tasks, referred to as misclassification detection, open-set recognition, or out-of-distribution detection. in this work, we argue that these tasks should be treated as fundamentally an identical problem because an ideal model should possess detection capability for all those tasks. therefore, we introduce the unknown detection task, an integration of previous individual tasks, for a rigorous examination of the detection capability of deep neural networks on a wide spectrum of unknown samples. to this end, unified benchmark datasets on different scales were constructed and the unknown detection capabilities of existing popular methods were subject to comparison. we found that deep ensemble consistently outperforms the other approaches in detecting unknowns; however, all methods are only successful for a specific type of unknown. the reproducible code and benchmark datasets are available at https://github.com/daintlab/unknown-detection-benchmarks.",AB_0102
"pascal_tcs is an efficient and scalable solver for large-scale direct numerical simulations of natural convective flow that considers temperature-dependent fluid properties. for increased scalability on a massive-scale distributed memory system, the solver decomposes the computational domain into a cubic sub-domain. the numerical procedure is based on the monolithic projection method with staggered time discretization [1], which is an implicit and non-iterative solver for wall-bounded domains. the pascal_tdma library is used to solve batched tridiagonal systems, which are partitioned according to domain decomposition. for parallel fast fourier transform in a fourier poisson solver for pressure, two transpose schemes with different communicator sizes are proposed and compared according to the number of cores. an explicit intermediate aggregation scheme for mpi-io is suggested to reduce the number of processes that simultaneously take part in parallel io. the overall implementation was evaluated using the nurion cluster system of the korea institute of science and technology information with detailed performance profiling. with the efficient communication, the results showed good strong and weak scalability up to 131,072 cores. the file io performance improved with the suggested mpi-io scheme using explicit aggregation, especially when many processes are involved in parallel io. the solver was verified by conducting large-scale differentially heated vertical convection flow simulations under the oberbeck-boussinesq (ob) approximation. the capability of the solver to capture and quantitatively describe the non-ob effect was demonstrated via glycerol rayleigh-benard convection flow simulations.program summaryprogram title: pascal_tcscpc library link to program files: https://doi .org /10 .17632 /3b2bysrcxm .1developer's repository link: https://github .com /mpmc-lablicensing provisions: mitprogramming language: fortran90nature of problem: solving the incompressible navier-stokes equations for natural convective flow considering the non-oberbeck-boussinesq effect, which considers the temperature dependence of fluids properties, in three-dimensional channel domain.solution method: pascal_tcs uses the monolithic projection-based method with staggered time discretization (mpm-std) [1]. it employs the crank-nicolson scheme along with staggered time which evaluates scalar and vector variables at the (n +1/2) and (n +1) time level, respectively. pascal_tdma [2] is used to solve multiple tridiagonal matrix systems for decoupled energy and momentum equations in a distributed memory system. the poisson equation solver for the pressure-correction scheme is based on fourier diagonalization. mpi_alltoallw is used for parallel fft. pascal_tcs supports a single-file-based parallel io for efficient data generation in large-scale computing.& copy; 2023 elsevier b.v. all rights reserved.",AB_0102
"mass spectrometry (ms)-based untargeted metabolomic and lipidomic approaches are being used increasingly in biomedical research. the adoption and integration of these data are critical to the overall multi-omic toolkit. recently, a sample extraction method called multi- able has been developed, which enables concurrent generation of proteomic and untargeted metabolomic and lipidomic data from a small amount of tissue. the proteomics field has a well-established set of software for processing of acquired data; however, there is a lack of a unified, off-the-shelf, ready-to-use bioinformatics pipeline that can take advantage of and prepare concurrently generated metabolomic and lipidomic data for joint downstream analyses. here we present an r pipeline called multiabler as a unified and simple upstream processing and analysis pipeline for both metabolomics and lipidomics datasets acquired using liquid chromatography-tandem mass spectrometry. the code is available via an open-source license at https://github.com/holab-hku/multiabler.",AB_0102
"the response of the hydrological cycle to anthropogenic climate change, especially across the tropical oceans, remains poorly understood due to the scarcity of long instrumental temperature and hydrological records. massive shallow-water corals are ideally suited to reconstructing past oceanic variability as they are widely distributed across the tropics, rapidly deposit calcium carbonate skeletons that continuously record ambient environmental conditions, and can be sampled at monthly to annual resolution. climate reconstructions based on corals primarily use the stable oxygen isotope composition (delta o-18), which acts as a proxy for sea surface temperature (sst), and the oxygen isotope composition of seawater (delta(18)osw), a measure of hydrological variability. increasingly, coral delta o-18 time series are paired with time series of strontium-to-calcium ratios (sr/ca), a proxy for sst, from the same coral to quantify temperature and delta(18)osw variability through time. to increase the utility of such reconstructions, we present the coralhydro2k database, a compilation of published, peer-reviewed coral sr/ca and delta o-18 records from the common era (ce). the database contains 54 paired sr/ca-delta o-18 records and 125 unpaired sr/ca or delta o-18 records, with 88% of these records providing data coverage from 1800 ce to the present. a quality-controlled set of metadata with standardized vocabulary and units accompanies each record, informing the use of the database. the coralhydro2k database tracks large-scale temperature and hydrological variability. as such, it is well-suited for investigations of past climate variability, comparisons with climate model simulations including isotope-enabled models, and application in paleodata-assimilation projects. the coralhydro2k database is available in linked paleo data (lipd) format with serializations in matlab, r, and python and can be downloaded from the noaa national center for environmental information's paleoclimate data archive at https://doi.org/10.25921/yp94-v135 (walter et al., 2022).",AB_0102
"the recognition of different activities in sports has gained attention in recent years for its applications in various athletic events, including soccer and cricket. cricket, in particular, presents a challenging task for automatic activity recognition methods due to its closely overlapped activities such as cover drive, and pull short, to name a few. existing methods often rely on hand-crafted features as the limited availability of public data has restricted the scope of research to only the significant categories of cricket activities. to this end, we proposed a cricket activities dataset and an intuitive end-to-end deep learning model for cricket activity recognition. the data is collected from online sources and pre-processed through cleaning, resizing, and organizing. similarly, an intuitive deep model is designed with a combination of time-distributed 2d cnn layers and lstm cells for extracting and learning the spatiotemporal information from the input sequences. for benchmarking, we evaluated the model on our cricket datasets and four standard datasets namely ucf101, hmdb51, youtube action, and kinetics. the quantitative results show that the proposed model outperforms different variants of recurrent neural networks and achieved an accuracy of 92%, recall of 91%, and f1 score of 91%. our code and dataset is publicly available for further research on https://drive.google.com/file/d/1c9qcaz4q00qvx4yfa3psudwfczm1cwul/view?usp=sharing. & copy; 2023 the authors. published by elsevier bv on behalf of faculty of engineering, alexandria licenses/by-nc-nd/4.0/).",AB_0102
"our objective is to compare seven blood pressure (bp) and heart rate (hr) estimation models based on shallow and deep regressors using single-channel photoplethysmograms (ppgs) under three evaluation cases. non-invasive, cuffless, continuous, and simultaneous estimation of systolic bp, diastolic bp, and hr using only ppgs is valuable for routinely monitoring the health status using simple device configuration. however, the performances of regressors cannot be directly compared based on the metrics presented in their respective papers because they use different datasets and evaluation procedures. therefore, we used a common dataset (containing 1811 patients) and prepared three evaluation cases in which the training and testing data included, partly included, and did not include data from the same subjects. shallow models using hand-crafted features performed reasonably well, and when the training data contained data similar to the testing data, deep models using raw signals as input achieved accurate bp and hr estimation. the spectral-temporal residual network showed the highest performance, with modified pp-net as the runner-up; this study revealed that they are strong candidates for non-invasive, cuffless, continuous, and simultaneous bp and hr estimation models using single-channel ppgs in clinical and daily-use scenarios. furthermore, results showed that the estimation performances rely on the level of inclusion of same subjects' data in the training and testing data. to address the discrepancy between pre-measured data domain and new user's data domain, we will investigate effective transfer learning techniques in the future work. the training and testing datasets used in this study are made public (https://drive.google.com/drive/folders/14nseg0v-metcis3lhpbh9tolxiycn8ea?usp=share_link).",AB_0102
"background: cluster and transmission analysis utilising pairwise snp distance are increasingly used in genomic epidemiological studies. however, current methods are often challenging to install and use, and lack interactive functionalities for easy data exploration.results: graphsnp is an interactive visualisation tool running in a web browser that allows users to rapidly generate pairwise snp distance networks, investigate snp distance distributions, identify clusters of related organisms, and reconstruct transmission routes. the functionality of graphsnp is demonstrated using examples from recent multi-drug resistant bacterial outbreaks in healthcare settings. conclusions: graphsnp is freely available at https://github.com/nalarbp/graphsnp. an online version of graphsnp, including demonstration datasets, input templates, and quick start guide is available for use at https://graphsnp.fordelab.com.",AB_0102
"to increase the accuracy of medical image analysis using supervised learning- based ai technology, a large amount of accurately labeled training data is required. however, the supervised learning approach may not be applicable to real-world medical imaging due to the lack of labeled data, the privacy of patients, and the cost of specialized knowledge. to handle these issues, we utilized kronecker-factored decomposition, which enhances both computational efficiency and stability of the learning process. we combined this approach with a model-agnostic meta-learning framework for the parameter optimization. based on this method, we present a bidirectional meta-kronecker factored optimizer (bm-kfo) framework to quickly optimize semantic segmentation tasks using just a few magnetic resonance imaging (mri) images as input. this model-agnostic approach can be implemented without altering network components and is capable of learning the learning process and meta-initial points while training on previously unseen data. we also incorporated a combination of average hausdorff distance loss (ahd-loss) and cross-entropy loss into our objective function to specifically target the morphology of organs or lesions in medical images. through evaluation of the proposed method on the abdominal mri dataset, we obtained an average performance of 78.07% in setting 1 and 79.85% in setting 2. our experiments demonstrate that bm-kfo with ahd-loss is suitable for general medical image segmentation applications and achieves superior performance compared to the baseline method in few-shot learning tasks. in order to replicate the proposed method, we have shared our code on github. the corresponding url can be found: https://github. com/yeongjoonkim/bmkfo.git.",AB_0102
