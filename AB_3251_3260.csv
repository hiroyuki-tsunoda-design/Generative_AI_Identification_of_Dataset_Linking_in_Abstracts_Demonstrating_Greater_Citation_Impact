AB,NO
"automatic short-answer grading (asag) is an application for recognizing textual entailment in smart education. with the continuous expansion of the application scope of artificial neural networks, many deep learning models have been applied to grading short-answer texts. however, the coding structures and interaction forms of existing models are still too simple to meet the requirements of the asag task, resulting in low scoring accuracy. to address these challenges, we propose a cross-lingual hybrid neural network with interaction enhancement for asag. first, we sequentially use a convolutional neural network and bidirectional long short-term memory (lstm) network to encode the answer text. then, we introduce an interaction enhancement layer consisting of reference-answer-to-student-answer and student-answer-to-reference-answer attentions, and we combine the attentions and their inputs to form enhanced representations of answer texts. finally, we introduce two siamese bi-lstm networks to fuse the enhanced representations of answer texts and combine their multiple pooled vectors for grade classification on a multi-linear prediction layer. the experimental results show that our model significantly improves the performance of various simple models for chinese and english asag tasks. the code is available online at https://github.com/wuhan-1222/dl_asag.",AB_0326
"the use of deep learning techniques in detecting anomalies in time series data has been an active area of research with a long history of development and a variety of approaches. in particular, reconstruction-based unsupervised anomaly detection methods have gained popularity due to their intuitive assumptions and low computational requirements. however, these methods are often susceptible to outliers and do not effectively model anomalies, leading to suboptimal results. this paper presents a novel approach for unsupervised anomaly detection, called the cooperative network time series (cnts) approach. the cnts system consists of two components: a detector and a reconstructor. the detector is responsible for directly detecting anomalies, while the reconstructor provides reconstruction information to the detector and updates its learning based on anomalous information received from the detector. the central aspect of cnts is a multi-objective optimization problem, which is solved through a cooperative solution strategy. experiments on three real-world datasets demonstrate the state-of-the-art performance of cnts and confirm the effectiveness of the detector and reconstructor. the source code for this study is publicly available on github (https://github.com/bombooooo/cnts/tree/main).",AB_0326
"in order to solve the problems about color distortion and low contrast of underwater images, we propose an underwater image enhancement algorithm that combines adaptive color correction with improved retinex algorithm. our algorithm is a single-image enhancement method that does not require specialized hardware and underwater scenes prior. firstly, the adaptive color correction is carried out on the underwater distorted images to solve the color cast problem effectively. then, on the one hand, we use the image decomposition to strengthen the detail part and obtain a detail enhanced image. on the other hand, we use the improved retinex algorithm to strengthen the edge part and obtain an edge enhanced image. finally, the detail enhanced image and the edge enhanced image are fused based on the non-subsampled shearlet transform (nsst) to obtain the final enhanced underwater image. the results show that our method outperforms several state-of-the-art methods about underwater image enhancement in terms of pcqi, uciqe, uiqm and ie. by scale invariant feature transform (sift) algorithm, we calculate the number of feature matching points of the input image and the enhanced image, and our proposed method achieves the best experimental results. the source code of our proposed algorithm is available at: https://github.com/lin9393/ underwater-image-enhance.",AB_0326
"enabling a flexible and natural human-robot interaction (hri) for industrial robots is a critical yet challenging task that can be facilitated by the use of conversational artificial intelligence (ai). prior research has concentrated on strengthening interactions through the deployment of social robots, while disregarding the capabilities required to boost the flexibility and user experience associated with human-robot collaboration (hrc) on manufacturing tasks. one of the main challenges is the lack of publicly available industrial-oriented dialogue datasets for the training of conversational ai. in this work, we present an industrial robot wizard-of-oz dialoguing dataset (irwoz) focused on enabling hrc in manufacturing tasks. the dataset covers four domains: assembly, transportation, position, and relocation. it is created using the wizard-of-oz technique to be less noisy. we manually constructed, annotated and validated dialogue segments (e.g., intentions, slots, annotations), as well as the responses. building upon the proposed dataset, we benchmark it on the state-of-the-art (sota) language models, generative pre-trained (gpt-2) models, on dialogue state tracking and response generation tasks. we expect that the irwoz dataset will facilitate exciting ongoing dialogue research and we provide it freely accessible at https://github.com/lcroy/tod4ir/tree/main/dataset.",AB_0326
"in electron beam lithography (ebl), the proximity effect seriously influences pattern resolution under high-precision conditions. mainstream proximity effect correction (pec) methods based on 2-d fast fourier transform (2d-fft) calculate a large number of unexposed points; thus, it may suffer from low efficiency especially when the exposure layout is unevenly distributed. this article proposes an efficient unequally spaced grid pec method for ebl based on the fast multipole method (fmm). fmm in pec just calculates the interaction between all the exposure points, and thus, it gets rid of the limitation of the equally spaced grid. compared to the state-of-the-art pec method based on 2d-fft, the calculation speed of fmm will exceed the current fastest 2d-fft convolution when the layout exposure density is below a certain proportion (approximately 80% under the 10-thread cpu parallel computing conditions). for the application of integrated circuit (ic) mask industry, the error of fmm is within the acceptable range of pec. the pec method in this article has been applied to a free software via software as a service (saas) mode, and a windows-based ebl simulation and optimization software toolkit hnu-ebl, which is freely available at http://www.ebeam.com.cn.",AB_0326
"intelligent detection of road cracks is crucial for road maintenance and safety. because of the interference of illumination and totally different background factors, the road crack extraction results of existing deep learning ways square measure incomplete, and therefore the extraction accuracy is low. we tend to designed a brand new network model, referred to as ar-unet, that introduces a convolutional block attention module (cbam) within the encoder and decoder of u-net to effectively extract global and local detail information. the input and output cbam features of the model are connected to increase the transmission path of features. the basicblock is adopted to replace the convolutional layer of the original network to avoid network degradation caused by gradient disappearance and network layer growth. we tested our method on deepcrack, crack forest dataset, and our own tagged road image dataset (rid). the experimental results show that our method focuses additional on crack feature info and extracts cracks with higher integrity. the comparison with existing deep learning ways conjointly demonstrates the effectiveness of our projected technique. the code is out there at: https://github.com/18435398440/arunet.",AB_0326
"network data is mostly hard to obtain and error-prone. however, most existing works assume that the studied network represents a perfect and complete picture of topological structure; nevertheless, it is rarely the case in real-world situations. such studies, performing downstream applications (e.g., vertex classification, link prediction, etc.) directly on original networks, will suffer greatly due to the noise and deteriorate the application performance. in this paper, we propose netrl, a novel method for network denoising, that works by creating missing edges and removing incorrect edges from a noisy network, thereby improving its quality and representative power. in particular, netrl turns the problem of network denoising into edge sequences generation, which can be formulated as a markov decision process. by doing this, netrl takes the complex long-term dependency between edge creations into consideration, i.e., the existence of an edge depends on which edges have been generated so far. it further takes advantage of downstream task to guide the network denoising process, by providing a deep reinforcement learning framework to conduct direct optimization on this task-specific objective. as a result, netrl ensures that the denoised network not only satisfies the topological property of the original network, but also improves the performance of the downstream application. experimental results on real-world networks show that, comparing with several baseline methods, netrl can denoise networks effectively with better performance for vertex classification. meanwhile, netrl can better preserve original network's properties (e.g., degree distribution and clustering coefficient. our implementation is available at: https://github.com/galina0217/netrl.",AB_0326
"tax evasion is an illegal activity in which individuals or entities avoid paying their true tax liabilities. efficient detection of tax evasion has always been a crucial issue for both governments and academic researchers. recent research has proposed the use of machine learning technology to detect tax evasion and has shown good results in some specific areas. regrettably, there are still two major obstacles to detect tax evasion. first, it is hard to extract powerful features because of the complexity of tax data. second, due to the complicated process of tax auditing, labeled data are limited in practice. such obstacles motivate the contributions of this work. in this paper, we propose a novel tax evasion detection framework named fbne-pu (fusion of the basic feature and network embedding with pu learning for tax evasion detection), a multistage method for detecting tax evasion in real-life scenarios. in this paper, we perform an in-depth analysis of the characteristics of the transaction network and propose a novel network embedding algorithm, the pncgcn. it significantly improves detection performance by extracting powerful features from basic features and the tax-related transaction network. moreover, we use nnpu (positive-unlabeled learning with non-negative risk estimator) to assign pseudo labels for unlabeled data. finally, an mlp is trained as the decision function. experiments on three real-world datasets demonstrate that our method significantly outperforms the comparison methods in the tax evasion detection task. additionally, the source code and the experimental details have been made available at (https://github.com/piggygaga/fbne-pu).",AB_0326
"current decision trees such as c4.5 and cart are widely used in different fields due to their simplicity, accuracy and intuitive interpretation. similar to other popular classifiers, these tree-based classification algorithms are developed for fixed-length vector data and suffer from intrinsic limitations in handling complex data such as sequences. to tackle the discrete sequence classification task, the dominant strategy is to adopt a two-step procedure: first transform the sequential dataset into a vector dataset and then apply existing tree-based classifiers on the new vector data. however, such methods are highly dependent on the feature generation procedure and some features that are critical to the tree construction may be missed. to alleviate these issues, we present a new tree-based sequence classification method, which is able to construct a concise decision tree from the feature space that is composed of all subsequences presented in the training sequences. experimental results on fourteen real datasets show that our method can achieve better performance than those state-of-the-art sequence classification algorithms. the source codes of our method are available at: https://github.com/ziyaowu/seqdt.",AB_0326
"the aluminum surface defect detection is not trivial for the high computational cost and labor-intensive data annotation. particularly, the characteristics of lots of very tiny objects, sample sparsity, and variations, limit the detection performance. in this paper, we demonstrate a sophisticated and efficient object detection model based on hierarchical attention and contextual information for aluminum surface defect detection. specially, we first use a deep residual learning strategy to obtain the defect feature maps. secondly, we add corresponding weight matrices to the defect feature maps by fusing attention mechanism and adaptive deformable convolution to achieve the fine feature. thirdly, we construct a feature pyramid structure to achieve the fusion of multi-scale feature information. finally, we use the obtained contextual feature information for class prediction and bounding box regression. the comprehensive experiments on the surface defect data set of aluminum and the surface defect data set of copper foil and aluminum foil respectively show that our method compared to state-of-the-art object detectors. code is available at https://github.com/yunsheng-wei/dfa-frcnn.git.",AB_0326
