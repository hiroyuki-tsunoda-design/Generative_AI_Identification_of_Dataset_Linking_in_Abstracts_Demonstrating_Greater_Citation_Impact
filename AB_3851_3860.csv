AB,NO
"unlike the success of neural architecture search (nas) in high-level vision tasks, it remains challenging to find computationally efficient and memory-efficient solutions to low-level vision problems such as image restoration through nas. one of the fundamental barriers to differential nas-based image restoration is the optimization gap between the super-network and the sub-architectures, causing instability during the searching process. in this paper, we present a novel approach to fill this gap in image denoising application by connecting model-guided design (mod) with nas (mod-nas). specifically, we propose to construct a new search space under a model-guided framework and develop more stable and efficient differential search strategies. mod-nas employs a highly reusable width search strategy and a densely connected search block to automatically select the operations of each layer as well as network width and depth via gradient descent. during the search process, the proposed mod-nas remains stable because of the smoother search space designed under the model-guided framework. experimental results on several popular datasets show that our mod-nas method has achieved at least comparable even better psnr performance than current state-of-the-art methods with fewer parameters, fewer flops, and less testing time. the code associate with this paper is available at: https://see.xidian.edu.cn/faculty/wsdong/projects/modnas.htm.",AB_0386
"deep-learning-based local feature extraction algorithms that combine detection and description have made significant progress in visible image matching. however, the end-to-end training of such frameworks is notoriously unstable due to the lack of strong supervision of detection and the inappropriate coupling between detection and description. the problem is magnified in cross-modal scenarios, in which most methods heavily rely on the pre-training. in this paper, we recouple independent constraints of detection and description of multimodal feature learning with a mutual weighting strategy, in which the detected probabilities of robust features are forced to peak and repeat, while features with high detection scores are emphasized during optimization. different from previous works, those weights are detached from back propagation so that the detected probability of indistinct features would not be directly suppressed and the training would be more stable. moreover, we propose the super detector, a detector that possesses a large receptive field and is equipped with learnable non-maximum suppression layers, to fulfill the harsh terms of detection. finally, we build a benchmark that contains cross visible, infrared, near-infrared and synthetic aperture radar image pairs for evaluating the performance of features in feature matching and image registration tasks. extensive experiments demonstrate that features trained with the recoulped detection and description, named redfeat, surpass previous state-of-the-arts in the benchmark, while the model can be readily trained from scratch. the code is released at https://github.com/acuooooo/redfeat.",AB_0386
"learning-based infrared small object detection methods currently rely heavily on the classification backbone network. this tends to result in tiny object loss and feature distinguishability limitations as the network depth increases. furthermore, small objects in infrared images are frequently emerged bright and dark, posing severe demands for obtaining precise object contrast information. for this reason, we in this paper propose a simple and effective u-net in u-net framework, uiu-net for short, and detect small objects in infrared images. as the name suggests, uiu-net embeds a tiny u-net into a larger u-net backbone, enabling the multi-level and multi-scale representation learning of objects. moreover, uiu-net can be trained from scratch, and the learned features can enhance global and local contrast information effectively. more specifically, the uiu-net model is divided into two modules: the resolution-maintenance deep supervision (rm-ds) module and the interactive-cross attention (ic-a) module. rm-ds integrates residual u-blocks into a deep supervision network to generate deep multi-scale resolution-maintenance features while learning global context information. further, ic-a encodes the local context information between the low-level details and high-level semantic features. extensive experiments conducted on two infrared single-frame image datasets, i.e., sirst and synthetic datasets, show the effectiveness and superiority of the proposed uiu-net in comparison with several state-of-the-art infrared small object detection methods. the proposed uiu-net also produces powerful generalization performance for video sequence infrared small object datasets, e.g., atr ground/air video sequence dataset. the codes of this work are available openly at https://github.com/danfenghong/ieee",AB_0386
"cross-modality face image synthesis such as sketch-to-photo, nir-to-rgb, and rgb-to-depth has wide applications in face recognition, face animation, and digital entertainment. conventional cross-modality synthesis methods usually require paired training data, i.e., each subject has images of both modalities. however, paired data can be difficult to acquire, while unpaired data commonly exist. in this paper, we propose a novel semi-supervised cross-modality synthesis method (namely cmos-gan), which can leverage both paired and unpaired face images to learn a robust cross-modality synthesis model. specifically, cmos-gan uses a generator of encoder-decoder architecture for new modality synthesis. we leverage pixel-wise loss, adversarial loss, classification loss, and face feature loss to exploit the information from both paired multi-modality face images and unpaired face images for model learning. in addition, since we expect the synthetic new modality can also be helpful for improving face recognition accuracy, we further use a modified triplet loss to retain the discriminative features of the subject in the synthetic modality. experiments on three cross-modality face synthesis tasks (nir-to-vis, rgb-to-depth, and sketch-to-photo) show the effectiveness of the proposed approach compared with the state-of-the-art. in addition, we also collect a large-scale rgb-d dataset (vipl-mumoface-3k) for the rgb-to-depth synthesis task. we plan to open-source our code and vipl-mumoface-3k dataset to the community (https://github.com/skgyu/cmos-gan).",AB_0386
"as an important yet challenging task in earth observation, change detection (cd) is undergoing a technological revolution, given the broadening application of deep learning. nevertheless, existing deep learning-based cd methods still suffer from two salient issues: 1) incomplete temporal modeling, and 2) space-time coupling. in view of these issues, we propose a more explicit and sophisticated modeling of time and accordingly establish a pair-to-video change detection (p2v-cd) framework. first, a pseudo transition video that carries rich temporal information is constructed from the input image pair, interpreting cd as a problem of video understanding. then, two decoupled encoders are utilized to spatially and temporally recognize the type of transition, and the encoders are laterally connected for mutual promotion. furthermore, the deep supervision technique is applied to accelerate the model training. we illustrate experimentally that the p2v-cd method compares favorably to other state-of-the-art cd approaches in terms of both the visual effect and the evaluation metrics, with a moderate model size and relatively lower computational overhead. extensive feature map visualization experiments demonstrate how our method works beyond making contrasts between bi-temporal images. source code is available at https://github.com/bobholamovic/cdlab.",AB_0386
"domain generalization aims to learn knowledge invariant across different distributions while semantically meaningful for downstream tasks from multiple source domains, to improve the model's generalization ability on unseen target domains. the fundamental objective is to understand the underlying invariance behind these observational distributions and such invariance has been shown to have a close connection to causality. while many existing approaches make use of the property that causal features are invariant across domains, we consider the invariance of the average causal effect of the features to the labels. this invariance regularizes our training approach in which interventions are performed on features to enforce stability of the causal prediction by the classifier across domains. our work thus sheds some light on the domain generalization problem by introducing invariance of the mechanisms into the learning process. experiments on several benchmark datasets demonstrate the performance of the proposed method against sotas. the codes are available at: https://github.com/lithostark/contrastive-ace.",AB_0386
"unsupervised person re-identification (re-id) remains a challenging task. while extensive research has focused on the framework design and loss function, this paper shows that sampling strategy plays an equally important role. we analyze the reasons for the performance differences between various sampling strategies under the same framework and loss function. we suggest that deteriorated over-fitting is an important factor causing poor performance, and enhancing statistical stability can rectify this problem. inspired by that, a simple yet effective approach is proposed, termed group sampling, which gathers samples from the same class into groups. the model is thereby trained using normalized group samples, which helps alleviate the negative impact of individual samples. group sampling updates the pipeline of pseudo-label generation by guaranteeing that samples are more efficiently classified into the correct classes. it regulates the representation learning process, enhancing statistical stability for feature representation in a progressive fashion. extensive experiments on market-1501, dukemtmc-reid and msmt17 show that group sampling achieves performance comparable to state-of-the-art methods and outperforms the current techniques under purely camera-agnostic settings. code has been available at https://github.com/ucas-vg/groupsampling.",AB_0386
"camouflaged object detection, which aims to detect/segment the object(s) that blend in with their surrounding, remains challenging for deep models due to the intrinsic similarities between foreground objects and background surroundings. ideally, an effective model should be capable of finding valuable clues from the given scene and integrating them into a joint learning framework to co-enhance the representation. inspired by this observation, we propose a novel mutual graph learning (mgl) model by shifting the conventional perspective of mutual learning from regular grids to graph domain. specifically, an image is decoupled by mgl into two task-specific feature maps - one for finding the rough location of the target and the other for capturing its accurate boundary details. then, the mutual benefits can be fully exploited by reasoning their high-order relations through graphs recurrently. it should be noted that our method is different from most mutual learning models that model all between-task interactions with the use of a shared function. to increase information interactions, mgl is built with typed functions for dealing with different complementary relations. to overcome the accuracy loss caused by interpolation to higher resolution and the computational redundancy resulting from recurrent learning, the s-mgl is equipped with a multi-source attention contextual recovery module, called r-mgl_v2, which uses the pixel feature information iteratively. experiments on challenging datasets, including chameleon, camo, cod10k, and nc4k demonstrate the effectiveness of our mgl with superior performance to existing state-of-the-art methods. the code can be found at https://github.com/fanyang587/mgl.",AB_0386
"point cloud semantic segmentation (pcss), for the purpose of labeling a set of points stored in irregular and unordered structures, is an important yet challenging task. it is vital for the task of learning a good representation for each 3d data point, which encodes rich context knowledge and hierarchically structural information. however, despite great success has been achieved by existing pcss methods, they are limited to make full use of important context information and rich hierarchical features for representation learning. in this paper, we propose to build 'hyperpoint' representations for 3d data point via a nested network architecture, which is able to explicitly exploit multi-scale, pyramidally hierarchical features and construct powerful representations for pcss. in particular, we introduce a pcss nested architecture search (pcss-nas) algorithm to automatically design the model's side-output branches at different levels as well as its skip-layer structures, enabling the resulting model to best deal with the scale-space problem. our searched architecture, named auto-nestednet, is evaluated on four well-known benchmarks: s3dis, scannet, semantic3d and paris-lille-3d. experimental results show that the proposed auto-nestednet achieves the state-of-the-art performance. our source code is available at https://github.com/fanyang587/nestednet.",AB_0386
"extracting building footprints from aerial images is essential for precise urban mapping with photogrammetric computer vision technologies. existing approaches mainly assume that the roof and footprint of a building are well overlapped, which may not hold in off-nadir aerial images as there is often a big offset between them. in this paper, we propose an offset vector learning scheme, which turns the building footprint extraction problem in off-nadir images into an instance-level joint prediction problem of the building roof and its corresponding roof to footprint offset vector. thus the footprint can be estimated by translating the predicted roof mask according to the predicted offset vector. we further propose a simple but effective feature-level offset augmentation module, which can significantly refine the offset vector prediction by introducing little extra cost. moreover, a new dataset, buildings in off-nadir aerial images (bonai), is created and released in this paper. it contains 268,958 building instances across 3,300 aerial images with fully annotated instance-level roof, footprint, and corresponding offset vector for each building. experiments on the bonai dataset demonstrate that our method achieves the state-of-the-art, outperforming other competitors by 3.37 to 7.39 points in f1-score. the codes, datasets, and trained models are available at https://github.com/jwwangchn/bonai.git.",AB_0386
