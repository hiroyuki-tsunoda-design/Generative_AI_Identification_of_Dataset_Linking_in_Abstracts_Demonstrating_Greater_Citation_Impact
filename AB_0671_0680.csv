AB,NO
"deep learning has shown success in several applications involving pattern recognition, expert systems, and scientific discovery. however, existing methods struggle with industrial applications, which are often challenged by non-ideal datasets. in many cases, the datasets are small, poorly labeled, noisy, or have unbalanced class distribution, or any combination of such problems. in this contribution, we propose a generative adversarial network (gan) strategy that is able to circumvent limitations imposed by very small datasets. as case study, we use the extrapolation of corrosion in automobiles and feed our deep learning framework with only few dozen images, as opposed to the thousands to million images commonly found in many computer vision problems. in order to handle such a reduced dataset, we use one gan for the rust level and one for the rust texture. the rust level gan is conditional on random samples from the dataset and uses an additive random noise in the latent space to add variability to the generated rust level maps. the rust texture gan adds the shades of brown to the outputs of the rust level gan. to improve the robustness of our approach, the rust level gan is a conditional gan with random noise added in the latent space of the generator; while the rust texture gan uses supervised loss functions coming from the observed data set as well as an unsupervised loss function coming from the rust level gan. in addition, given the very reduced size of our dataset, it is unfeasible to break down the data into training, validation, and test sets. we overcome this limitation by using the discrepancy between the generated and target distributions of the rust level and texture intensities as a way to monitor convergence of training. the resulting models are able to ingest an image with a car having no corrosion and generates an image of this car with parts exhibiting varying degrees of corrosion (from mild, to moderate, to severe). the source codes and links to the data can be found in the following github repository https://github.com/pml-ucf/rusty_fender_gan.",AB_0068
"although the cretaceous is widely regarded as a time of great evolutionary transition for the freshwater fish fauna of north america, the fossil record of this period is notoriously poor, consisting mostly of fragments and isolated skeletal elements. exceptions include the acipenseriforms, discussed in this paper, and some exceedingly rare teleosts. here we describe two new species of well-preserved sturgeons (acipenseridae) from the tanis site in the late cretaceous hell creek formation of north dakota. the type and referred materials were preserved in a loosely consolidated matrix. dagger acipenser praeparatorum n. sp. is represented by multiple body fossils (including the head and relatively complete postcranial remains) and a specimen of an intact, three dimensionally preserved skull and pectoral girdle. this taxon can be diagnosed based on features of the opercular elements (exceptionally tall and narrow branchiostegal). the second species, dagger acipenser anisinferos n. sp., is represented by a partially preserved skull, and can be diagnosed by a relatively elongate preorbital region (i.e., snout) and the absence of thorn-like spines on the skull roofing bones. most known sturgeon fossils from the cretaceous are represented only by undiagnosable fragmentary remains (i.e., scutes and pectoral-fin spines) or poorly preserved partial skeletons (e.g., dagger protoscaphirhynchus), with dagger priscosturion and dagger anchiacipenser (both monotypic) being rare exceptions. therefore, the newly discovered tanis fossils give a rare glimpse into the evolution of acipenseridae at a critical time in the phylogenetic history of acipenseriforms, and suggest significant morphological and taxonomic diversity early in the evolution of this group. uuid: http://zoobank.org/375b586a-2dd8-4a31-b6c4-42151e6e8fc8",AB_0068
"this work presents a novel general tool for modeling the process of evaporation without the need for modifying existing software using python. the tool was developed based on the mdanalysis package, which is used to import a molecular dynamics trajectory. the tool then removes solvent molecules and outputs a new structure file to be used for further simulation and analysis. this process is designed to be iterated by using the resulting dynamic simulation trajectory as the input file. the evaporation is designed to randomly delete solvent molecules while preserving solvation shells around solutes. the evaporation rate can be controlled by the length of the md simulations and the number of particles removed between dynamic simulations. validity of the tool was tested extensively using the gromacs suite. advantages of this tool include its genericness, simplicity and user friendliness, as no significant modification of existing software platform or gromacs specific tools are needed. program summary program title: genevapa cpc library link to program files: https://doi .org /10 .17632 /y5c3jnbjvs .1 developer's repository link: github .com /bradsharris /genevapa licensing provisions: gnu general public license 3 programming language: python >3 nature of problem: approximate evaporation/drying processes in atomistic and coarse-grained molecular dynamics simulations while maintaining solvation shells around solute of interest. forced drying in this manner allows for the study of a range of concentrations and self-assembly interactions. solution method: a python wrapper for existing molecular dynamics codes that randomly selects solvent for removal relative to a distance criteria around a solute to maintain solvation shells. removal on final structure files maintains generic applicability for md source codes and enables incorporation into automated loops to study longer drying. (c) 2022 the authors. published by elsevier b.v. this is an open access article under the cc by-nc-nd license ().",AB_0068
"in this work, we introduce an open-source julia project, express, an extensible, lightweight, high-throughput, high-level workflow framework that aims to automate ab initio calculations for the materials science community. express is shipped with well-tested workflow templates, including structure optimization, equation of state (eos) fitting, phonon spectrum (lattice dynamics) calculation, and thermodynamic property calculation in the framework of the quasi-harmonic approximation (qha). it is designed to be highly modularized so that its components can be reused across various occasions, and customized workflows can be built on top of that. users can also track the status of workflows in real-time, and rerun failed jobs thanks to the data lineage feature express provides. two working examples, i.e., all workflows applied to lime and akimotoite, are also presented in the code and this paper. program summary program title: express cpc library link to program files: https://doi .org /10 .17632 /bbzzrxsm4b .1 developer's repository link: https://github .com /mineralscloud /express .jl licensing provisions: gnu general public license v3.0 programming language: julia nature of problem: high-performance ab initio calculation is gaining more and more popularity when investigating the physical and chemical properties of materials in the scientific community. there are several ab initio programs in the market, but they are often not user-friendly to new users because of their intrinsic complexity. even for familiar users, dealing with daily preparation and post-analysis could be trivial and fallible. therefore, plenty of workflow frameworks have been developed to solve this problem. however, most of them cannot meet our expectations for several reasons. they are either too complex to read and modify, or too heavy for simple tasks. plus, they put little focus on addressing the real pain points of everyday calculations, such as data preparation and output parsing. solution method: we developed a workflow framework that can simplify this process, i.e., most of the mundane work can be replaced by writing a few lines of configurations. we also automated the three most-used work procedures into configurable workflows: equations of state fitting (structural optimization), phonon spectrum calculation, and thermodynamics calculation. (c) 2022 elsevier b.v. all rights reserved.",AB_0068
"genasis mathematics provides modern fortran classes furnishing extensible object-oriented function-ality for the solution of fields governed by selected partial differential equations. the initial release included extensible object-oriented implementations of simple meshes and the evolution of generic con-served currents thereon. this revision-version 2 of mathematics-includes significant reorganization and streamlining of these classes, higher-order reconstruction by a different method, a poisson solver, coarsening to avoid courant time step limitations near coordinate singularities, and the offloading of computational kernels to gpus. new version program summary program title: homogeneousspheroid, rectangularcontraction, rectangularexpansion, sphericalcontraction, sphericalexpansion (example problems illustrating genasis mathematics) cpc library link to program files: https://doi.org/10.17632/mzvxngwtw6.2 developer's repository link: https://github.com/genasis code ocean capsule: https://codeocean.com/capsule/4106782 licensing provisions: gplv3 programming language: modern fortran; openmp (tested with recent versions of gnu compiler collection (gcc), cray compiler environment (cce), ibm xl fortran compiler) journal reference of previous version: computer physics communications 222 (2018) 384 does the new version supersede the previous version?: yes reasons for the new version: this version includes significant reorganization and streamlining, higher-order reconstruction by a different method, a poisson solver, coarsening to avoid courant time step limitations near coordinate singularities, and the offloading of computational kernels to gpus. summary of revisions: the left part of fig. 1 shows the revised structure of genasis mathematics. the algebra and calculus divisions contain relatively simple stand-alone functionality for such tasks as root finding, interpolation, numerical integration, and the solution of ordinary differential equations. the major functionality currently provided in mathematics, which focuses on the solution of selected classes of partial differential equations, is now collected in the cauchyproblems division. the structure of cauchyproblems is displayed in the right part of fig. 1. the solution of partial differential equations presumes the existence of manifolds (or spaces') and fields thereon whose configuration we seek. the two basic types of cauchy problems are boundary value problems, or constraints; and initial value problems, or evolutions. the structures of manifolds, fields, constraints, and evolutions are displayed in fig. 2. individual coordinate patches or charts are combined into atlases in order to represent manifolds. as for fields, the most basic functionality for a set of fields on a manifold (including i/o) is represented by classes in fieldsets. specialized sets of fields needed for cauchy problems are geometries, including coarsening functionality to avoid courant time step limitations near coordinate singularities; and currentsets, which represent conserved'& mdash;or more properly, when source terms are present, balanced'& mdash;currents, that is, densities and their corresponding fluxes. calculusfields provides integrals of fields on manifolds, including volume and surface integrals used to tally conserved and balanced quantities, as well as spherical and azimuthal averaging. new to this version of mathematics is the solution of poissonequations via multipole expansion, the first capability for constraints or boundary value problems. the solution of balance equations is the type of initial value problem currently handled by evolutions. as in version 1, finite-volume discretization results in a large set of ordinary differential equations solved by the runge-kutta method. now however, the right-hand sides, or slopes, are handled by variations of a recursive class that allows the construction of hierarchies of terms. thus complicated equations can be computed and included in i/o in pieces representing various terms, with results assembled in an extensible and quasi-automated fashion. for example, the partial derivatives, and geometric source terms from connection coefficients, can be computed and visualized separately, while being automatically combined to give the full divergence of an energy-momentum tensor. third-order reconstruction (with limiting) to discretize cell interfaces is now included. multi-stage runge-kutta steps are then taken by integrators in order to advance the solution of fields from their initial condition. relative to version 1 of mathematics, the manifolds and fields classes are significantly reorganized and streamlined. previously, classes for fields, geometry, and i/o were interleaved with those of charts and atlases. this provided for chart and atlas classes with self-contained geometry and i/o functionality, but in a manner that proved unwieldy and indeed unnecessary. this has now been disentangled: classes for fields, geometry and i/o are built entirely on top of full manifolds (i.e. classes for atlases).all computational kernels (except those specifically needed for checkpointing, that is, i/o) are coded for offloading to gpus via openmp. nature of problem: by way of illustrating genasis mathematics functionality, solve an example potential problem and imposed advection problems. solution method: multipole expansion for solution of the poisson equation; finite-volume discretization; second- and third-order reconstruction with limiting; hll riemann solver; runge-kutta integration. additional comments including restrictions and unusual features: uses the mpi [1] and silo [2] libraries. the example problems named above are not ends in themselves, but serve to illustrate the functionality available though genasis mathematics. in addition to these more substantial examples, we provide individual unit test programs for the classes comprised by genasis mathematics. (c) 2022 elsevier b.v. all rights reserved.",AB_0068
"collision avoidance for multi-robot systems is a well-studied problem. recently, control barrier functions (cbfs) have been proposed for synthesizing controllers that guarantee collision avoidance and goal stabilization for multiple robots. however, it has been noted that reactive control synthesis methods (such as cbfs) are prone to deadlock, an equilibrium of system dynamics that causes the robots to stall before reaching their goals. in this paper, we analyze the closed-loop dynamics of robots using cbfs, to characterize controller parameters, initial conditions, and goal locations that invariably lead the system to deadlock. using tools from duality theory, we derive geometric properties of robot configurations of an n robot system once it is in deadlock and we justify them using the mechanics interpretation of kkt conditions. our key deductions are that (1) system deadlock is characterized by a force equilibrium on robots and (2) deadlock occurs to ensure safety when safety is at the brink of being violated. these deductions allow us to interpret deadlock as a subset of the state space, and we show that this set is non-empty and located on the boundary of the safe set. by exploiting these properties, we analyze the number of admissible robot configurations in deadlock and develop a provably correct decentralized algorithm for deadlock resolution to safely deliver the robots to their goals. this algorithm is validated in simulations as well as experimentally on khepera-iv robots. for an interactive version of this paper, please visit https://arxiv.org/abs/2206.01781.",AB_0068
"purpose - guided by the collective action theory, signaling theory and social identity approach, this study examines backing behavior by individuals who have created projects under cc licenses. two motivational mechanisms were examined: (1) identification via common interests in the cc space; (2) resource signaling by other users via their diverse project creation experience, funding or commenting activity. design/methodology/approach - data were collected from kickstarter.com. exponential random graph modeling was used to examine how the two reviewed mechanisms influence the tie formation probability between creative commons (cc) project creators and other creators. the analysis was conducted on two subnetworks: one with ties between cc creators; and one with ties from cc creators to non-cc creators. findings - the study found that cc creators exhibit distinct backing patterns when considering funding other cc creators compared to non-cc users. when considering funding their peer cc creators, cc identity can help them allocate and support perceived in-group members; when considering funding non-cc creators, shared common interests in competitive project categories potentially triggers a competition mindset and makes them hold back when they see potential rivals. originality/value - this study makes three contributions. first, it draws from multiple theoretical frameworks to investigate unique motivations when crowdfunders take on dual roles of creators and funders and offered implications on how to manage competition and collaboration simultaneously. second, with network analysis our study not only identifies multiple motivators at work for collective action, but also demonstrates their differential effects in crowdfunding. third, the integration of multiple theoretical frameworks allows opportunities for theory building. peer review - the peer review history for this article is available at: https://publons.com/publon/10.1108/oir05-2020-0166.",AB_0068
"for any research program examining how ambiguous words are processed in broader linguistic contexts, a first step is to establish factors relating to the frequency balance or dominance of those words' multiple meanings, as well as the similarity of those meanings to one other. homonyms-words with divergent meanings-are one ambiguous word type commonly utilized in psycholinguistic research. in contrast, although polysemes-words with multiple related senses-are far more common in english, they have been less frequently used as tools for understanding one-to-many word-to-meaning mappings. the current paper details two norming studies of a relatively large number of ambiguous english words. in the first, offline dominance norming is detailed for 547 homonyms and polysemes via a free association task suitable for words across the ambiguity continuum, with a goal of identifying words with more equibiased meanings. the second norming assesses offline meaning similarity for a partial subset of 318 ambiguous words (including homonyms, unambiguous words, and polysemes divided into regular and irregular types) using a novel, continuous rating method reliant on the linguistic phenomenon of zeugma. in addition, we conduct computational analyses on the human similarity norming data using the bert pretrained neural language model (devlin et al., 2018, bert: pre-training of deep bidirectional transformers for language understanding. arxiv preprint. arxiv:1810.04805) to evaluate factors that may explain variance beyond that accounted for by dictionary-criteria ambiguity categories. finally, we make available the summarized item dominance values and similarity ratings in resultant appendices (see supplementary material), as well as individual item and participant norming data, which can be accessed online (https://osf.io/g7fmv/).",AB_0068
"learning low-dimensional representations of bipartite graphs enables e-commerce applications, such as recommendation, classification, and link prediction. a layerwise-trained bipartite graph neural network (l-bgnn) embedding method, which is unsupervised, efficient, and scalable, is proposed in this work. to aggregate the information across and within two partitions of a bipartite graph, a customized interdomain message passing (idmp) operation and an intradomain alignment (ida) operation are adopted by the proposed l-bgnn method. furthermore, we develop a layerwise training algorithm for l-bgnn to capture the multihop relationship of large bipartite networks and improve training efficiency. we conduct extensive experiments on several datasets and downstream tasks of various scales to demonstrate the effectiveness and efficiency of the l-bgnn method as compared with state-of-the-art methods. our codes are publicly available at https://github.com/tianxieusc/l-bgnn.",AB_0068
"the record-breaking performance of deep neural networks (dnns) comes with heavy parameter budgets, which leads to external dynamic random access memory (dram) for storage. the prohibitive energy of dram accesses makes it nontrivial for dnn deployment on resource-constrained devices, calling for minimizing the movements of weights and data in order to improve the energy efficiency. driven by this critical bottleneck, we present smartdeal, a hardware-friendly algorithm framework to trade higher-cost memory storage/access for lower-cost computation, in order to aggressively boost the storage and energy efficiency, for both dnn inference and training. the core technique of smartdeal is a novel dnn weight matrix decomposition framework with respective structural constraints on each matrix factor, carefully crafted to unleash the hardware-aware efficiency potential. specifically, we decompose each weight tensor as the product of a small basis matrix and a large structurally sparse coefficient matrix whose nonzero elements are readily quantized to the power-of-2. the resulting sparse and readily quantized dnns enjoy greatly reduced energy consumption in data movement as well as weight storage, while incurring minimal overhead to recover the original weights thanks to the required sparse bit-operations and cost-favorable computations. beyond inference, we take another leap to embrace energy-efficient training, by introducing several customized techniques to address the unique roadblocks arising in training while preserving the smartdeal structures. we also design a dedicated hardware accelerator to fully utilize the new weight structure to improve the real energy efficiency and latency performance. we conduct experiments on both vision and language tasks, with nine models, four datasets, and three settings (inference-only, adaptation, and fine-tuning). our extensive results show that 1) being applied to inference, smartdeal achieves up to 2.44x improvement in energy efficiency as evaluated using real hardware implementations and 2) being applied to training, smartdeal can lead to 10.56x and 4.48x reduction in the storage and the training energy cost, respectively, with usually negligible accuracy loss, compared to state-of-the-art training baselines. our source codes are available at: https://github.com/vita-group/smartdeal.",AB_0068
