AB,NO
"3d structured data like point clouds and skeletons often can be represented as data in a 3d rotation group (denoted as so(3)). however, most existing neural networks are tailored for the data in the euclidean space, which makes the 3d rotation data not closed under their algebraic operations and leads to sub-optimal performance in 3d-related learning tasks. to resolve the issues caused by the above mismatching between data and model, we propose a novel non-real neuron model called quaternion product unit (qpu) to represent data on 3d rotation groups. the proposed qpu leverages quaternion algebra and the law of the 3d rotation group, representing 3d rotation data as quaternions and merging them via a weighted chain of hamilton products. we demonstrate that the qpu mathematically maintains the so(3) structure of the 3d rotation data during the inference process and disentangles the 3d representations into rotation-invariant features and rotation-equivariant features, respectively. moreover, we design a fast qpu to accelerate the computation of qpu. the fast qpu applies a tree-structured data indexing process, and accordingly, leverages the power of parallel computing, which reduces the computational complexity of qpu in a single thread from o(n) to o(logn). taking the fast qpu as a basic module, we develop a series of quaternion neural networks (qnns), including quaternion multi-layer perceptron (qmlp), quaternion message passing (qmp), and so on. in addition, we make the qnns compatible with conventional real-valued neural networks and applicable for both skeletons and point clouds. experiments on synthetic and real-world 3d tasks show that the qnns based on our fast qpus are superior to state-of-the-art real-valued models, especially in the scenarios requiring the robustness to random rotations. the code of this work is available at https://github.com/suferqin/fast-qpu.",AB_0364
"photon-efficient imaging, which captures 3d images with single-photon sensors, has enabled a wide range of applications. however, two major challenges limit the reconstruction performance, i.e., the low photon counts accompanied by low signal-to-background ratio (sbr) and the multiple returns. in this paper, we propose a unified deep neural network that, for the first time, explicitly addresses these two challenges, and simultaneously recovers depth maps and intensity images from photon-efficient measurements. starting from a general image formation model, our network is constituted of one encoder, where a non-local block is utilized to exploit the long-range correlations in both spatial and temporal dimensions of the raw measurement, and two decoders, which are designed to recover depth and intensity, respectively. meanwhile, we investigate the statistics of the background noise photons and propose a noise prior block to further improve the reconstruction performance. the proposed network achieves decent reconstruction fidelity even under extremely low photon counts / sbr and heavy blur caused by the multiple-return effect, which significantly surpasses the existing methods. moreover, our network trained on simulated data generalizes well to real-world imaging systems, which greatly extends the application scope of photon-efficient imaging in challenging scenarios with a strict limit on optical flux. code is available at https://github.com/jiayongo-o/penonlocal.",AB_0364
"in this paper, we propose some efficient multi-view stereo methods for accurate and complete depth map estimation. we first present our basic methods with adaptive checkerboard sampling and multi-hypothesis joint view selection (acmh $\&$& acmh+). based on our basic models, we develop two frameworks to deal with the depth estimation of ambiguous regions (especially low-textured areas) from two different perspectives: multi-scale information fusion and planar geometric clue assistance. for the former one, we propose a multi-scale geometric consistency guidance framework (acmm) to obtain the reliable depth estimates for low-textured areas at coarser scales and guarantee that they can be propagated to finer scales. for the latter one, we propose a planar prior assisted framework (acmp). we utilize a probabilistic graphical model to contribute a novel multi-view aggregated matching cost. at last, by taking advantage of the above frameworks, we further design a multi-scale geometric consistency guided and planar prior assisted multi-view stereo (acmmp). this greatly enhances the discrimination of ambiguous regions and helps their depth sensing. experiments on extensive datasets show our methods achieve state-of-the-art performance, recovering the depth estimation not only in low-textured areas but also in details. related codes are available at https://github.com/ghixu.",AB_0364
"occlusions between human and objects, especially for the activities of human-object interactions, are very common in practical applications. however, most of the existing approaches for 3d human shape and pose estimation require that human bodies are well captured without occlusions or with minor self-occlusions. in this paper, we focus on the problem of directly estimating the object-occluded human shape and pose from single color images. our key idea is to utilize a partial uv map to represent an object-occluded human body, and the full 3d human shape estimation is ultimately converted as an image inpainting problem. we propose a novel two-branch network architecture to train an end-to-end regressor via a latent distribution consistency, which also includes a novel visible feature sub-net to extract the human information from object-occluded color images. to supervise the network training, we further build a novel dataset named as 3doh50k. several experiments are conducted to reveal the effectiveness of the proposed method. experimental results demonstrate that the proposed method achieves state-of-the-art compared with previous methods. the dataset and codes are publicly available at https://www.yangangwang.com/papers/zhang-ooh-2020-03.html.",AB_0364
"we propose a simple yet effective multiple kernel clustering algorithm, termed simple multiple kernel k-means (simplemkkm). it extends the widely used supervised kernel alignment criterion to multi-kernel clustering. our criterion is given by an intractable minimization-maximization problem in the kernel coefficient and clustering partition matrix. to optimize it, we equivalently rewrite the minimization-maximization formulation as a minimization of an optimal value function, prove its differenentiablity, and design a reduced gradient descent algorithm to decrease it. furthermore, we prove that the resultant solution of simplemkkm is the global optimum. we theoretically analyze the performance of simplemkkm in terms of its clustering generalization error. after that, we develop extensive experiments to investigate the proposed simplemkkm from the perspectives of clustering accuracy, advantage on the formulation and optimization, variation of the learned consensus clustering matrix with iterations, clustering performance with varied number of samples and base kernels, analysis of the learned kernel weight, the running time and the global convergence. the experimental study demonstrates the effectiveness of the proposed simplemkkm by considerably and consistently outperforming state of the art multiple kernel clustering alternatives. in addition, the ablation study shows that the improved clustering performance is contributed by both the novel formulation and new optimization. our work provides a more effective approach to integrate multi-view data for clustering, and this could trigger novel research on multiple kernel clustering. the source code and data for simplemkkm are available at https://github.com/xinwangliu/simplemkkmcodes/.",AB_0364
"scene graph generation (sgg) is one of the hottest topics in computer vision and has attracted many interests since it provides rich semantic information between objects. in practice, the sgg datasets are often dual imbalanced, presented as a large number of backgrounds and rarely few foregrounds, and highly skewed foreground relationships categories (i.e., the long-tailed distribution). how to tackle this dual imbalanced problem is crucial but rarely studied in literature. existing methods only consider the long-tailed distribution of foregrounds classes and ignore the background-foreground imbalance in sgg, which results in a biased model and prevents it from being applied in the downstream tasks widely. to reduce its side effect and make the contributions of different categories equally, we propose a novel debiased sgg method (named dsdi) by incorporating biased resistance loss and causal intervention tree. we first deeply analyze the potential causes of dual imbalanced problem in sgg. then, to learn more discriminate representation of the foreground by expanding the foreground features space, the biased resistance loss decouples the background classification from foreground relationship recognition. meanwhile, a causal graph of content and context is designed to remove the context bias and learn unbiased relationship features via casual intervention tree. extensive experimental results on two extremely imbalanced datasets: vg150 and vrr-vg, demonstrate our dsdi outperforms other state-of-the-art methods. all our models will be available in https://github.com/zhouhao0515/unbiasedsgg-dsdi.",AB_0364
"spatial redundancy widely exists in visual recognition tasks, i.e., discriminative features in an image or video frame usually correspond to only a subset of pixels, while the remaining regions are irrelevant to the task at hand. therefore, static models which process all the pixels with an equal amount of computation result in considerable redundancy in terms of time and space consumption. in this paper, we formulate the image recognition problem as a sequential coarse-to-fine feature learning process, mimicking the human visual system. specifically, the proposed glance and focus network (gfnet) first extracts a quick global representation of the input image at a low resolution scale, and then strategically attends to a series of salient (small) regions to learn finer features. the sequential process naturally facilitates adaptive inference at test time, as it can be terminated once the model is sufficiently confident about its prediction, avoiding further redundant computation. it is worth noting that the problem of locating discriminant regions in our model is formulated as a reinforcement learning task, thus requiring no additional manual annotations other than classification labels. gfnet is general and flexible as it is compatible with any off-the-shelf backbone models (such as mobilenets, efficientnets and tsm), which can be conveniently deployed as the feature extractor. extensive experiments on a variety of image classification and video recognition tasks and with various backbone models demonstrate the remarkable efficiency of our method. for example, it reduces the average latency of the highly efficient mobilenet-v3 on an iphone xs max by 1.3x without sacrificing accuracy. code and pre-trained models are available at https://github.com/blackfeather-wang/gfnet-pytorch.",AB_0364
"in this article, we propose a unified framework to solve the following two challenging problems in incomplete multi-view representation learning: i) how to learn a consistent representation unifying different views, and ii) how to recover the missing views. to address the challenges, we provide an information theoretical framework under which the consistency learning and data recovery are treated as a whole. with the theoretical framework, we propose a novel objective function which jointly solves the aforementioned two problems and achieves a provable sufficient and minimal representation. in detail, the consistency learning is performed by maximizing the mutual information of different views through contrastive learning, and the missing views are recovered by minimizing the conditional entropy through dual prediction. to the best of our knowledge, this is one of the first works to theoretically unify the cross-view consistency learning and data recovery for representation learning. extensive experimental results show that the proposed method remarkably outperforms 20 competitive multi-view learning methods on six datasets in terms of clustering, classification, and human action recognition. the code could be accessed from https://pengxi.me.",AB_0364
"compressive learning (cl) is an emerging framework that integrates signal acquisition via compressed sensing (cs) and machine learning for inference tasks directly on a small number of measurements. it can be a promising alternative to classical image-domain methods and enjoys great advantages in memory saving and computational efficiency. however, previous attempts on cl are not only limited to a fixed cs ratio, which lacks flexibility, but also limited to mnist/cifar-like datasets and do not scale to complex real-world high-resolution (hr) data or vision tasks. in this article, a novel transformer-based compressive learning framework on large-scale images with arbitrary cs ratios, dubbed transcl, is proposed. specifically, transcl first utilizes the strategy of learnable block-based compressed sensing and proposes a flexible linear projection strategy to enable cl to be performed on large-scale images in an efficient block-by-block manner with arbitrary cs ratios. then, regarding cs measurements from all blocks as a sequence, a pure transformer-based backbone is deployed to perform vision tasks with various task-oriented heads. our sufficient analysis presents that transcl exhibits strong resistance to interference and robust adaptability to arbitrary cs ratios. extensive experiments for complex hr data demonstrate that the proposed transcl can achieve state-of-the-art performance in image classification and semantic segmentation tasks. in particular, transcl with a cs ratio of 10% can obtain almost the same performance as when operating directly on the original data and can still obtain satisfying performance even with an extremely low cs ratio of 1%. the source codes of our proposed transcl is available at https://github.com/mc-e/transcl/.",AB_0364
"conventional deep cnn-based segmentation approaches have achieved satisfactory performance in recent years, however, they are essentially big data-driven technologies and are difficult to generalize to unseen categories. few-shot segmentation is subsequently developed to perform pertinent operations in a low-data regime. unfortunately, due to the training paradigm and network architecture factors, existing methods are prone to overfit the targets of base categories and yield inaccurate segmentation boundaries, which impedes the research progress to some extent. in this paper, we propose a holistic prototype activation (hpa) network to alleviate these problems. its novel designs can be summarized in three aspects: 1) a training-free scheme to derive the prior representations of base categories. 2) prototype activation module (pam) that generates reliable activation maps and well-matched query features by filtering the objects of irrelevant classes with high confidence. 3) cross-referenced decoder (crd) for interacted feature reweighting and multi-level feature aggregation. extensive experiments on standard few-shot segmentation benchmarks (pascal-5(i) and coco-20(i)) verify the effectiveness of our method. on top of that, the superior performance on multiple extended tasks, such as weak-label segmentation, zero-shot segmentation, and video object segmentation, also illustrates its flexibility and versatility. our code is publicly available at https://github.com/chunbolang/hpa.",AB_0364
