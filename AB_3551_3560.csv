AB,NO
"the high intrinsic similarity between camouflaged objects and the background makes camouflaged object detection (cod) more challenging than traditional salient object detection (sod). existing deep-learning methods often fall into the following shortcomings: (1) when dealing with high-level features, it is difficult to fully and accurately extract semantic information, which is crucial for locating camouflaged targets. (2) the camouflaged targets identified by existing methods in complex scenes have rough boundaries and incomplete spatial information. (3) existing methods cannot effectively exert the uniqueness and complementarity of multiple features when integrating multiple features. to this end, we propose an attention-induced semantic and boundary interaction network for accurately identifying camouflaged objects. specifically, we propose a contrastive positioning module (cpm), which adopts a contrastive learning way to separate the camouflaged object from the background, thereby obtaining the exact location of the camouflaged target and retaining more semantic information. then, we design a boundary exploration module (bem), which can reduce background noise interference, focus on the structural details of camouflaged targets, and explore rich fine-grained spatial information. finally, we propose an attention-induced interaction module (aim) to fuse multivariate information (i.e., semantic location information, boundary information, and the side output information of the backbone), and multivariate information is complementarily fused under attention guidance to generate more powerful camouflaged target features. extensive experiments show that the proposed method on the different backbones (i.e., resnet-50, res2net-50, pvtv2-b2, efficientnet-b1) produces state-of-the-art results on several camouflaged benchmarks. the code is available https://github.com/zhangqiao970914/asbi.",AB_0356
"shadow detection aims to identify shadow regions from images, which plays a significant role in scene understanding. existing approaches tend to ignore the annotation noises in ground truths, which will be overfitted in the later training phase and potentially degrade detection performance. to alleviate the impact of such noisy labels, this work proposes a framework for robust shadow detection (rsd) by locating and correcting them. specifically, we first introduce a noise-rate blind sample selection scheme based on the prediction-level stability to identify the reliable parts from all pixel-level samples. next, we design a label correction strategy based on the graph convolutional network, which can propagate the label information between reliable and unreliable parts. finally, we enable subsequent robust learning by using a new training target with fewer noisy labels for each image. experimental results on public benchmarks (i.e., sbu, istd, ucf and cuhk-shadow) show that our method can be favorable against sotas. our source code is available at https://github.com/wuwen1994/rsd. (c) 2023 elsevier b.v. all rights reserved.",AB_0356
"the textual semantics contained in the plm (pre-trained language model) is constrained by the text distribution in the original training corpus. due to the lack of sufficient contextual training corpus, the low-frequency word representations in the plm often have difficulty capturing their actual semantics. previous research has shown that using semantic information from dictionaries can alleviate this problem. unfortunately, these works neglected the infinite potential of example sentences from different target words with various meanings. to re-explore methods for enhancing plm using the dictionary, we propose a novel comprehensive dictionary-based tuning approach integrating the latest prompt learning (dictprompt). we first collect a dataset based on the oxford advanced learner's english dictionary. then, we designed a set of comprehensive prompt templates with the corpus combining the word, the definition, and its example sentence. finally, we insert a word game training task between pre-training and fine-tuning using these templates, allowing the model to inject more semantic information into plm. we test our dictprompt tuning method on three commonly used plms. the testing results on five fine-grained semantic tasks show that our dictionary-based secondary tuning can bring additional gains to the model's performance. the best accuracy improves 3.09% on average with our tuning on the wic task and 7.93% on the wsc task. we also plot the sentence embedding scatters of polysemy words. our method can smooth the decision boundary and help the model output more distinguishable embedding. the code is available at https://github.com/xbdxwyh/dictprompt. (c) 2023 elsevier b.v. all rights reserved.",AB_0356
"in this paper, we introduce tive, a toolbox for identifying video instance segmentation errors. by directly operating output prediction files, tive defines isolated error types and weights each type's dam-age to map, for the purpose of distinguishing model characters. by decomposing localization quality in spatial-temporal dimensions, model's potential drawbacks on spatial segmentation and temporal asso-ciation can be revealed. tive can also report map over instance temporal length for real applications. we conduct extensive experiments by the toolbox to further illustrate how spatial segmentation and temporal association affect each other. we expect the analysis of tive can give the researchers more insights, guiding the community to promote more meaningful explorations for video instance segmenta-tion. the proposed toolbox is available at https://github.com/wenhe-jia/tive. (c) 2023 elsevier b.v. all rights reserved.",AB_0356
"visual question answering (vqa) have made stunning advances by exploiting transformer architecture and large-scale visual-linguistic pretraining. state-of-the-art methods generally require large amounts of data and devices to predict textualized answers and fail to provide visualized evidence of the answers. to mitigate these limitations, we propose a novel dual-decoder transformer network (ddtn) for pre-dicting the language answer and corresponding vision instance. specifically, the linguistic features are first embedded by long short-term memory (lstm) block and transformer encoder, which are shared between the transformer dual-decoder. then, we introduce object detector to obtain vision region fea-tures and grid features for reducing the size and cost of ddtn. these visual features are combined with the linguistic features and are respectively fed into two decoders. moreover, we design an in-stance query to guide the fused visual-linguistic features for outputting the instance mask or bounding box. the classification layers aggregate results from decoders and predict answer as well as correspond-ing instance coordinates at last. without bells and whistles, ddtn achieves state-of-the-art performance and even competitive to pretraining models on vizwizground and gqa dataset. the code is available at https://github.com/zlj63501/ddtn .(c) 2023 published by elsevier b.v.",AB_0356
"multi-scale learning frameworks have been regarded as a capable class of models to boost semantic segmentation. the problem nevertheless is not trivial especially for the real-world deployments, which often demand high efficiency in inference latency. in this paper, we thoroughly analyze the design of convolutional blocks (the type of convolutions and the number of channels in convolutions), and the ways of interactions across multiple scales, all from lightweight standpoint for semantic segmentation. with such in-depth comparisons, we conclude three principles, and accordingly devise lightweight and progressively-scalable networks (lps-net) that novelly expands the network complexity in a greedy manner. technically, lps-net first capitalizes on the principles to build a tiny network. then, lps-net progressively scales the tiny network to larger ones by expanding a single dimension (the number of convolutional blocks, the number of channels, or the input resolution) at one time to meet the best speed/accuracy tradeoff. extensive experiments conducted on three datasets consistently demonstrate the superiority of lps-net over several efficient semantic segmentation methods. more remarkably, our lps-net achieves 73.4% miou on cityscapes test set, with the speed of 413.5fps on an nvidia gtx 1080ti, leading to a performance improvement by 1.5% and a 65% speed-up against the state-of-the-art stdc. code is available at https://github.com/yihengzhang-cv/lps-net.",AB_0356
"the incremental learning paradigm in machine learning has consistently been a focus of academic research. it is similar to the way in which biological systems learn, and reduces energy consumption by avoiding excessive retraining. existing studies utilize the powerful feature extraction capabilities of pre-trained models to address incremental learning, but there remains a problem of insufficient utiliza-tion of neural network feature knowledge. to address this issue, this paper proposes a novel method called pre-trained model knowledge distillation (pmkd) which combines knowledge distillation of neu-ral network representations and replay. this paper designs a loss function based on centered kernel align-ment to transfer neural network representations knowledge from the pre-trained model to the incremental model layer-by-layer. additionally, the use of memory buffer for dark experience replay helps the model retain past knowledge better. experiments show that pmkd achieved superior perfor-mance on various datasets and different buffer sizes. compared to other methods, our class incremental learning accuracy reached the best performance. the open-source code is published athttps://github.-com/tiansongs/pmkd-il.(c) 2023 the author(s). published by elsevier b.v. this is an open access article under the cc by license ().",AB_0356
"traffic text detection is an important and meaningful research task as it can provide abundant semantic information for autonomous driving. although major breakthroughs have been achieved on conventional datasets, existing scene text detectors generally suffer from significant performance degradation in real-life traffic scenes since various extreme scenarios may cause a large domain shift. to realize robust traffic text detection under scene changes, we propose a novel network for cross-domain traffic text detection, which integrates both text detection and domain adaptation into one framework. in the text detection pipeline, we introduce a multigranularity text proposal network (mg-tpn) to generate fine-grained and coarse-grained text proposals, which could deeply interact during both training and inference stages, benefiting the pipeline in learning more robust text features and generating accurate detection results. to transfer the text detection ability from common scenes to unlabeled extreme traffic scenes, we propose an inter&intradomain adaptation (i2-da) strategy, which adequately excavates domain-invariant features between the source domain and target domain (interdomain), as well as multiple extreme scenarios of the target domain (intradomain). to the best of our knowledge, this is the first study on cross-domain text detection under extreme traffic scenes. extensive experiments on the traffic text datasets and standard benchmarks, including synthtext, visd, icdar2013 and icdar2015 validate the superiority of our method. the proposed two datasets (ctst and es-ctst) are available at https://github.com/pummi823/test.",AB_0356
"the severity of diseases develops gradually, and early screening is critical to apply timely medical inter-ventions. previous deep learning classification methods for disease grading have ignored the ordinal rela-tionships among stages of disease severity, but this study shows they can be used to boost disease -grading performance. in this paper, we design an ordinal regularized module to represent the orderliness in disease severity, which can be flexibly embedded into general classification networks to grade diseases more accurately. in addition, this ordinal regularized module also predicts the progress of disease devel-opment. the proposed method is evaluated on three public benchmark datasets: the idrid challenge dataset, lung nodule analysis 2016 (luna16) dataset, and messidor dataset. experiments show that the proposed method is not only superior to the baselines from common classification models but also outperforms deep learning approaches, especially on the idrid challenge dataset, where our method has a joint accuracy of 68.0%. furthermore, the proposed method achieves excellent performance in both single-disease and joint-disease grading tasks on the aforementioned datasets, and it can be applied to other disease-grading tasks. our code is publicly available at https://github.com/ahtwq/ornet. (c) 2023 published by elsevier b.v.",AB_0356
"in this paper, we target the precision visual tracking problem, which is the precise tracking of a target point on a moving object. compared with the abundant efforts in object-level tracking, which aims at accurately predicting the bounding box (or the mask) of the object, much less attention has been drawn to precision visual tracking. to this end, we present a template tracking framework that consists of two parts, template matching and template updating. for template matching, we trained a deep architecture to directly estimate the projective transformation that deforms the template to the search image. for template updating, we came up with a systematic strategy to update the initial and new templates. to avoid drift build-ups, we incorporate a fast-running dense correspondence matching module into the template update step. the proposed method was extensively tested on both synthetic and real data. to generate the synthetic data, we created a dataset of 480 image sequences. each image sequence comes with the trajectory ground truth of a target point moving across all the frames. we compared the proposed method with six comparative approaches on this dataset. the tracking accuracy achieved by the proposed method was as follows: around 44% of the tracking errors were less than one pixel, about 78% of them were less than three pixels, and only about 14% of them exceeded five pixels. the proposed method is fast, running at 14 frames-per-second (fps) on 960x540 image sequences on a workstation equipped with a nvidia rtx 2080ti graphic card. qualitative results also show that the proposed method is applicable to real-world image sequences. the related code, pre-trained models, and the test data will be made publicly available at https://github.com/xm-peng/precision-visual-tracking/.",AB_0356
