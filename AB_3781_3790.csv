AB,NO
"dynamic point cloud is a volumetric visual data representing realistic 3d scenes for virtual reality and augmented reality applications. however, its large data volume has been the bottleneck of data processing, transmission, and storage, which requires effective compression. in this paper, we propose a perceptually weighted rate-distortion optimization (pwrdo) scheme for video-based point cloud compression (v-pcc), which aims to minimize the perceptual distortion of reconstructed point cloud at the given bit rate. firstly, we propose a general framework of perceptually optimized v-pcc to exploit visual redundancies in point clouds. secondly, a multi-scale projection based point cloud quality metric (ppcm) is proposed to measure the perceptual quality of 3d point cloud. the ppcm model comprises 3d-to-2d patch projection, multi-scale structural distortion measurement, and fusion model. approximations and simplifications of the proposed ppcm are also presented for both v-pcc integration and low complexity. thirdly, based on the simplified ppcm model, we propose a pwrdo scheme with lagrange multiplier adaptation, which is incorporated into the v-pcc to enhance the coding efficiency. experimental results show that the proposed ppcm models can be used as standalone quality metrics, and they are able to achieve higher consistency with the human subjective scores than the state-of-the-art objective visual quality metrics. also, compared with the latest v-pcc reference model, the proposed pwrdo-based v-pcc scheme achieves an average bit rate reduction of 13.52%, 8.16%, 10.56% and 9.54%, respectively, in terms of four objective visual quality metrics for point clouds. it is significantly superior to the state-of-the-art coding algorithms. the computational complexity of the proposed pwrdo increases by 1.71% and 0.05% on average to the v-pcc encoder and decoder, respectively, which is negligible. the source codes of the ppcm and pwrdo schemes are available at https://github.com/vvcodec/ppcm-pwrdo.",AB_0379
"context modeling or multi-level feature fusion methods have been proved to be effective in improving semantic segmentation performance. however, they are not specialized to deal with the problems of pixel-context mismatch and spatial feature misalignment, and the high computational complexity hinders their widespread application in real-time scenarios. in this work, we propose a lightweight context and spatial feature calibration network (csfcn) to address the above issues with pooling-based and sampling-based attention mechanisms. csfcn contains two core modules: context feature calibration (cfc) module and spatial feature calibration (sfc) module. cfc adopts a cascaded pyramid pooling module to efficiently capture nested contexts, and then aggregates private contexts for each pixel based on pixel-context similarity to realize context feature calibration. sfc splits features into multiple groups of sub-features along the channel dimension and propagates sub-features therein by the learnable sampling to achieve spatial feature calibration. extensive experiments on the cityscapes and camvid datasets illustrate that our method achieves a state-of-the-art trade-off between speed and accuracy. concretely, our method achieves 78.7% miou with 70.0 fps and 77.8% miou with 179.2 fps on the cityscapes and camvid test sets, respectively. the code is available at https://nave.vr3i.com/ and https://github.com/kaigelee/csfcn.",AB_0379
"by exploring the localizable representations in deep cnn, weakly supervised object localization (wsol) methods could determine the position of the object in each image just trained by the classification task. however, the partial activation problem caused by the discriminant function makes the network unable to locate objects accurately. to alleviate this problem, we propose structure-preserved attention activated network (spa(2)net), a simple and effective one-stage wsol framework to explore the ability of structure preservation of deep features. different from traditional wsol approaches, we decouple the object localization task from the classification branch to reduce their mutual influence by involving a localization branch which is online refined by a self-supervised structural-preserved localization mask. specifically, we employ the high-order self-correlation as structural prior to enhance the perception of spatial interaction within convolutional features. by succinctly combining the structural prior with spatial attention, activations by spa(2)net will spread from part to the whole object during training. to avoid the structure-missing issue caused by the classification network, we furthermore utilize the restricted activation loss (ral) to distinguish the difference between foreground and background in the channel dimension. in conjunction with the self-supervised localization branch, spa(2)net can directly predict the class-irrelevant localization map while prompting the network to pay more attention to the target region for accurate localization. extensive experiments on two publicly available benchmarks, including cub-200-2011 and ilsvrc, show that our spa(2)net achieves substantial and consistent performance gains compared with baseline approaches. the code and models are available at https://github.com/msterdc/spa2net.",AB_0379
"color plays an important role in human visual perception, reflecting the spectrum of objects. however, the existing infrared and visible image fusion methods rarely explore how to handle multi-spectral/channel data directly and achieve high color fidelity. this paper addresses the above issue by proposing a novel method with diffusion models, termed as dif-fusion, to generate the distribution of the multi-channel input data, which increases the ability of multi-source information aggregation and the fidelity of colors. in specific, instead of converting multi-channel images into single-channel data in existing fusion methods, we create the multi-channel data distribution with a denoising network in a latent space with forward and reverse diffusion process. then, we use the the denoising network to extract the multi-channel diffusion features with both visible and infrared information. finally, we feed the multi-channel diffusion features to the multi-channel fusion module to directly generate the three-channel fused image. to retain the texture and intensity information, we propose multi-channel gradient loss and intensity loss. along with the current evaluation metrics for measuring texture and intensity fidelity, we introduce delta e as a new evaluation metric to quantify color fidelity. extensive experiments indicate that our method is more effective than other state-of-the-art image fusion methods, especially in color fidelity. the source code is available at https://github.com/geovectormatrix/dif-fusion.",AB_0379
"camera lenses often suffer from optical aberrations, causing radial distortion in the captured images. in those images, there exists a clear and general physical distortion model. however, in existing solutions, such rich geometric prior is under-utilized, and the formulation of an effective prediction target is under-explored. to this end, we introduce radial distortion transformer (rdtr), a new framework for radial distortion rectification. our rdtr includes a model-aware pre-training stage for distortion feature extraction and a deformation estimation stage for distortion rectification. technically, on the one hand, we formulate the general radial distortion (i.e., barrel distortion and pincushion distortion) in camera-captured images with a shared geometric distortion model and perform a unified model-aware pre-training for its learning. with the pre-training, the network is capable of encoding the specific distortion pattern of a radially distorted image. after that, we transfer the learned representations to the learning of distortion rectification. on the other hand, we introduce a new prediction target called backward warping flow for rectifying images with any resolution while avoiding image defects. extensive experiments are conducted on our synthetic dataset, and the results demonstrate that our method achieves state-of-the-art performance while operating in real-time. besides, we also validate the generalization of rdtr on real-world images. our source code and the proposed dataset are publicly available at https://github.com/wwd-ustc/rdtr.",AB_0379
"concepts, a collective term for meaningful words that correspond to objects, actions, and attributes, can act as an intermediary for video captioning. while many efforts have been made to augment video captioning with concepts, most methods suffer from limited precision of concept detection and insufficient utilization of concepts, which could provide caption generation with inaccurate and inadequate prior information. considering these issues, we propose a concept-aware video captioning framework (care) to facilitate plausible caption generation. based on the encoder-decoder structure, care detects concepts precisely via multimodal-driven concept detection (mcd) and offers sufficient prior information to caption generation by global-local semantic guidance (g-lsg). specifically, we implement mcd by leveraging video-to-text retrieval and the multimedia nature of videos. to achieve g-lsg, given the concept probabilities predicted by mcd, we weight and aggregate concepts to mine the video's latent topic to affect decoding globally and devise a simple yet efficient hybrid attention module to exploit concepts and video content to impact decoding locally. finally, to develop care, we emphasize on the knowledge transfer of a contrastive vision-language pre-trained model (i.e., clip) in terms of visual understanding and video-to-text retrieval. with the multi-role clip, care can outperform clip-based strong video captioning baselines with affordable extra parameter and inference latency costs. extensive experiments on msvd, msr-vtt, and vatex datasets demonstrate the versatility of our approach for different encoder-decoder networks and the superiority of care against state-of-the-art methods. our code is available at https://github.com/yangbang18/care.",AB_0379
"video frame interpolation (vfi) aims to generate predictive frames by motion-warping from bidirectional references. most examples of vfi utilize spatiotemporal semantic information to realize motion estimation and interpolation. however, due to variable acceleration, irregular movement trajectories, and camera movement in real-world cases, they can not be sufficient to deal with non-linear middle frame estimation. in this paper, we present a reformulation of the vfi as a joint non-linear motion regression (jnmr) strategy to model the complicated inter-frame motions. specifically, the motion trajectory between the target frame and multiple reference frames is regressed by a temporal concatenation of multi-stage quadratic models. then, a comprehensive joint distribution is constructed to connect all temporal motions. moreover, to reserve more contextual details for joint regression, the feature learning network is devised to explore clarified feature expressions with dense skip-connection. later, a coarse-to-fine synthesis enhancement module is utilized to learn visual dynamics at different resolutions with multi-scale textures. the experimental vfi results show the effectiveness and significant improvement of joint motion regression over the state-of-the-art methods. the code is available at https://github.com/ruhig6/jnmr.",AB_0379
"recently, learning-based multi-exposure fusion (mef) methods have made significant improvements. however, these methods mainly focus on static scenes and are prone to generate ghosting artifacts when tackling a more common scenario, i.e., the input images include motion, due to the lack of a benchmark dataset and solution for dynamic scenes. in this paper, we fill this gap by creating an mef dataset of dynamic scenes, which contains multi-exposure image sequences and their corresponding high-quality reference images. to construct such a dataset, we propose a 'static-for-dynamic' strategy to obtain multi-exposure sequences with motions and their corresponding reference images. to the best of our knowledge, this is the first mef dataset of dynamic scenes. correspondingly, we propose a deep dynamic mef (ddmef) framework to reconstruct a ghost-free high-quality image from only two differently exposed images of a dynamic scene. ddmef is achieved through two steps: pre-enhancement-based alignment and privilege-information-guided fusion. the former pre-enhances the input images before alignment, which helps to address the misalignments caused by the significant exposure difference. the latter introduces a privilege distillation scheme with an information attention transfer loss, which effectively improves the deghosting ability of the fusion network. extensive qualitative and quantitative experimental results show that the proposed method outperforms state-of-the-art dynamic mef methods. the source code and dataset are released at https://github.com/tx000/deep_dynamicmef.",AB_0379
"existing methods for salient object detection in optical remote sensing images (orsi-sod) mainly adopt convolutional neural networks (cnns) as the backbone, such as vgg and resnet. since cnns can only extract features within certain receptive fields, most orsi-sod methods generally follow the local-to-contextual paradigm. in this paper, we propose a novel global extraction local exploration network (gelenet) for orsi-sod following the global-to-local paradigm. specifically, gelenet first adopts a transformer backbone to generate four-level feature embeddings with global long-range dependencies. then, gelenet employs a direction-aware shuffle weighted spatial attention module (d-swsam) and its simplified version (swsam) to enhance local interactions, and a knowledge transfer module (ktm) to further enhance cross-level contextual interactions. d-swsam comprehensively perceives the orientation information in the lowest-level features through directional convolutions to adapt to various orientations of salient objects in orsis, and effectively enhances the details of salient objects with an improved attention mechanism. swsam discards the direction-aware part of d-swsam to focus on localizing salient objects in the highest-level features. ktm models the contextual correlation knowledge of two middle-level features of different scales based on the self-attention mechanism, and transfers the knowledge to the raw features to generate more discriminative features. finally, a saliency predictor is used to generate the saliency map based on the outputs of the above three modules. extensive experiments on three public datasets demonstrate that the proposed gelenet outperforms relevant state-of-the-art methods. the code and results of our method are available at https://github.com/mathlee/gelenet.",AB_0379
"benefiting from advances in few-shot learning techniques, their application to dense prediction tasks (e.g., segmentation) has also made great strides in the past few years. however, most existing few-shot segmentation (fss) approaches follow a similar pipeline to that of few-shot classification, where some core components are directly exploited regardless of various properties between tasks. we note that such an ill-conceived framework introduces unnecessary information loss, which is clearly unacceptable given the already very limited training sample. to this end, we delve into the typical types of information loss and provide a reasonably effective way, namely retain and recover (rare). the main focus of this paper can be summarized as follows: (i) the loss of spatial information due to global pooling; (ii) the loss of boundary information due to mask interpolation; (iii) the degradation of representational power due to sample averaging. accordingly, we propose a series of strategies to retain/recover the avoidable/unavoidable information, such as unidirectional pooling, error-prone region focusing, and adaptive integration. extensive experiments on two popular benchmarks (i.e.,pascal-5(i) and coco-20(i)) demonstrate the effectiveness of our scheme, which is not restricted to a particular baseline approach. the ultimate goal of our work is to address different information loss problems within a unified framework, and it also exhibits superior performance compared to other methods with similar motivations. the source code will be made available at https://github.com/chunbolang/rare.",AB_0379
