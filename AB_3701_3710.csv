AB,NO
"the research on attack transferability is of great importance as it can guide how to conduct an ad-versarial attack without knowing any information about target models. however, it remains challeng-ing for adversarial examples to maintain a good attack transferability performance, especially for the black-box attack implemented in the physical world. to enhance black-box transferability of physical at-tacks on object detectors, we present a novel adversarial learning method to produce adversarial patches by redistributing separable attention maps. concretely, we first develop smoothed multilayer attention maps by introducing serial composite transformations, which could suppress model-specific noise on the one hand, and cover objects to be concealed at various resolutions on the other hand. besides, our method resorts to a scalable mask to separate object attention from the background and adjust their distribution with a novel loss function. extensive experiments show that our approach outper-forms state-of-the-art methods in both the digital space and the physical world. our code is available at https://github.com/zhangyu13a/transphyatt .(c) 2023 elsevier ltd. all rights reserved.",AB_0371
"domain adaptive object detection (daod) aims to alleviate transfer performance degradation caused by the cross-domain discrepancy. however, most existing daod methods are dominated by outdated and computationally intensive two-stage faster r-cnn, which is not the first choice for industrial applications. in this paper, we propose a novel semi-supervised domain adaptive yolo (ssda-yolo) based method to improve cross-domain detection performance by integrating the compact one-stage stronger detector yolov5 with domain adaptation. specifically, we adapt the knowledge distillation framework with the mean teacher model to assist the student model in obtaining instance-level features of the unlabeled target domain. we also utilize the scene style transfer to cross-generate pseudo images in different domains for remedying image-level differences. in addition, an intuitive consistency loss is proposed to further align cross-domain predictions. we evaluate ssda-yolo on public benchmarks including pascalvoc, clipart1k, cityscapes, and foggy cityscapes. moreover, to verify its generalization, we conduct experiments on yawning detection datasets collected from various real classrooms. the results show considerable improvements of our method in these daod tasks, which reveals both the effectiveness of proposed adaptive modules and the urgency of applying more advanced detectors in daod. our code is available on https://github.com/hnuzhy/ssda-yolo.",AB_0371
"complicated underwater environments bring new challenges to object detection, such as unbalanced light conditions, low contrast, occlusion, and mimicry of aquatic organisms. under these circumstances, the objects captured by the underwater camera will become vague, and the generic detectors often fail on these vague objects. this work aims to solve the problem from two perspectives: uncertainty modeling and hard example mining. we propose a two-stage underwater detector named boosting r-cnn, which comprises three key components. first, a new region proposal network named retinarpn is proposed, which provides high-quality proposals and considers objectness and iou prediction for uncertainty to model the object prior probability. second, the probabilistic inference pipeline is introduced to combine the first-stage prior uncertainty and the second-stage classification score to model the final detection score. finally, we propose a new hard example mining method named boosting reweighting. specifically, when the region proposal network miscalculates the object prior probability for a sample, boosting reweighting will increase the classification loss of the sample in the r-cnn head during training, while reducing the loss of easy samples with accurately estimated priors. thus, a robust detection head in the second stage can be obtained. during the inference stage, the r-cnn has the capability to rectify the error of the first stage to improve the performance. comprehensive experiments on two underwater datasets and two generic object detection datasets demonstrate the effectiveness and robustness of our method. the link of code: https://github.com/mousecpn/boosting-r-cnn. (c) 2023 elsevier b.v. all rights reserved.",AB_0371
"in this paper, we study the local visual modeling with grid features for image captioning, which is critical for generating accurate and detailed captions. to achieve this target, we propose a locality-sensitive trans-former network (lstnet) with two novel designs, namely locality-sensitive attention (lsa) and locality-sensitive fusion (lsf). lsa is deployed for the intra-layer interaction in transformer via modeling the relationship between each grid and its neighbors. it reduces the difficulty of local object recognition during captioning. lsf is used for inter-layer information fusion, which aggregates the information of different encoder layers for cross-layer semantical complementarity. with these two novel designs, the proposed lstnet can model the local visual information of grid features to improve the captioning qual-ity. to validate lstnet, we conduct extensive experiments on the competitive ms-coco benchmark. the experimental results show that lstnet is not only capable of local visual modeling, but also outperforms a bunch of state-of-the-art captioning models on offline and online testings, i.e., 134.8 cider and 136.3 cider, respectively. besides, the generalization of lstnet is also verified on the flickr8k and flickr30k datasets. the source code is available on github: https://www.github.com/xmu-xiaoma666/lstnet .(c) 2023 elsevier ltd. all rights reserved.",AB_0371
"the increasing availability of remote sensing data allows dealing with spatial-spectral limitations by means of pan-sharpening methods. however, fusing inter-sensor data poses important challenges, in terms of resolution differences, sensor-dependent deformations and ground-truth data availability, that demand more accurate pan-sharpening solutions. in response, this paper proposes a novel deep learningbased pan-sharpening model which is termed as the double-u network for self-supervised pansharpening (w-netpan). in more details, the proposed architecture adopts an innovative w-shape that integrates two u-net segments which sequentially work for spatially matching and fusing inter-sensor multi-modal data. in this way, a synergic effect is produced where the first segment resolves intersensor deviations while stimulating the second one to achieve a more accurate data fusion. additionally, a joint loss formulation is proposed for effectively training the proposed model without external data supervision. the experimental comparison, conducted over four coupled sentinel-2 and sentinel-3 datasets, reveals the advantages of w-netpan with respect to several of the most important state-of-the-art pan-sharpening methods available in the literature. the codes related to this paper will be available at https://github.com/rufernan/wnetpan. (c) 2023 the author(s). published by elsevier b.v. this is an open access article under the cc by-nc-nd license ().",AB_0371
"change detection (cd) aims to identify changes that occur in an image pair taken different times. prior methods devise specific networks from scratch to predict change masks in pixel-level, and struggle with general segmentation problems. in this paper, we propose a new paradigm that reduces cd to semantic segmentation which means tailoring an existing and powerful semantic segmentation network to solve cd. this new paradigm conveniently enjoys the mainstream semantic segmentation techniques to deal with general segmentation problems in cd. hence we can concentrate on studying how to detect changes. we propose a novel and importance insight that different change types exist in cd and they should be learned separately. based on it, we devise a module named mtf to extract the change information and fuse temporal features. mtf enjoys high interpretability and reveals the essential characteristic of cd. and most segmentation networks can be adapted to solve the cd problems with our mtf module. finally, we propose c-3po, a network to detect changes at pixel-level. c-3po achieves state-of-the-art performance without bells and whistles. it is simple but effective and can be considered as a new baseline in this field. our code for c-3po is available at https://github.com/doctorkey/c-3po .(c) 2023 elsevier ltd. all rights reserved.",AB_0371
"emotional support conversation (esconv) aims to reduce help-seekers' emotional distress with a supportive strategy and response. it is essential for the supporter to select an appropriate strategy according to the feedback of the help-seeker (e.g., emotion change during dialog turns, etc.) in esconv. however, previous methods mainly rely on the dialog history to select the strategy and ignore the help-seeker's feedback, causing wrong and user-irrelevant strategy predictions. meanwhile, these methods only model the context-to-strategy flow but pay less attention to the strategy-to-context flow involving the strategy-related context for generating strategy-constrained responses. in this paper, a feedback -aware double controlling network (fado) is proposed to make a strategy schedule and generate supportive responses. the core modules in fado include a dual-level feedback strategy selector and a double control reader, where the former leverages the turn-level and conversation-level feedback to encourage or penalize strategies, and the latter constructs a novel strategy-to-context flow to generate strategy-constrain responses. besides, a strategy dictionary is designed to enrich the semantic information of the strategy and improve the quality of the strategy-constrained response. experimental results on esconv indicate that the proposed fado achieves sota performance in terms of strategy selection and response generation. our code is available at https://github.com/thedatababbler/fado. (c) 2023 elsevier b.v. all rights reserved.",AB_0371
"in the stereo matching task, the 3d convolution network can effectively aggregate the cost volume with the strong representation ability to model the spatial and depth dimensions but with the disadvantage of a high computational cost. in this letter, we revisit the 3d convolution network and its common variant, and then propose the depth shift module (dsm) to model the cost volume in the depth dimension which could imitate the 3d convolution function with the computational complexity of the 2d convolution. the proposed dsm is easy to extend to present 3d cost aggregation methods in stereo matching with less inference time, lower computational complexity, and minor precision loss. moreover, a novel compact but efficient stereo matching framework named hybridnet is proposed. this framework can hybridize the 2d convolution layer with the proposed dsm to effectively aggregate the cost volume. the proposed hybridnet achieves a better trade-off between the performance, computational complexity, and model size (e.g., 30% less than the size of aanet and 25% less than the size of psmnet) in public open-source datasets (e.g., scene flow and kitti stereo 2015). the relevant code is available at https://github.com/ ganwanshui/hybridnet .(c) 2023 published by elsevier b.v.",AB_0371
"color image denoising is frequently encountered in various image processing and computer vision tasks. one traditional strategy is to convert the rgb image to a less correlated color space and denoise each channel of the new space separately. however, such a strategy can not fully exploit the correlated information between channels and is inadequate to obtain satisfactory results. to address this issue, this paper proposes a new multi-channel optimization model for color image denoising under the nuclear norm minus frobenius norm minimization framework. specifically, based on the block-matching, the color image is decomposed into overlapping rgb patches. for each patch, we stack its similar neighbors to form the corresponding patch matrix. the proposed model is performed on the patch matrix to recover its noise-free version. during the recovery process, a) a weight matrix is introduced to fully utilize the noise difference between channels; b) the singular values are shrunk adaptively without additionally assigning weights. with them, the proposed model can achieve promising results while keeping simplicity. to solve the proposed model, an accurate and effective algorithm is built based on the alternating direction method of multipliers (admm) framework. the solution of each updating step can be analytically expressed in closed-from. rigorous theoretical analysis proves that the solution sequences generated by the proposed algorithm converge to their respective stationary points. experimental results on both synthetic and real noise data sets demonstrate the proposed model outperforms state-of-the-art models. matlab code is available at https://www.github.com/wangzhi-swu/mcnnfnm . (c) 2023 elsevier b.v. all rights reserved.",AB_0371
"flame detection is of great significance in a fire prevention system. yolov4 has poor real-time performance on flame detection caused by the complex structure and high parameter size. to address this problem, a novel flame detection framework, yolo for flame (yolo-f), is proposed in this paper. the backbone of yolov4 is simplified from the original 53 convolutional layers to 34 convolutional layers to reduce the number of parameters by simplifying the structure of the cspblock. based on the fpn, an effective and light-weight feature pyramid architecture, namely fpns-se, is then proposed and the neck part of yolov4 is replaced by fpns-se to enhance the feature extraction ability of different scales. in addition, the ciou loss in the yolov4 ignores the similarity measure of the area between the predicted bounding box and the ground-truth bounding box. an effective loss named aciou is proposed in this paper in order to handle the above issue and further improve the detection accuracy. the proposed methods are tested on flame dataset and network crawled dataset, respectively. the map, recall, and precision of yolo-f are higher by 2.01%, 4.0%, 2.0% on average than those of yolov4. with input size of 416x416 and on a single gtx 1660, the operating speed of our method can reach 24.53fps, which is improved by 38.04% compared with yolov4. the experimental results show that our method is more robust to the small flame and flame-like objects and can achieve the best balance of detection speed and accuracy. the code is made available at https://github.com/windxy/yolo-f.",AB_0371
