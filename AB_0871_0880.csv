AB,NO
"objective the use of thrombectomy alone (endovascular thrombectomy [evt]) was found to be noninferior to combi-nation therapy (evt plus intravenous thrombolysis [ivt] with alteplase before thrombectomy [evt+ivt]) in the direct -mt (direct intra-arterial thrombectomy in order to revascularize ais patients with large vessel occlusion efficiently in chinese tertiary hospitals: a multicenter randomized clinical trial), yet the economic value of omitting alteplase was unclear. thus, in this paper the authors assessed the cost-effectiveness of evt alone versus evt+ivt in the direct-mt. methods in the context of the chinese healthcare system, the authors conducted a post hoc economic analysis of the direct-mt based on an intention-to-treat approach. index stroke costs were collected at the individual level, while costs after discharge were complemented with published literature and government websites. utility weights assessed at 90 days using the 5-level eq-5d questionnaire were prospectively collected. for long-term modeled cost-effectiveness analysis, a markov model with 7 health states corresponding to the 7 modified rankin scale scores was used. determin-istic and probabilistic sensitivity analyses were performed. all costs are expressed in 2019 us dollars, discounted using the annual consumer price index in china. results during the index hospitalization, the mean medication cost in the evt-alone group was $487 lower than that in the evt+ivt group ($2453 [95% ci $2205-$2701] vs $2940 [95% ci $2703-$3178], p = 0.01), but the mean overall costs were similar between the groups ($15,565 [95% ci $14,876-$16,254] vs $15,472 [95% ci $14,714-$16,230], p = 0.73). within 90 days of the trial, there were no significant differences in total costs (difference -$222 [95% ci -$603 to $161], p = 0.06, bootstrapping) or utility values (median 0.84 [iqr 0.48-0.95] vs median 0.85 [iqr 0.26-1.00]; beta coefficient < 0.01 [95% ci -0.06 to 0.07]) between evt alone and evt+ivt. over the lifetime horizon, evt alone and evt+ivt yielded comparable lifetime qalys (2.02 qalys [95% ci -0.07 to 4.55 qalys] vs 1.90 qalys [95% ci -0.09 to 4.55 qalys]) and costs ($26,795 [95% ci $15,281-$54,463] vs $27,632 [95% ci $14,558-$52,251]). conclusions in this economic analysis based on a trial conducted in china, the authors found that evt alone was not associated with economic dominance over evt+ivt in patients with anterior circulation large-vessel occlusion. https://thejns.org/doi/abs/10.3171/2022.12.jns221791",AB_0088
"the next generation of internet services, such as metaverse, rely on mixed reality (mr) technology to provide immersive user experiences. however, limited computation power of mr headset-mounted devices (hmds) hinders the deployment of such services. therefore, we propose an efficient information-sharing scheme based on full-duplex device-to-device (d2d) semantic communications to address this issue. our approach enables users to avoid heavy and repetitive computational tasks, such as artificial intelligence-generated content (aigc) in the view images of all mr users. specifically, a user can transmit the generated content and semantic information extracted from their view image to nearby users, who can then use this information to obtain the spatial matching of computation results under their view images. we analyze the performance of full-duplex d2d communications, including the achievable rate and bit error probability, by using generalized small-scale fading models. to facilitate semantic information sharing among users, we design a contract theoretic ai-generated incentive mechanism. the proposed diffusion model generates the optimal contract design, outperforming two deep reinforcement learning algorithms, i.e., proximal policy optimization and soft actor-critic algorithms. our numerical analysis experiment proves the effectiveness of our proposed methods. the code for this paper is available at https://github.com/hongyangdu/semsharing.",AB_0088
"graphs are widely used to model various practical applications. in recent years, graph convolution networks (gcns) have attracted increasing attention due to the extension of convolution operation from traditional grid data to graph one. however, the representation ability of current gcns is undoubtedly limited because existing work fails to consider feature interactions. toward this end, we propose a dual feature interaction-based gcn. specifically, it models feature interaction in the aspects of 1) node features where we use newton's identity to extract different-order cross features implicit in the original features and design an attention mechanism to fuse them; and 2) graph convolution where we capture the pairwise interactions among nodes in the neighborhood to expand a weighted sum operation. we evaluate the proposed model with graph data from different fields, and the experimental results on semi-supervised node classification and link prediction demonstrate the effectiveness of the proposed gcn. the data and source codes of this work are available at https://github.com/zzy-graphmininglab/dfi-gcn.",AB_0088
"dynamic neural networks can greatly reduce computation redundancy without compromising accuracy by adapting their structures based on the input. in this paper, we explore the robustness of dynamic neural networks against energy-oriented attacks targeted at reducing their efficiency. specifically, we attack dynamic models with our novel algorithm gradmdm. gradmdm is a technique that adjusts the direction and the magnitude of the gradients to effectively find a small perturbation for each input, that will activate more computational units of dynamic models during inference. we evaluate gradmdm on multiple datasets and dynamic models, where it outperforms previous energy-oriented attack techniques, significantly increasing computation complexity while reducing the perceptibility of the perturbations https://github.com/lingengfoo/gradmdm.",AB_0088
"many real-world problems deal with collections of data with missing values, e.g., rna sequential analytics, image completion, video processing, etc. usually, such missing data is a serious impediment to a good learning achievement. existing methods tend to use a universal model for all incomplete data, resulting in a suboptimal model for each missingness pattern. in this paper, we present a general model for learning with incomplete data. the proposed model can be appropriately adjusted with different missingness patterns, alleviating competitions between data. our model is based on observable features only, so it does not incur errors from data imputation. we further introduce a low-rank constraint to promote the generalization ability of our model. analysis of the generalization error justifies our idea theoretically. in additional, a subgradient method is proposed to optimize our model with a proven convergence rate. experiments on different types of data show that our method compares favorably with typical imputation strategies and other state-of-the-art models for incomplete data. more importantly, our method can be seamlessly incorporated into the neural networks with the best results achieved. the source code is released at https://github.com/ys-gong/missingness-patterns.",AB_0088
"genome-wide association studies (gwas) excels at harnessing dense genomic variant datasets to identify candidate regions responsible for producing a given phenotype. however, gwas and traditional fine-mapping methods do not provide insight into the complex local landscape of linkage that contains and has been shaped by the causal variant(s). here, we present crosshap, an r package that performs robust density-based clustering of variants based on their linkage profiles to capture haplotype structures in a local genomic region of interest. following this, crosshap is equipped with visualization tools for choosing optimal clustering parameters (e) before producing an intuitive figure that provides an overview of the complex relationships between linked variants, haplotype combinations, phenotype, and metadata traits. availability and implementation: the crosshap package is freely available under the mit license and can be downloaded directly from cran with r >4.0.0. the development version is available on github alongside issue support (https://github.com/jacobimarsh/crosshap). tutorial vignettes and documentation are available (https://jacobimarsh.github.io/crosshap/).",AB_0088
"automatically generating a report from a patient's chest x-rays (cxrs) is a promising solution to reducing clinical workload and improving patient care. however, current cxr report generators -- which are predominantly encoder-to-decoder models -- lack the diagnostic accuracy to be deployed in a clinical setting. to improve cxr report generation, we investigate warm starting the encoder and decoder with recent open-source computer vision and natural language processing checkpoints, such as the vision transformer (vit) and pubmedbert. to this end, each checkpoint is evaluated on the mimic-cxr and iu x-ray datasets. our experimental investigation demonstrates that the convolutional vision transformer (cvt) imagenet-21k and the distilled generative pre-trained transformer 2 (distilgpt2) checkpoints are best for warm starting the encoder and decoder, respectively. compared to the state-of-the-art (m2 transformer progressive), cvt2distilgpt2 attained an improvement of 8.3\% for ce f-1, 1.8\% for bleu-4, 1.6\% for rouge-l, and 1.0\% for meteor. the reports generated by cvt2distilgpt2 have a higher similarity to radiologist reports than previous approaches. this indicates that leveraging warm starting improves cxr report generation. code and checkpoints for cvt2distilgpt2 are available at this https://github.com/achre/cvt2distiglgpt2.",AB_0088
"backgroundmountain biking and hiking continue to grow in popularity. with new participants to these sports, it is likely the number of injuries will increase. to assist medical personnel in the management of these patients we attempted to quantify the types and locations of injuries sustained by mountain bikers and hikers. objective the objective of this systematic review is to identify the type and anatomical location of injuries for both mountain bikers and hikers.methodsa systematic search was undertaken using cinahl, cochrane, proquest, pubmed and scopus databases. reviewers assessed the eligibility of articles by a title/abstract review and final full-text review. studies were included if the types of injuries were reported by medical personnel and contained anatomical locations. studies were excluded if it did not take place on a trail or if the injuries were self-reported. risk of bias was assessed utilising the joanna briggs institute (jbi) checklists for study quality. no meta-analysis or comparison between mountain bikers and hikers was possible due to the high heterogeneity of the definition of injury.resultsa total of 24 studies met the inclusion criteria, 17 covering mountain biking and 7 hiking. this represented 220,935 injured mountain bikers and 17,757 injured hikers. the most common type of injuries sustained by mountain bikers included contusions, abrasions and minor lacerations, which made up between 45-74% of reported injuries in studies on competitive racing and 8-67% in non-competitive studies. fractures represented between 1.5-43% of all reported injuries. the most injured region was the upper limbs reported in 10 of 17 studies. for hikers the most common injuries included blisters and ankle sprains with blisters representing 8-33% of all reported injuries. the most common body location to be injured by hikers was a lower limb in all 7 studies.conclusionsthis is the first systematic review to report on the injury epidemiology of the two most common trail users; mountain bikers and hikers. for participants in both activities the majority of injuries were of minor severity. despite this, the high proportions of upper limb fractures in mountain bikers and ankle sprains in hikers cannot be ignored.trial registration registration:this systematic review was prospectively registered with the university of york prospero database on the 12/4/2021 (crd42021229623) https://www.crd.york.ac.uk/prospero/display_record.php?id=crd42021229623.",AB_0088
"this article suggests a novel positive false discovery rate (pfdr) controlling method for testing gene-specific hypotheses using a gene-specific covariate variable, such as gene length. we suppose the null probability depends on the covariate variable. in this context, we propose a rejection rule that accounts for heterogeneity among tests by using two distinct types of null probabilities. we establish a pfdr estimator for a given rejection rule by following storey's q-value framework. a condition on a type 1 error posterior probability is provided that equivalently characterizes our rejection rule. we also present a suitable procedure for selecting a tuning parameter through cross-validation that maximizes the expected number of hypotheses declared significant. a simulation study demonstrates that our method is comparable to or better than existing methods across realistic scenarios. in data analysis, we find support for our method's premise that the null probability varies with a gene-specific covariate variable. availability and implementation: the source code repository is publicly available at https://github.com/hsjeon1217/conditional_method.",AB_0088
"motivation: efficiently aligning sequences is a fundamental problem in bioinformatics. many recent algorithms for computing alignments through smith-waterman-gotoh dynamic programming (dp) exploit single instruction multiple data (simd) operations on modern cpus for speed. however, these advances have largely ignored difficulties associated with efficiently handling complex scoring matrices or large gaps (insertions or deletions). results: we propose a new simd-accelerated algorithm called block aligner for aligning nucleotide and protein sequences against other sequences or position-specific scoring matrices. we introduce a new paradigm that uses blocks in the dp matrix that greedily shift, grow, and shrink. this approach allows regions of the dp matrix to be adaptively computed. our algorithm reaches over 5-10 times faster than some previous methods while incurring an error rate of less than 3% on protein and long read datasets, despite large gaps and low sequence identities. availability and implementation: our algorithm is implemented for global, local, and x-drop alignments. it is available as a rust library (with c bindings) at https://github.com/daniel-liu-c0deb0t/block-aligner.",AB_0088
