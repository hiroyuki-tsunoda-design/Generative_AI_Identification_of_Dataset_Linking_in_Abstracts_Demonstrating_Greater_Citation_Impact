AB,NO
"typical adversarial-training-based unsupervised domain adaptation (uda) methods are vulnerable when the source and target datasets are highly complex or exhibit a large discrepancy between their data distributions. recently, several lipschitz-constraint-based methods have been explored. the satisfaction of lipschitz continuity guarantees a remarkable performance on a target domain. however, they lack a mathematical analysis of why a lipschitz constraint is beneficial to uda and usually perform poorly on large-scale datasets. in this article, we take the principle of utilizing a lipschitz constraint further by discussing how it affects the error bound of uda. a connection between them is built, and an illustration of how lipschitzness reduces the error bound is presented. a local smooth discrepancy is defined to measure the lipschitzness of a target distribution in a pointwise way. when constructing a deep end-to-end model, to ensure the effectiveness and stability of uda, three critical factors are considered in our proposed optimization strategy, i.e., the sample amount of a target domain, dimension, and batchsize of samples. experimental results demonstrate that our model performs well on several standard benchmarks. our ablation study shows that the sample amount of a target domain, the dimension, and batchsize of samples, indeed, greatly impact lipschitz-constraint-based methods' ability to handle large-scale datasets. code is available at https://github.com/cuthbertcai/srda.",AB_0002
"by relabeling past experience with heuristic or curriculum goals, state-of-the-art reinforcement learning (rl) algorithms such as hindsight experience replay (her), hindsight goal generation (hgg), and graph-based hgg (g-hgg) have been able to solve challenging robotic manipulation tasks in multigoal settings with sparse rewards. hgg outperforms her in challenging tasks in which goals are difficult to explore by learning from a curriculum, in which intermediate goals are selected based on the euclidean distance to target goals. g-hgg enhances hgg by selecting intermediate goals from a precomputed graph representation of the environment, which enables its applicability in an environment with stationary obstacles. however, g-hgg is not applicable to manipulation tasks with dynamic obstacles, since its graph representation is only valid in static scenarios and fails to provide any correct information to guide the exploration. in this article, we propose bounding-box-based hgg (bbox-hgg), an extension of g-hgg selecting hindsight goals with the help of image observations of the environment, which makes it applicable to tasks with dynamic obstacles. we evaluate bbox-hgg on four challenging manipulation tasks, where significant enhancements in both sample efficiency and overall success rate are shown over state-of-the-art algorithms. the videos can be viewed at https://videoviewsite.wixsite.com/bbhgg.",AB_0002
"accurate object detection requires correct classification and high-quality localization. currently, most of the single shot detectors (ssds) conduct simultaneous classification and regression using a fully convolutional network. despite high efficiency, this structure has some inappropriate designs for accurate object detection. the first one is the mismatch of bounding box classification, where the classification results of the default bounding boxes are improperly treated as the results of the regressed bounding boxes during the inference. the second one is that only one-time regression is not good enough for high-quality object localization. to solve the problem of classification mismatch, we propose a novel reg-offset-cls (roc) module including three hierarchical steps: the regression of the default bounding box, the prediction of new feature sampling locations, and the classification of the regressed bounding box with more accurate features. for high-quality localization, we stack two roc modules together. the input of the second roc module is the output of the first roc module. in addition, we inject a feature enhanced (fe) module between two stacked roc modules to extract more contextual information. the experiments on three different datasets (i.e., ms coco, pascal voc, and uavdt) are performed to demonstrate the effectiveness and superiority of our method. without any bells or whistles, our proposed method outperforms state-of-the-art one-stage methods at a real-time speed. the source code is available at https://github.com/jialecao001/hsd.",AB_0002
"multiple kernel clustering (mkc) optimally utilizes a group of pre-specified base kernels to improve clustering performance. among existing mkc algorithms, the recently proposed late fusion mkc methods demonstrate promising clustering performance in various applications and enjoy considerable computational acceleration. however, we observe that the kernel partition learning and late fusion processes are separated from each other in the existing mechanism, which may lead to suboptimal solutions and adversely affect the clustering performance. in this article, we propose a novel late fusion multiple kernel clustering with proxy graph refinement (lfmkc-pgr) framework to address these issues. first, we theoretically revisit the connection between late fusion kernel base partition and traditional spectral embedding. based on this observation, we construct a proxy self-expressive graph from kernel base partitions. the proxy graph in return refines the individual kernel partitions and also captures partition relations in graph structure rather than simple linear transformation. we also provide theoretical connections and considerations between the proposed framework and the multiple kernel subspace clustering. an alternate algorithm with proved convergence is then developed to solve the resultant optimization problem. after that, extensive experiments are conducted on 12 multi-kernel benchmark datasets, and the results demonstrate the effectiveness of our proposed algorithm. the code of the proposed algorithm is publicly available at https://github.com/wangsiwei2010/graphlatefusion_mkc.",AB_0002
"fine-grained visual categorization (fgvc) is a challenging task because there are many hard examples existing between fine-grained classes which differ subtly in particular local regions. to address this issue, many methods have recourse to high-resolution source images and others adopt effective regularization like ``mixup'' or ``between class learning.'' despite their promising achievements, mixup tends to cause the manifold intrusion problem which would result in under-fitting and degradation of the model performance and high-resolution input inevitably leads to high computational costs. in view of this, we present a multiresolution discriminative mixup network (mrdmn). different from standard mixup, the proposed discriminative mixup strategy mixes discriminative regions linearly instead of entire images to avoid manifold intrusion, which makes it learn the local detail features more effectively and contributes to more precise categorization. furthermore, an innovative resolution-based distillation strategy is designed to transfer the multiresolution detail feature representations to a low-resolution network, which speeds up the testing and boosts the categorization accuracy simultaneously. extensive experiments demonstrate that our proposed mrdmn remarkably outperforms most competitive approaches with less computation time on the cub-200-2011, stanford-cars, stanford-dogs, food-101, and inaturalist 2017 datasets. the codes are in https://github.com/aztc/mrdmn.",AB_0002
"the decrease in the widths of spectral bands in hyperspectral imaging leads to a decrease in signal-to-noise ratio (snr) of measurements. the decreased snr reduces the reliability of measured features or information extracted from hyperspectral images (hsis). furthermore, the image degradations linked with various mechanisms also result in different types of noise, such as gaussian noise, impulse noise, deadlines, and stripes. this article introduces a fast and parameter-free hyperspectral image mixed noise removal method (termed fasthymix), which characterizes the complex distribution of mixed noise by using a gaussian mixture model and exploits two main characteristics of hyperspectral data, namely, low rankness in the spectral domain and high correlation in the spatial domain. the gaussian mixture model enables us to make a good estimation of gaussian noise intensity and the locations of sparse noise. the proposed method takes advantage of the low rankness using subspace representation and the spatial correlation of hsis by adding a powerful deep image prior, which is extracted from a neural denoising network. an exhaustive array of experiments and comparisons with state-of-the-art denoisers was carried out. the experimental results show significant improvement in both synthetic and real datasets. a matlab demo of this work is available at https://github.com/linazhuang for the sake of reproducibility.",AB_0002
"deep learning models have been able to generate rain-free images effectively, but the extension of these methods to complex rain conditions where rain streaks show various blurring degrees, shapes, and densities has remained an open problem. among the major challenges are the capacity to encode the rain streaks and the sheer difficulty of learning multi-scale context features that preserve both global color coherence and exactness of detail. to address the first problem, we design a non-local fusion module (nfm) and an attention fusion module (afm), and construct the multi-level pyramids' architecture to explore the local and global correlations of rain information from the rain image pyramid. more specifically, we apply the non-local operation to fully exploit the self-similarity of rain streaks and perform the fusion of multi-scale features along the image pyramid. to address the latter challenge, we additionally design a residual learning branch that is capable of adaptively bridging the gaps (e.g., texture and color information) between the predicted rain-free image and the clean background via a hybrid embedding representation. extensive results have demonstrated that our proposed method is able to generate much better rain-free images on several benchmark datasets than the state-of-the-art algorithms. moreover, we conduct the joint evaluation experiments with respect to deraining performance and the detection/segmentation accuracy to further verify the effectiveness of our deraining method for downstream vision tasks/applications. the source code is available at https://github.com/kuihua/mshfn.",AB_0002
"multitask joint learning technology continues gaining more attention as a paradigm shift and has shown promising performance in many applications. depth estimation and semantic understanding from monocular images emerge as a challenging problem in computer vision. while the other joint learning frameworks establish the relationship between the semantics and depth from stereo pairs, the lack of learning camera motion renders the frameworks that fail to model the geometric structure of the image scene. we make a further step in this article by proposing a multitask learning method, namely dpsnet, which can jointly perform depth and camera pose estimation and semantic scene segmentation. our core idea for depth and camera pose prediction is that we present the rigid semantic consistency loss to overcome the limitation of moving pixels from image reconstruction technology and further infer the segmentation of moving instances based on them. in addition, our proposed model performs semantic segmentation by reasoning the geometric correspondences between the pixel semantic outputs and the semantic labels at multiscale resolutions. experiments on open-source datasets and a video dataset captured on a micro-smart car show the effectiveness of each component of dpsnet, and dpsnet achieves state-of-the-art results in all three tasks compared with the best popular methods. all our models and code are available at https://github.com/jn-z/dpsnet: multitask learning using geometry reasoning for scene depth and semantics.",AB_0002
"domain adaptation techniques have been widely applied to the problem of cross-scene hyperspectral image (hsi) classification. most existing methods use convolutional neural networks (cnns) to extract statistical features from data and often neglect the potential topological structure information between different land cover classes. cnn-based approaches generally only model the local spatial relationships of the samples, which largely limits their ability to capture the nonlocal topological relationship that would better represent the underlying data structure of hsi. in order to make up for the above shortcomings, a topological structure and semantic information transfer network (tstnet) is developed. the method employs the graph structure to characterize topological relationships and the graph convolutional network (gcn) that is good at processing for cross-scene hsi classification. in the proposed tstnet, graph optimal transmission (got) is used to align topological relationships to assist distribution alignment between the source domain and the target domain based on the maximum mean difference (mmd). furthermore, subgraphs from the source domain and the target domain are dynamically constructed based on cnn features to take advantage of the discriminative capacity of cnn models that, in turn, improve the robustness of classification. in addition, to better characterize the correlation between distribution alignment and topological relationship alignment, a consistency constraint is enforced to integrate the output of cnn and gcn. experimental results on three cross-scene hsi datasets demonstrate that the proposed tstnet performs significantly better than some state-of-the-art domain-adaptive approaches. the codes will be available from the website: https://github.com/yuxiangzhang-bit/ieee_tnnls_tstnet.",AB_0002
"this paper proposes a forest fire detection framework using superpixel-based suspicious fire region proposal and light-weight convolutional neural network. the proposed methodology contains two main steps. in suspicious fire region proposal, we introduce a novel superpixel algorithm (scmm) driven by cauchy mixture model. then, the negative under-segmentation error (ue) of each superpixel is applied to inter-frame comparison for predicting varying superpixels. after that, by computing the features of motion superpixels using local difference binary (ldb) descriptor for two adjacent frames, the suspicious fire regions are localized. in following fire identification, to improve network performance while reducing computational complexity, this study presents a light-weight network architecture, called expanded squeeze-and-excitation shufflenet (ese-shufflenet). all suspicious fire regions are sent into this network to identify as either fire or non-fire included. experiments show that our framework performs well on fire detection tasks. code is available at http://www.imagetech-polynomials.com/ese-shuff.lenet.html.",AB_0002
