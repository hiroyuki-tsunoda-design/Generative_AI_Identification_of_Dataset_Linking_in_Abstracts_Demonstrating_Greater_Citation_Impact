AB,NO
"self-supervised learning enables networks to learn discriminative features from massive data itself. most state-of-the-art methods maximize the similarity between two augmentations of one image based on contrastive learning. by utilizing the consistency of two augmentations, the burden of manual annotations can be freed. contrastive learning exploits instance-level information to learn robust features. however, the learned information is probably confined to different views of the same instance. in this paper, we attempt to leverage the similarity between two distinct images to boost representation in self-supervised learning. in contrast to instance-level information, the similarity between two distinct images may provide more useful information. besides, we analyze the relation between similarity loss and feature-level cross-entropy loss. these two losses are essential for most deep learning methods. however, the relation between these two losses is not clear. similarity loss helps obtain instance-level representation, while feature-level cross-entropy loss helps mine the similarity between two distinct images. we provide theoretical analyses and experiments to show that a suitable combination of these two losses can get state-of-the-art results. code is available at https://github.com/guijiejie/iccl.",AB_0382
"saturation information in hazy images is conducive to effective haze removal, however, existing saturation-based dehazing methods just focus on the saturation value of each pixel itself, while the higher-level distribution characteristic between pixels regarding saturation remains to be harnessed. in this paper, we observe that the pixels, which share the same surface reflectance coefficient in the local patches of haze-free images, exhibit a linear relationship between their saturation component and the reciprocal of their brightness component in the corresponding hazy images normalized by atmospheric light. furthermore, the intercept of the line described by this linear relationship on the saturation axis is exactly the saturation value of these pixels in the haze-free images. using this characteristic of saturation, termed saturation line prior (slp), the transmission estimation is translated into the construction of saturation lines. accordingly, a new dehazing framework using slp is proposed, which employs the intrinsic relevance between pixels to achieve a reliable saturation line construction for transmission estimation. this approach can recover the fine details and attain realistic colors from hazy scenes, resulting in a remarkable visibility improvement. extensive experiments in real-world and synthetic hazy images show that the proposed method performs favorably against state-of-the-art dehazing methods. code is available on https://github.com/lpengyang/saturation-line-prior.",AB_0382
"benefiting from the intuitiveness and naturalness of sketch interaction, sketch-based video retrieval (sbvr) has received considerable attention in the video retrieval research area. however, most existing sbvr research still lacks the capability of accurate video retrieval with fine-grained scene content. to address this problem, in this paper we investigate a new task, which focuses on retrieving the target video by utilizing a fine-grained storyboard sketch depicting the scene layout and major foreground instances' visual characteristics (e.g., appearance, size, pose, etc.) of video; we call such a task fine-grained scene-level sbvr. the most challenging issue in this task is how to perform scene-level cross-modal alignment between sketch and video. our solution consists of two parts. first, we construct a scene-level sketch-video dataset called sketchvideo, in which sketch-video pairs are provided and each pair contains a clip-level storyboard sketch and several keyframe sketches (corresponding to video frames). second, we propose a novel deep learning architecture called sketch query graph convolutional network (sq-gcn). in sq-gcn, we first adaptively sample the video frames to improve video encoding efficiency, and then construct appearance and category graphs to jointly model visual and semantic alignment between sketch and video. experiments show that our fine-grained scene-level sbvr framework with sq-gcn architecture outperforms the state-of-the-art fine-grained retrieval methods. the sketchvideo dataset and sq-gcn code are available in the project webpage https://iscas-mmsketch.github.io/fg-sl-sbvr/.",AB_0382
"in 3d face reconstruction, orthogonal projection has been widely employed to substitute perspective projection to simplify the fitting process. this approximation performs well when the distance between camera and face is far enough. however, in some scenarios that the face is very close to camera or moving along the camera axis, the methods suffer from the inaccurate reconstruction and unstable temporal fitting due to the distortion under the perspective projection. in this paper, we aim to address the problem of single-image 3d face reconstruction under perspective projection. specifically, a deep neural network, perspective network (perspnet), is proposed to simultaneously reconstruct 3d face shape in canonical space and learn the correspondence between 2d pixels and 3d points, by which the 6dof (6 degrees of freedom) face pose can be estimated to represent perspective projection. besides, we contribute a large arkitface dataset to enable the training and evaluation of 3d face reconstruction solutions under the scenarios of perspective projection, which has 902,724 2d facial images with ground-truth 3d face mesh and annotated 6dof pose parameters. experimental results show that our approach outperforms current state-of-the-art methods by a significant margin. the code and data are available at https://github.com/cbsropenproject/6dof_face.",AB_0382
"not everybody can be equipped with professional photography skills and sufficient shooting time, and there can be some tilts in the captured images occasionally. in this paper, we propose a new and practical task, named rotation correction, to automatically correct the tilt with high content fidelity in the condition that the rotated angle is unknown. this task can be easily integrated into image editing applications, allowing users to correct the rotated images without any manual operations. to this end, we leverage a neural network to predict the optical flows that can warp the tilted images to be perceptually horizontal. nevertheless, the pixel-wise optical flow estimation from a single image is severely unstable, especially in large-angle tilted images. to enhance its robustness, we propose a simple but effective prediction strategy to form a robust elastic warp. particularly, we first regress the mesh deformation that can be transformed into robust initial optical flows. then we estimate residual optical flows to facilitate our network the flexibility of pixel-wise deformation, further correcting the details of the tilted images. to establish an evaluation benchmark and train the learning framework, a comprehensive rotation correction dataset is presented with a large diversity in scenes and rotated angles. extensive experiments demonstrate that even in the absence of the angle prior, our algorithm can outperform other state-of-the-art solutions requiring this prior. the code and dataset are available at https://github.com/nie-lang/rotationcorrection.",AB_0382
"the light absorption and scattering of underwater impurities lead to poor underwater imaging quality. the existing data-driven based underwater image enhancement (uie) techniques suffer from the lack of a large-scale dataset containing various underwater scenes and high-fidelity reference images. besides, the inconsistent attenuation in different color channels and space areas is not fully considered for boosted enhancement. in this work, we built a large scale underwater image (lsui) dataset, which covers more abundant underwater scenes and better visual quality reference images than existing underwater datasets. the dataset contains 4279 real-world underwater image groups, in which each raw image's clear reference images, semantic segmentation map and medium transmission map are paired correspondingly. we also reported an u-shape transformer network where the transformer model is for the first time introduced to the uie task. the u-shape transformer is integrated with a channel-wise multi-scale feature fusion transformer (cmsfft) module and a spatial-wise global feature modeling transformer (sgfmt) module specially designed for uie task, which reinforce the network's attention to the color channels and space areas with more serious attenuation. meanwhile, in order to further improve the contrast and saturation, a novel loss function combining rgb, lab and lch color spaces is designed following the human vision principle. the extensive experiments on available datasets validate the state-of-the-art performance of the reported technique with more than 2db superiority. the dataset and demo code are available at https://bianlab.github.io/.",AB_0382
"known as a hard nut, the single-model transferable targeted attacks via decision-level optimization objectives have attracted much attention among scholars for a long time. on this topic, recent works devoted themselves to designing new optimization objectives. in contrast, we take a closer look at the intrinsic problems in three commonly adopted optimization objectives, and propose two simple yet effective methods in this paper to mitigate these intrinsic problems. specifically, inspired by the basic idea of adversarial learning, we, for the first time, propose a unified adversarial optimization scheme (aos) to release both the problems of gradient vanishing in cross-entropy loss and gradient amplification in po+trip loss, and indicate that our aos, a simple transformation on the output logits before passing them to the objective functions, can yield considerable improvements on the targeted transferability. besides, we make a further clarification on the preliminary conjecture in vanilla logit loss (vll) and point out the problem of unbalanced optimization in vll, in which the source logit may risk getting increased without the explicit suppression on it, leading to the low transferability. then, the balanced logit loss (bll) is further proposed, where we take both the source logit and the target logit into account. comprehensive validations witness the compatibility and the effectiveness of the proposed methods across most attack frameworks, and their effectiveness can also span two tough cases (i.e., the low-ranked transfer scenario and the transfer to defense methods) and three datasets (i.e., the imagenet, cifar-10, and cifar-100). our source code is available at https://github.com/xuxiangsun/dllttaa.",AB_0382
"weakly supervised semantic segmentation (wsss) models relying on class activation maps (cams) have achieved desirable performance comparing to the non-cams-based counterparts. however, to guarantee wsss task feasible, we need to generate pseudo labels by expanding the seeds from cams which is complex and time-consuming, thus hindering the design of efficient end-to-end (single-stage) wsss approaches. to tackle the above dilemma, we resort to the off-the-shelf and readily accessible saliency maps for directly obtaining pseudo labels given the image-level class labels. nevertheless, the salient regions may contain noisy labels and cannot seamlessly fit the target objects, and saliency maps can only be approximated as pseudo labels for simple images containing single-class objects. as such, the achieved segmentation model with these simple images cannot generalize well to the complex images containing multi-class objects. to this end, we propose an end-to-end multi-granularity denoising and bidirectional alignment (mdba) model, to alleviate the noisy label and multi-class generalization issues. specifically, we propose the online noise filtering and progressive noise detection modules to tackle image-level and pixel-level noise, respectively. moreover, a bidirectional alignment mechanism is proposed to reduce the data distribution gap at both input and output space with simple-to-complex image synthesis and complex-to-simple adversarial learning. mdba can reach the miou of 69.5% and 70.2% on validation and test sets for the pascal voc 2012 dataset. the source codes and models have been made available at https://github.com/nust-machineintelligence-laboratory/mdba.",AB_0382
"in recent years, various neural network architectures for computer vision have been devised, such as the visual transformer and multilayer perceptron (mlp). a transformer based on an attention mechanism can outperform a traditional convolutional neural network. compared with the convolutional neural network and transformer, the mlp introduces less inductive bias and achieves stronger generalization. in addition, a transformer shows an exponential increase in the inference, training, and debugging times. considering a wave function representation, we propose the wavenet architecture that adopts a novel vision task-oriented wavelet-based mlp for feature extraction to perform salient object detection in rgb (red-green-blue)-thermal infrared images. in addition, we apply knowledge distillation to a transformer as an advanced teacher network to acquire rich semantic and geometric information and guide wavenet learning with this information. following the shortestpath concept, we adopt the kullback-leibler distance as a regularization term for the rgb features to be as similar to the thermal infrared features as possible. the discrete wavelet transform allows for the examination of frequency-domain features in a local time domain and time-domain features in a local frequency domain. we apply this representation ability to perform cross-modality feature fusion. specifically, we introduce a progressively cascaded sine-cosine module for cross-layer feature fusion and use low-level features to obtain clear boundaries of salient objects through the mlp. results from extensive experiments indicate that the proposed wavenet achieves impressive performance on benchmark rgb-thermal infrared datasets. the results and code are publicly available at https://github.com/nowander/wavenet.",AB_0382
"both salient object detection (sod) and camouflaged object detection (cod) are typical object segmentation tasks. they are intuitively contradictory, but are intrinsically related. in this paper, we explore the relationship between sod and cod, and then borrow successful sod models to detect camouflaged objects to save the design cost of cod models. the core insight is that both sod and cod leverage two aspects of information: object semantic representations for distinguishing object and background, and context attributes that decide object category. specifically, we start by decoupling context attributes and object semantic representations from both sod and cod datasets through designing a novel decoupling framework with triple measure constraints. then, we transfer saliency context attributes to the camouflaged images through introducing an attribute transfer network. the generated weakly camouflaged images can bridge the context attribute gap between sod and cod, thereby improving the sod models' performances on cod datasets. comprehensive experiments on three widely-used cod datasets verify the ability of the proposed method. code and model are available at: https://github.com/wdzhao123/sat.",AB_0382
