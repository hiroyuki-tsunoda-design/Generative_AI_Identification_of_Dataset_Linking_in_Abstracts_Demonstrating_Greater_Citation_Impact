AB,NO
"nonnegative matrix factorization is a data analysis method to discover parts-based, linear representations of data. it has been successfully used in a great variety of applications. deep nonnegative matrix factorization (deep nmf) was recently established to cope with the extraction of hierarchical latent feature representation, and it has been demonstrated to achieve outstanding results in unsupervised representation learning. however, defining a suitable regularization for the deep models is a key challenge, and the existing deep nmf approaches lack a well-suited regularization. in this paper, we propose the deep autoencoder-like nmf with contrastive regularization and feature relationship preservation (danmf-crfr) to address the above problem. inspired by contrastive learning, this deep model is able to learn discriminative and instructive deep features while adequately enforcing the local and global structures of the data to its decoder and encoder components. meanwhile, danmf-crfr also imposes feature correlations on the basis matrices during feature learning to improve part-based learning capabilities. multiplicative updating rules and convergence guarantees are also provided. extensive experimental results demonstrate the advantages of the proposed model. the source code for reproducing our results can be found at https://github.com/navidsalahian/danmf_crfr.",AB_0059
"machine learning algorithms spend a lot of time processing data because they are not fast enough to commit huge data sets. instance selection algorithms especially aim to tackle this trouble. however, even instance selection algorithms can suffer from it. we propose a new unsupervised instance selection algorithm based on conjectural hyper-rectangles. in this study, the proposed algorithm is compared with one conventional and four state-of-the-art instance selection algorithms by using fifty-five data sets from different domains. the experimental results demonstrate the supremacy of the proposed algorithm in terms of classification accuracy, reduction rate, and running time. the time and space complexities of the proposed algorithm are log-linear and linear, respectively. furthermore, the proposed algorithm can obtain better results with an accuracy-reduction trade-off without decreasing reduction rates extremely. the source code of the proposed algorithm and the data sets are available at https://github.com/fatihaydin1/nis for computational reproducibility.",AB_0059
"purpose - in parallel with technological developments, mobile devices have become an important part of our daily lives. nowadays, people, particularly generation z, actively engage with the internet and mobile technologies, including smartphones. the new technologies have also made electronic learning (e-learning) and mobile learning (m-learning) tempting for learners in higher education. despite the growing use of m-learning, particularly in developed countries, its utilization in higher education is still at its early stage and far from its potential. this study investigates the university students' acceptance of using m-learning with smartphones in a developing country context. design/methodology/approach- quantitative data collected through conducting questionnaires with 405 higher education students in turkey were analyzed with structural equation modelling. findings - the findings show that the acceptance of using m-learning is affected by several factors, including perceived enjoyment, complexity and facilitating conditions. the findings are particularly relevant in the current context, where the covid-19 pandemic has pushed many higher education institutions to adapt m-learning systems to enhance student learning experiences. originality/value - this study's findings offer fresh and important insights that can be used by m-learning developers and educators for designing m-learning systems and using m-learning applications in enhancing students' experience and performance with m-learning. peer review - the peer review history for this article is available at: https://publons.com/publon/10.1108/oir-10-2021-0516.",AB_0059
"the presented software package is an advanced analogue of the famous hbook and, in part, root packages. the main features of the package are as follows. standard operations (accumulation, simulation, transformation) are extended up to 5d objects. a new type of transformations of objects has been introduced, the x-transformation, which includes convolution of distributions. objects are accessed mainly by alphabetical name. the formation of the object space is carried out using a data file, where the user can choose the form of setting attributes. automatic adjustment of object attributes is possible (only the number of channels is set). the number of channels for each of the axes of the object is unlimited. operations between two objects (addition, subtraction, etc.) are possible with mismatched attributes. data output is carried out in two forms, for graphics and for fit. group operations are provided for visualization, outputting files for graphics. all programmes are written in fortran-90. the investigation has been performed at the veksler and baldin laboratory of high energy physics, jinr. program summary program title: nora cpc library link to program files: https://doi.org/10.17632/k363z3kp3x.1 licensing provisions: gplv3 programming language: fortran-90 nature of problem: creation and transformation of statistical objects, preparing graphics for publication. solution method: statistical analysis and data reconfiguration. (c) 2022 elsevier b.v. all rights reserved.",AB_0059
"we present the kkmcee 5.00.2 monte carlo event generator for lepton and quark pair production for the high energy electron-positron annihilation process. it is still the most sophisticated event generator for such processes. its entire source code is re-written in the modern c++ language. it reproduces all features of the older kkmc code in fortran 77. however, a number of improvements in the monte carlo algorithm are also implemented. most importantly, it is intended to be a starting point for the future improvements, which will be mandatory for the future high precision lepton collider projects. as in the older version, in addition to higher order qed corrections, it includes so-called o(alpha(1.5)) genuine weak corrections using a version of the classic dizet library and polarized tau decays using tauola program. both dizet and tauola external libraries are still in fortran 77. in addition, a hepmc3 interface to other mc programs, like parton showers and detector simulation, replaces the older hepevt interface. the hepmc3 interface is also exploited in the implementation of the additional photon final state emissions in tau decays using an external photos library rewritten in c. program summary program title: kkmcee 5.00.2 cpc library link to program files: https://doi.org/10.17632/7drvvhbw92.1 licensing provisions: gpl-3.0 programming languages: c++, fortran77 external routines: cern root library, photos, hepmc v.3.0, tauola, photos, foam, hepmc3 nature of the problem: fermion pair production is and will be used as an important data source for precise tests of the standard electroweak theory at a high luminosity future circular collider near the z resonance and above and/or at the future linear lepton colliders of higher energies than those at lep. the qed corrections to fermion pair production (especially tau leptons) have to be known to at least second order, including spin polarization effects, with 4-5 digit precision. the standard model predictions at the sub-permille precision level, taking into account multiple emission of photons for realistic experimental acceptances, can only be obtained using a monte carlo event generator. the realistic and precise simulation of tau lepton decays taking into account spin effects is an indispensable ingredient in the monte carlo event generator for the fermion pair production process. solution method: monte carlo methods are used to simulate most of the two-fermion final-state processes in e(+)e(-) collisions in the presence of multiphoton initial and final state radiation. the multiphoton effects are described in the framework of coherent exclusive exponentiation (ceex) extending/upgrading the older yennie-frautschi-suura exclusive exponentiation (eex) scheme. ceex treats correctly to infinite",AB_0059
"extensive in vitro cancer drug screening datasets have enabled scientists to identify biomarkers and develop machine learning models for predicting drug sensitivity. while most advancements have focused on omics profiles, cancer drug sensitivity scores pre-calculated by the original sources are often used as-is, without consideration for variabilities between studies. it is well-known that significant inconsistencies exist between the drug sensitivity scores across datasets due to differences in experimental setups and preprocessing methods used to obtain the sensitivity scores. as a result, many studies opt to focus only on a single dataset, leading to underutilization of available data and a limited interpretation of cancer pharmacogenomics analysis. to overcome these caveats, we have developed creammist (https://creammist.mtms.dev), an integrative database that enables users to obtain an integrative dose-response curve, to capture uncertainty (or high certainty when multiple datasets well align) across five widely used cancer cell-line drug-response datasets. we utilized the bayesian framework to systematically integrate all available dose-response values across datasets (>14 millions dose-response data points). creammist provides easy-to-use statistics derived from the integrative dose-response curves for various downstream analyses such as identifying biomarkers, selecting drug concentrations for experiments, and training robust machine learning models.",AB_0059
"the most common and aggressive malignant brain tumor in adults is glioma, which leads to short life expectancy. a reliable and efficient automatic segmentation method is beneficial for clinical practice. deep neural networks have achieved great success in brain tumor segmentation recently. however, their computational complexity and storage costs hinder their deployment in real-time applications or on resource-constrained devices in clinics. network pruning, which has attracted many researchers recently, alleviates this limitation by removing trivial parameters of the network. network pruning is challenging in the medical field because pruning should not degrade the performance of models. as a result, it is vital to choose unimportant parts of networks correctly. in this paper, we employ the genetic algorithm to identify redundant filters of our u-net-based network. we consider filter pruning a multiobjective optimization problem in which performance and inference time are optimized simultaneously. then, we use our compressed network for brain tumor segmentation. predicted segmentation masks are often used to predict patients' survival time. although several studies have recently achieved good results, they require different feature engineering techniques to extract suitable features, which is difficult and time-consuming. to tackle this problem, we easily extract deep features from the endpoint of the encoder of our compact network and use them for survival prediction. regarding the popularity of u-net-based models for brain tumor segmentation, many researchers can employ our technique to predict the survival time without spending lots of time on feature engineering. the experimental results on the brats 2018 dataset demonstrate that filter pruning is a reliable technique to reduce the storage cost and accelerate the network during inference while maintaining performance. furthermore, our survival time prediction technique achieves high efficiency compared to state-of-the-art methods. preprocessed data, the full implementation of the project, and the trained networks are available at https://github.com/fbehrad/evo_conv.",AB_0059
"the mapping of the time-dependent evolution of the human brain connectivity using longitudinal and multimodal neuroimaging datasets provides insights into the development of neurological disorders and the way they alter the brain morphology, structure and function over time. recently, the connectional brain template (cbt) was introduced as a compact representation integrating a population of brain multigraphs, where two brain regions can have multiple connections, into a single graph. given a population of brain multigraphs observed at a baseline timepoint t1, we aim to learn how to predict the evolution of the population cbt at follow-up timepoints t > t1. such model will allow us to foresee the evolution of the connectivity patterns of healthy and disordered individuals at the population level. here we present recurrent multigraph integrator network (remi-net?) to forecast population templates at consecutive timepoints from a given single timepoint. in particular, we unprecedentedly design a graph neural network architecture to model the changes in the brain multigraph and identify the biomarkers that differentiate between the typical and atypical populations. addressing such issues is of paramount importance in diagnosing neurodegenerative disorders at early stages and promoting new clinical studies based on the pinned-down biomarker brain regions or connectivities. in this paper, we demonstrate the design and use of the remi-net? model, which learns both the multigraph node level and time level dependencies concurrently. thanks to its novel graph convolutional design and normalization layers, remi-net? predicts well-centered, discriminative, and topologically sound connectional templates over time. additionally, the results show that our model outperforms all benchmarks and state-of-the-art methods by comparing and discovering the atypical connectivity alterations over time. our remi-net? code is available on github at https://github.com/basiralab/remi-net-star.",AB_0059
"although convolutional neural networks (cnns) are widely used in image classification tasks and have demonstrated promising classification accuracy results, designing a cnn architecture requires a manual adjustment of parameters through a series of experiments as well as sufficient knowledge both in the problem domain and cnn architecture design. therefore, it is difficult for users without prior experience to design a cnn for specific purposes. in this paper, we propose a framework for the automatic construction of cnn architectures based on resnet, densenet, and inception blocks and the roulette wheel selection method with a dynamic learning rate. compared with the state of the art, the proposed approach has a significant improvement in the domain of image classification. experimental evaluation of our approach including a comparison with the previous works on three benchmark datasets demonstrates the effectiveness of the overall method. the proposed algorithm not only improves the previous algorithm but also keeps the advantages of automatic cnn construction without requiring manual interventions. the source code of the our framework can be found at https://github.com/ btogzhan2000/ea-cnn-complab.",AB_0059
"we suggest an effective approach for the semi-automated segmentation of biomedical images according to their patchiness based on local edge density estimation. our approach does not require any preliminary learning or tuning, although a couple of free parameters directly controllable by the end user adjust the analysis resolution and sensitivity, respectively. we show explicitly that the local edge density exhibits excellent correlations with the cell monolayer density obtained by manual domain-expert based assessment, characterized by correlation coefficients rho > 0.97. our results indicate that the proposed algorithm is capable of an efficient segmentation and quantification of patchy areas in various biomedical microscopic images. in particular, the proposed algorithm achieves 95 to 99% median accuracy in the segmentation of image areas covered by the cell monolayer in an in vitro scratch assay. moreover, the proposed algorithm effectively distinguishes between the native and regenerated tissue fragments in microscopic images of histological sections, indicated by nearly three-fold discrepancy between the local edge densities in the corresponding image areas. we believe that the local edge density estimate could be further applicable as a surrogate image channel characterizing its patchiness either as a substitute or as a complementary source to the conventional cell-or tissue-specific fluorescent staining, in some cases either avoiding or limiting the use of complex experimental protocols. we implemented a simple open-source software tool with for on-the-fly visualization allowing for a straightforward feedback by a domain expert without any specific expertise in image analysis techniques. our tool is freely available online at https://gitlab.com/digiratory/biomedimaging/bcanalyzer.",AB_0059
