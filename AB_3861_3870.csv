AB,NO
"the goal of few-shot image recognition (fsir) is to identify novel categories with a small number of annotated samples by exploiting transferable knowledge from training data (base categories). most current studies assume that the transferable knowledge can be well used to identify novel categories. however, such transferable capability may be impacted by the dataset bias, and this problem has rarely been investigated before. besides, most of few-shot learning methods are biased to different datasets, which is also an important issue that needs to be investigated deeply. in this paper, we first investigate the impact of transferable capabilities learned from base categories. specifically, we use the relevance to measure relationships between base categories and novel categories. distributions of base categories are depicted via the instance density and category diversity. the fsir model learns better transferable knowledge from relevant training data. in the relevant data, dense instances or diverse categories can further enrich the learned knowledge. experimental results on different sub-datasets of imagenet demonstrate category relevance, instance density and category diversity can depict transferable bias from distributions of base categories. second, we investigate performance differences on different datasets from the aspects of dataset structures and different few-shot learning methods. specifically, we introduce image complexity, intra-concept visual consistency, and inter-concept visual similarity to quantify characteristics of dataset structures. we use these quantitative characteristics and eight few-shot learning methods to analyze performance differences on multiple datasets. based on the experimental analysis, some insightful observations are obtained from the perspective of both dataset structures and few-shot learning methods. we hope these observations are useful to guide future few-shot learning research on new datasets or tasks. our data is available at http://123.57.42.89/dataset-bias/dataset-bias.html.",AB_0387
"compared with short-term tracking, long-term tracking remains a challenging task that usually requires the tracking algorithm to track targets within a local region and re-detect targets over the entire image. however, few works have been done and their performances have also been limited. in this paper, we present a novel robust and real-time long-term tracking framework based on the proposed local search module and re-detection module. the local search module consists of an effective bounding box regressor to generate a series of candidate proposals and a target verifier to infer the optimal candidate with its confidence score. for local search, we design a long short-term updated scheme to improve the target verifier. the verification capability of the tracker can be improved by using several templates updated at different times. based on the verification scores, our tracker determines whether the tracked object is present or absent and then chooses the tracking strategies of local or global search, respectively, in the next frame. for global re-detection, we develop a novel re-detection module that can estimate the target position and target size for a given base tracker. we conduct a series of experiments to demonstrate that this module can be flexibly integrated into many other tracking algorithms for long-term tracking and that it can improve long-term tracking performance effectively. numerous experiments and discussions are conducted on several popular tracking datasets, including vot, oxuva, tlp, and lasot. the experimental results demonstrate that the proposed tracker achieves satisfactory performance with a real-time speed. code is available at https://github.com/difhnp/elglt.",AB_0387
"in the past few decades, license plate detection and recognition (lpdr) systems have made great strides relying on convolutional neural networks (cnn). however, these methods are evaluated on small and non-representative datasets that perform poorly in complex natural scenes. besides, most of existing license plate datasets are based on a single image, while the information source in the actual application of license plates is frequently based on video. the mainstream algorithms also ignore the dynamic clue between consecutive frames in the video, which makes the lpdr system have a lot of room for improvement. in order to solve these problems, this paper constructs a large-scale video-based license plate dataset named lsv-lp, which consists of 1,402 videos, 401,347 frames and 364,607 annotated license plates. compared with other data sets, lsv-lp has stronger diversity, and at the same time, it has multiple sources due to different collection methods. there may be multiple license plates in a frame, which is more in line with complex natural scenes. based on the proposed dataset, we further design a new framework that explores the information between adjacent frames, called mflpr-net. in addition to these, we release the annotation tools for license plates or vehicles in videos. by evaluating the performance of mflpr-net and some mainstream methods, it is proved that the proposed model is superior to other lpdr systems.in order to be more intuitive, we put some samples on https://drive.google.com/file/d/1udqrddpjzmptdhhqdwzrll6vayaluiql/view? usp=sharinggoogle drive. the whole dataset is available at https://github.com/forest-art/lsv-lp.",AB_0387
"differentiable architecture search, i.e., darts, has drawn great attention in neural architecture search. it tries to find the optimal architecture in a shallow search network and then measures its performance in a deep evaluation network. the independent optimization of the search and evaluation networks, however, leaves a room for potential improvement by allowing interaction between the two networks. to address the problematic optimization issue, we propose new joint optimization objectives and a novel cyclic differentiable architecture search framework, dubbed cdarts. considering the structure difference, cdarts builds a cyclic feedback mechanism between the search and evaluation networks with introspective distillation. first, the search network generates an initial architecture for evaluation, and the weights of the evaluation network are optimized. second, the architecture weights in the search network are further optimized by the label supervision in classification, as well as the regularization from the evaluation network through feature distillation. repeating the above cycle results in a joint optimization of the search and evaluation networks and thus enables the evolution of the architecture to fit the final evaluation network. the experiments and analysis on cifar, imagenet and nats-bench [95] demonstrate the effectiveness of the proposed approach over the state-of-the-art ones. specifically, in the darts search space, we achieve 97.52% top-1 accuracy on cifar10 and 76.3% top-1 accuracy on imagenet. in the chain-structured search space, we achieve 78.2% top-1 accuracy on imagenet, which is 1.1% higher than efficientnet-b0. our code and models are publicly available at https://github.com/microsoft/cream.",AB_0387
"target tracking, the essential ability of the human visual system, has been simulated by computer vision tasks. however, existing trackers perform well in austere experimental environments but fail in challenges like occlusion and fast motion. the massive gap indicates that researches only measure tracking performance rather than intelligence. how to scientifically judge the intelligence level of trackers? distinct from decision-making problems, lacking three requirements (a challenging task, a fair environment, and a scientific evaluation procedure) makes it strenuous to answer the question. in this article, we first propose the global instance tracking (git) task, which is supposed to search an arbitrary user-specified instance in a video without any assumptions about camera or motion consistency, to model the human visual tracking ability. whereafter, we construct a high-quality and large-scale benchmark videocube to create a challenging environment. finally, we design a scientific evaluation procedure using human capabilities as the baseline to judge tracking intelligence. additionally, we provide an online platform with toolkit and an updated leaderboard. although the experimental results indicate a definite gap between trackers and humans, we expect to take a step forward to generate authentic human-like trackers. the database, toolkit, evaluation server, and baseline results are available at http://videocube.aitestunion.com.",AB_0387
"appearance-based gaze estimation from rgb images provides relatively unconstrained gaze tracking from commonly available hardware. the accuracy of subject-independent models is limited partly by small intra-subject and large inter-subject variations in appearance, and partly by a latent subject-dependent bias. to improve estimation accuracy, we have previously proposed a gaze decomposition method that decomposes the gaze angle into the sum of a subject-independent gaze estimate from the image and a subject-dependent bias. estimating the bias from images outperforms previously proposed calibration algorithms, unless the amount of calibration data is prohibitively large. this paper extends that work with a more complete characterization of the interplay between the complexity of the calibration dataset and estimation accuracy. in particular, we analyze the effect of the number of gaze targets, the number of images used per gaze target and the number of head positions in calibration data using a new nislgaze dataset, which is well suited for analyzing these effects as it includes more diversity in head positions and orientations for each subject than other datasets. a better understanding of these factors enables low complexity high performance calibration. our results indicate that using only a single gaze target and single head position is sufficient to achieve high quality calibration. however, it is useful to include variability in head orientation as the subject is gazing at the target. our proposed estimator based on these studies (geddnet) outperforms state-of-the-art methods by more than $6.3\%$6.3%. one of the surprising findings of our work is that the same estimator yields the best performance both with and without calibration. this is convenient, as the estimator works well straight out of the box, but can be improved if needed by calibration. however, this seems to violate the conventional wisdom that train and test conditions must be matched. to better understand the reasons, we provide a new theoretical analysis that specifies the conditions under which this can be expected. the dataset is available at http://nislgaze.ust.hk. source code is available at https://github.com/hkust-nisl/geddnet.",AB_0387
"applying an image processing algorithm independently to each video frame often leads to temporal inconsistency in the resulting video. to address this issue, we present a novel and general approach for blind video temporal consistency. our method is only trained on a pair of original and processed videos directly instead of a large dataset. unlike most previous methods that enforce temporal consistency with optical flow, we show that temporal consistency can be achieved by training a convolutional network on a video with deep video prior (dvp). moreover, a carefully designed iteratively reweighted training strategy is proposed to address the challenging multimodal inconsistency problem. we demonstrate the effectiveness of our approach on 7 computer vision tasks on videos. extensive quantitative and perceptual experiments show that our approach obtains superior performance than state-of-the-art methods on blind video temporal consistency. we further extend dvp to video propagation and demonstrate its effectiveness in propagating three different types of information (color, artistic style, and object segmentation). a progressive propagation strategy with pseudo labels is also proposed to enhance dvp's performance on video propagation. our source codes are publicly available at https://github.com/chenyanglei/deep-video-prior.",AB_0387
"in recent years, a variety of gradient-based methods have been developed to solve bi-level optimization (blo) problems in machine learning and computer vision areas. however, the theoretical correctness and practical effectiveness of these existing approaches always rely on some restrictive conditions (e.g., lower-level singleton, lls), which could hardly be satisfied in real-world applications. moreover, previous literature only proves theoretical results based on their specific iteration strategies, thus lack a general recipe to uniformly analyze the convergence behaviors of different gradient-based blos. in this work, we formulate blos from an optimistic bi-level viewpoint and establish a new gradient-based algorithmic framework, named bi-level descent aggregation (bda), to partially address the above issues. specifically, bda provides a modularized structure to hierarchically aggregate both the upper- and lower-level subproblems to generate our bi-level iterative dynamics. theoretically, we establish a general convergence analysis template and derive a new proof recipe to investigate the essential theoretical properties of gradient-based blo methods. furthermore, this work systematically explores the convergence behavior of bda in different optimization scenarios, i.e., considering various solution qualities (i.e., global/local/stationary solution) returned from solving approximation subproblems. extensive experiments justify our theoretical results and demonstrate the superiority of the proposed algorithm for hyper-parameter optimization and meta-learning tasks. source code is available at https://github.com/vis-opt-group/bda.",AB_0387
"we explore the potential of pooling techniques on the task of salient object detection by expanding its role in convolutional neural networks. in general, two pooling-based modules are proposed. a global guidance module (ggm) is first built based on the bottom-up pathway of the u-shape architecture, which aims to guide the location information of the potential salient objects into layers at different feature levels. a feature aggregation module (fam) is further designed to seamlessly fuse the coarse-level semantic information with the fine-level features in the top-down pathway. we can progressively refine the high-level semantic features with these two modules and obtain detail enriched saliency maps. experimental results show that our proposed approach can locate the salient objects more accurately with sharpened details and substantially improve the performance compared with the existing state-of-the-art methods. besides, our approach is fast and can run at a speed of 53 fps when processing a $300 \times 400$300x400 image. to make our approach better applied to mobile applications, we take mobilenetv2 as our backbone and re-tailor the structure of our pooling-based modules. our mobile version model achieves a running speed of 66 fps yet still performs better than most existing state-of-the-art methods. to verify the generalization ability of the proposed method, we apply it to the edge detection, rgb-d salient object detection, and camouflaged object detection tasks, and our method achieves better results than the corresponding state-of-the-art methods of these three tasks. code can be found at http://mmcheng.net/poolnet/.",AB_0387
"large-scale genome-wide association studies (gwas) and expression quantitative trait locus (eqtl) studies have identified multiple non-coding variants associated with genetic diseases by affecting gene expression. however, pinpointing causal variants effectively and efficiently remains a serious challenge. here, we developed carmen, a novel algorithm to identify functional non-coding expression-modulating variants. multiple evaluations demonstrated carmen's superior performance over state-of-the-art tools. applying carmen to gwas and eqtl datasets further pinpointed several causal variants other than the reported lead single-nucleotide polymorphisms (snps). carmen scales well with the massive datasets, and is available online as a web server at http://carmen.gao-lab.org.",AB_0387
