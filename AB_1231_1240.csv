AB,NO
"for hyperspectral image (hsi) classification, two branch networks generally use convolutional neural networks (cnns) to extract the spatial features and long short-term memory (lstm) to learn the spectral features. however, cnns with a local kernel neglect the global properties of the whole hsi. lstm does not consider the macroscopic and detailed information of spectra. in this article, we propose a dual-view spectral and global spatial feature fusion network (dsgsf) to extract the spatial-spectral features for hsi classification (hsic), including a spatial subnetwork and a spectral subnetwork. in the spatial subnetwork, we propose a global spatial feature representation model based on the encoder-decoder structure with channel attention and spatial attention to learn the global spatial features. in the spectral subnetwork, we design a dual-view spectral feature aggregation model with view attention to learn the diversity of spectral features. by fusing the two subnetworks, we construct dsgsf to extract the spatial-spectral features of hsi with strong discriminating performance. experimental results on three public datasets illustrate that the proposed method can achieve competitive results compared with the state-of-the-art methods. code: https://github.com/rzwang-wh/dsgsf.",AB_0124
"background: the covid-19 pandemic has the potential to accelerate another pandemic: physical inactivity. daily steps, a proxy of physical activity, are closely related to health. recent studies indicate that over 7000 steps per day is the critical physical activity standard for minimizing the risk of all-cause mortality. moreover, the risk of cardiovascular events has been found to increase by 8% for every 2000 steps per day decrement. objective: to quantify the impact of the covid-19 pandemic on daily steps in the general adult population.methods: this study follows the guidelines of the moose (meta-analysis of observational studies in epidemiology) checklist. pubmed, embase, and web of science were searched from inception to february 11, 2023. eligible studies were observational studies reporting monitor-assessed daily steps before and during the confinement period of the covid-19 pandemic in the general adult population. two reviewers performed study selection and data extraction independently. the modified newcastle-ottawa scale was used to assess the study quality. a random effects meta-analysis was conducted. the primary outcome of interest was the number of daily steps before (ie, january 2019 to february 2020) and during (ie, after january 2020) the confinement period of covid-19. publication bias was assessed with a funnel plot and further evaluated with the egger test. sensitivity analyses were performed by excluding studies with low methodological quality or small sample sizes to test the robustness of the findings. other outcomes included subgroup analyses by geographic location and gender.results: a total of 20 studies (19,253 participants) were included. the proportion of studies with subjects with optimal daily steps (ie, >= 7000 steps/day) declined from 70% before the pandemic to 25% during the confinement period. the change in daily steps between the 2 periods ranged from -5771 to -683 across studies, and the pooled mean difference was -2012 (95% ci -2805 to -1218). the asymmetry in the funnel plot and egger test results did not indicate any significant publication bias. results remained stable in sensitivity analyses, suggesting that the observed differences were robust. subgroup analyses revealed that the decline in daily steps clearly varied by region worldwide but that there was no apparent difference between men and women.conclusions: our findings indicate that daily steps declined substantially during the confinement period of the covid-19 pandemic. the pandemic further exacerbated the ever-increasing prevalence of low levels of physical activity, emphasizing the necessity of adopting appropriate measures to reverse this trend. further research is required to monitor the consequence of long-term physical inactivity.trial registration: prospero crd42021291684; https://www.crd.york.ac.uk/prospero/display_record.php?recordid=291684",AB_0124
"attributing to material identification ability powered by a large number of spectral bands, hyperspectral videos (hsvs) have great potential for object tracking. most hyperspectral trackers employ manually designed features rather than deeply learned features to describe objects due to limited available hsvs for training, leaving a huge gap to improve the tracking performance. in this paper, we propose an end-to-end deep ensemble network (see-net) to address this challenge. specifically, we first establish a spectral self-expressive model to learn the band correlation, indicating the importance of a single band in forming hyperspectral data. we parameterize the optimization of the model with a spectral self-expressive module to learn the nonlinear mapping from input hyperspectral frames to band importance. in this way, the prior knowledge of bands is transformed into a learnable network architecture, which has high computational efficiency and can fast adapt to the changes of target appearance because of no iterative optimization. the band importance is further exploited from two aspects. on the one hand, according to the band importance, each frame of hsvs is divided into several three-channel false-color images which are then used for deep feature extraction and location. on the other hand, based on the band importance, the importance of each false-color image is computed, which is then used to assemble the tracking results from individual false-color images. in this way, the unreliable tracking caused by false-color images of low importance can be suppressed to a large extent. extensive experimental results show that see-net performs favorably against the state-of-the-art approaches. the source code will be available at https://github.com/hscv/see-net.",AB_0124
"unsupervised person re-identification is a challenging and promising task in computer vision. nowadays unsupervised person re-identification methods have achieved great progress by training with pseudo labels. however, how to purify feature and label noise is less explicitly studied in the unsupervised manner. to purify the feature, we take into account two types of additional features from different local views to enrich the feature representation. the proposed multi-view features are carefully integrated into our cluster contrast learning to leverage more discriminative cues that the global feature easily ignored and biased. to purify the label noise, we propose to take advantage of the knowledge of teacher model in an offline scheme. specifically, we first train a teacher model from noisy pseudo labels, and then use the teacher model to guide the learning of our student model. in our setting, the student model could converge fast with the supervision of the teacher model thus reduce the interference of noisy labels as the teacher model greatly suffered. after carefully handling the noise and bias in the feature learning, our purification modules are proven to be very effective for unsupervised person re-identification. extensive experiments on two popular person re-identification datasets demonstrate the superiority of our method. especially, our approach achieves a state-of-the-art accuracy 85.8% @map and 94.5% @rank-1 on the challenging market-1501 benchmark with resnet-50 under the fully unsupervised setting. code has been available at: https://github.com/tengxiao14/purification_reid.",AB_0124
"deep learning-based image signal processor (isp) models for mobile cameras can generate high-quality images that rival those of professional dslr cameras. however, their computational demands often make them unsuitable for mobile settings. additionally, modern mobile cameras employ non-bayer color filter arrays (cfa) such as quad bayer, nona bayer, and qxq bayer to enhance image quality, yet most existing deep learning-based isp (or demosaicing) models focus primarily on standard bayer cfas. in this study, we present pynet-qxq, a lightweight demosaicing model specifically designed for qxq bayer cfa patterns, which is derived from the original pynet. we also propose a knowl-edge distillation method called progressive distillation to train the reduced network more effectively. consequently, pynet-qxq contains less than 2.5% of the parameters of the original pynet while preserving its performance. experiments using qxq images captured by a prototype qxq camera sen-sor show that pynet-qxq outperforms existing conventional algorithms in terms of texture and edge reconstruction, despite its significantly reduced parameter count. code and partial datasets can be found at https://github.com/minhyeok01/pynet-qxq.",AB_0124
"recent person re-identification (reid) systems have been challenged by changes in personnel clothing, leading to the study of cloth-changing person reid (cc-reid). commonly used techniques involve incorporating auxiliary information (e.g., body masks, gait, skeleton, and keypoints) to accurately identify the target pedestrian. however, the effectiveness of these methods heavily relies on the quality of auxiliary information and comes at the cost of additional computational resources, ultimately increasing system complexity. this paper focuses on achieving cc-reid by effectively leveraging the information concealed within the image. to this end, we introduce an auxiliary-free competitive identification (acid) model. it achieves a win-win situation by enriching the identity (id)-preserving information conveyed by the appearance and structure features while maintaining holistic efficiency. in detail, we build a hierarchical competitive strategy that progressively accumulates meticulous id cues with discriminating feature extraction at the global, channel, and pixel levels during model inference. after mining the hierarchical discriminative clues for appearance and structure features, these enhanced id-relevant features are crosswise integrated to reconstruct images for reducing intra-class variations. finally, by combing with self- and cross-id penalties, the acid is trained under a generative adversarial learning framework to effectively minimize the distribution discrepancy between the generated data and real-world data. experimental results on four public cloth-changing datasets (i.e., prcc-reid, vc-cloth, ltcc-reid, and celeb-reid) demonstrate the proposed acid can achieve superior performance over state-of-the-art methods. the code is available soon at: https://github.com/boomshakay/wincc-reid.",AB_0124
"focusing on the task of point-to-point navigation for an autonomous driving vehicle, we propose a novel deep learning model trained with end-to-end and multi-task learning manners to perform both perception and control tasks simultaneously. the model is used to drive the ego vehicle safely by following a sequence of routes defined by the global planner. the perception part of the model is used to encode high-dimensional observation data provided by an rgbd camera while performing semantic segmentation, semantic depth cloud (sdc) mapping, and traffic light state and stop sign prediction. then, the control part decodes the encoded features along with additional information provided by gps and speedometer to predict waypoints that come with a latent feature space. furthermore, two agents are employed to process these outputs and make a control policy that determines the level of steering, throttle, and brake as the final action. the model is evaluated on carla simulator with various scenarios made of normal-adversarial situations and different weathers to mimic real-world conditions. in addition, we do a comparative study with some recent models to justify the performance in multiple aspects of driving. moreover, we also conduct an ablation study on sdc mapping and multi-agent to understand their roles and behavior. as a result, our model achieves the highest driving score even with fewer parameters and computation load. to support future studies, we share our codes at https://github.com/oskarnatan/end-to-end-driving.",AB_0124
"convolutional neural networks (cnns) for hyperspectral image (hsi) classification have generated good progress. meanwhile, graph convolutional networks (gcns) have also attracted considerable attention by using unlabeled data, broadly and explicitly exploiting correlations between adjacent parcels. however, the cnn with a fixed square convolution kernel is not flexible enough to deal with irregular patterns, while the gcn using the superpixel to reduce the number of nodes will lose the pixel-level features, and the features from the two networks are always partial. in this article, to make good use of the advantages of cnn and gcn, we propose a novel multiple feature fusion model termed attention multihop graph and multiscale convolutional fusion network (amgcfn), which includes two subnetworks of multiscale fully cnn and multihop gcn to extract the multilevel information of hsi. specifically, the multiscale fully cnn aims to comprehensively capture pixel-level features with different kernel sizes, and a multihead attention fusion module (mafm) is used to fuse the multiscale pixel-level features. the multihop gcn systematically aggregates the multihop contextual information by applying multihop graphs on different layers to transform the relationships between nodes, and an mafm is adopted to combine the multihop features. finally, we design a cross-attention fusion module (cafm) to adaptively fuse the features of two subnetworks. the amgcfn makes full use of multiscale convolution and multihop graph features, which is conducive to the learning of multilevel contextual semantic features. experimental results on three benchmark hsi datasets show that the amgcfn has a better performance than a few state-of-the-art methods. code: https://github.com/edwardhaoz/ieee_tgrs_amgcfn.",AB_0124
"recently, clustering-based methods have been the dominant solution for unsupervised person re-identification (reid). memory-based contrastive learning is widely used for its effectiveness in unsupervised representation learning. however, we find that the inaccurate cluster proxies and the momentum updating strategy do harm to the contrastive learning system. in this paper, we propose a real-time memory updating strategy (rtmem) to update the cluster centroid with a randomly sampled instance feature in the current mini-batch without momentum. compared to the method that calculates the mean feature vectors as the cluster centroid and updating it with momentum, rtmem enables the features to be up-to-date for each cluster. based on rtmem, we propose two contrastive losses, i.e., sample-to-instance and sample-to-cluster, to align the relationships between samples to each cluster and to all outliers not belonging to any other clusters. on the one hand, sample-to-instance loss explores the sample relationships of the whole dataset to enhance the capability of density-based clustering algorithm, which relies on similarity measurement for the instance-level images. on the other hand, with pseudo-labels generated by the density-based clustering algorithm, sample-to-cluster loss enforces the sample to be close to its cluster proxy while being far from other proxies. with the simple rtmem contrastive learning strategy, the performance of the corresponding baseline is improved by 9.3% on market-1501 dataset. our method consistently outperforms state-of-the-art unsupervised learning person reid methods on three benchmark datasets. code is made available at:https://github.com/pris-cv/rtmem.",AB_0124
"for the reduction of jpeg compression artifacts, there have been many methods using deep neural networks. most of them use the jpeg compression quality factor (qf) as prior knowledge in designing and training the networks. however, since the images we get from the internet are often recompressed, the given qf is not so informative or misleading. also, early works validated their methods on low qfs less than 50, while recent smartphones use high qfs larger than or equal to 90. in this paper, we propose a new jpeg artifacts reduction network considering the above-stated problems. specifically, to extract quality information from the input image itself instead of the qf provided in the header of the jpeg file, we use a variational autoencoder (vae) and regard its latent vector as quality information. in designing the artifact reduction network, we let the network change flexibly according to the input image quality by employing a deformable offset gating (dog) network. the gating network and vae are merged as our overall network, dubbed dog-vae, where the information from the vae is used to adjust the dog network according to the input quality. the dog-vae is trained end-to-end with the qfs in the range of [10], [90]. extensive experiments validate that our method achieves comparable results to the state-of-the-art method for monochrome images and better results for color images. our codes are available at https://github.com/yunjh410/dognet.",AB_0124
