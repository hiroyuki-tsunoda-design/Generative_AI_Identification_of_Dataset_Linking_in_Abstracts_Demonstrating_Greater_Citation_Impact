AB,NO
"video anomaly detection is a promising yet challenging task, where only normal events are observed in the training phase. without any explicit classification boundary between normal and abnormal events, anomaly detection can be turned into an outlier detection problem by regarding any event that does not conform to the normal patterns as an anomaly. most of the existing works mainly focus on improving the representation of normal events, while ignore the relationship between normal and abnormal events. besides, the lack of restrictions on classification boundaries also leads to performance degradation. to ad-dress the above problems, we design a novel autoencoder-based memory-augmented appearance-motion network (maam-net), which consists of a novel end-to-end network to learn appearance and motion fea-ture of a given input frame, a fused memory module to build a bridge for normal and abnormal events, a well-designed margin-based latent loss to relieve the computation costs, and a pointed patch-based stride convolutional detection (pscd) algorithm to eliminate the degradation phenomenon. specifically, the memory module is embedded between the encoder and decoder, which serves as a sparse dictionary of normal patterns, therefore it can be further employed to reintegrate abnormal events during inference. to further distort the reintegration quality of abnormal events, the margin-based latent loss is leveraged to enforce the memory module to select a sparse set of critical memory items. last but not least, the simple yet effective detection method focuses on patches rather than the overall frame responses, which can benefit from the distortion of abnormal events. extensive experiments and ablation studies on three anomaly detection benchmarks, i.e., ucsd ped2, cuhk avenue, and shanghaitech, demonstrate the ef-fectiveness and efficiency of our proposed maam-net. notably, we achieve superior auc performances on ucsd ped2 (0.977), chuk avenue (0.909), and shanghaitech (0.713). the code is publicly available at https://github.com/owen- tian/maam-net .(c) 2023 published by elsevier ltd.",AB_0375
"pixel-level crack extraction (pce) is challenging due to topology complexity, irregular edges, low contrast ratio, and complex background. recently, transformer architectures have shown great potential on many vision tasks and even outperform convolutional neural networks (cnns). benefiting from the self-attention mechanism, transformers can invariably capture the global context information to establish long-range dependencies on the detected objects. however, there was little work on the transformer architectures for pce. in this paper, a systematic analysis of three well-designed transformer architectures for pce task in terms of network structures and parameters, feature fusion modes, training data and strategy, and generalization ability was developed for the first time. we proposed a crack extraction network with vision transformer (crackvit) that jointly captures the detailed structures and long-distance dependencies with a novel hybrid encoder with cnn and transformer to keep the corresponding topologies. in order to be more suitable for pce task, we explored three feature fusion modes between cnn and transformer. in addition, a novel feature aggregation block was proposed to sharpen the edges of the decoder upsampling and reduce the noise effect of shallow features. moreover, a multi-task supervised training strategy was adopted to further improve the details of crack edges. results on four challenging datasets, including crackforest, deepcrack, crkwh100, and crack500, show that crackvit outperforms state-of-the-art cnn-based methods and the other two novel transformer architectures. our codes are available at: https:// github.com/smilqe/crackvit.",AB_0375
"land cover change detection (cd) in very-high-resolution (vhr) images is still impeded by weak pattern separability and land cover complexity. to address these challenges, a self-structured pyramid network (s 2 pnet) with a parallel spatial-channel attention mechanism (psam) and a self-structured feature pyramid (sfp) is proposed for a finer annotation of changed land cover. the proposed psam refines the features of different levels in dual-branch coordinated by running parallel without mutual influence for a better recognition of varied objects, which can lead to less incorrectly detected land cover. and the sfp integrates the embedded multi-scale features to acquire an improved cognition over multi-scale objects, which can contribute to a more complete annotation over diverse objects. all-round experiments over several widely used open large-scale vhr cd data sets are carried out, which indicate the efficiency and effectiveness of the proposed method. related comparisons suggest that the proposed method can achieve higher performance over several existing state-of-the-art cd methods. the source codes will be released at https://github.com/haixing- 1998/s2pnet- cd .",AB_0375
"knowledge distillation (kd) has emerged as an essential technique not only for model compression, but also other learning tasks such as continual learning. given the richer application spectrum and potential online usage of kd, knowledge distillation efficiency becomes a pivotal component. in this work, we study this little-explored but important topic. unlike previous works that focus solely on the accuracy of stu-dent network, we attempt to achieve a harder goal - to obtain a performance comparable to conventional kd with a lower computation cost during the transfer. to this end, we present uncertainty-aware mixup (unix), an effective approach that can reduce transfer cost by 20% to 30% and yet maintain comparable or achieve even better student performance than conventional kd. this is made possible via effective uncertainty sampling and a novel adaptive mixup approach that select informative samples dynamically over ample data and compact knowledge in these samples. we show that our approach inherently per-forms hard sample mining. we demonstrate the applicability of our approach to improve various existing kd approaches by reducing their queries to a teacher network. extensive experiments are performed on cifar100 and imagenet. code and model are available at https://github.com/xuguodong03/unixkd .(c) 2023 elsevier ltd. all rights reserved.",AB_0375
"the total variation (tv) regularization has been widely used in hyperspectral image (hsi) denoising ow-ing to its powerful capabilities in terms of structure preservation. however, existing tv terms ignore the geometrical structures, such as edges and textures, in hsis. to this end, we present a novel 3d geometri-cal total variation (3d gtv) regularizer to capture the local information in hsis. 3d gtv can preserve the spatial and spectral information in 3d cubes effectively due to the introduction of geometrical structures. by incorporating this term, a 3d gtv regularized low-rank matrix factorization (3dgtvlr) method is pro-posed for the noise removal of noisy hsis. in the proposed method, different norms are adopted to model the structures in 3d cubes. more specifically, the l 1 norm is employed to enhance the structural informa-tion in the 3d cubes containing edges or textures, while the l 2 norm is considered to ensure the infor-mation in flat 3d cubes. the local structures in hsis are represented more compatibly by the combination of l 1 and l 2 norms. then, we derive an efficient algorithm based on the alternating direction method of multipliers (admm) to solve the optimization problem. experiments are performed on synthetic and real hsi datasets and the effectiveness of the proposed method is demonstrated when compared with the state-of-the-art methods. our code is available at https://github.com/rsmagneto/3dgtvlr . (c) 2023 elsevier b.v. all rights reserved.",AB_0375
"quantization-based hashing methods have become increasingly popular to adjust the global data distribution and accurately capture the data similarity compared with pairwise/triplet similarity-based methods. however, the existing image quantization hashing approaches adopt fixed hash centers, which consider neither the semantic information of each hash center nor the scale size of each object appearing in a multi-label image, resulting in that each hash code will deviate from its corresponding hash centroid. to address this issue, we propose hccst, a hash centroid construction method with swin transformer for multi-label image retrieval. hccst consists of a hash code generation module, a hash centroid construction module and an interaction module between each hash code and its corresponding hash centroid. in the hash code generation module, we first adopt swin transformer to extract the feature vector for each input multi-label image and then generate the initialized hash code of this image. in the hash centroid construction module, we first utilize the object semantic information to construct semantic hash centers and then consider the object scale size by learning the object weight coefficient to compute the hash centroid for each sample. after obtaining both the hash code and hash centroid of each sample, in the last interaction module, we constantly limit the distance between each hash code and its hash centroid to preserve the similarity between samples. our model will be trained in an end-to-end manner to alternately update the net parameters of hash code generation module, hash centroid construction module and the object weight coefficient. we conduct extensive experiments on 3 multi-label image datasets including voc2012, ms-coco and nus-wide. the experimental results demonstrate that hccst can achieve better retrieval performance compared with the state-of-the-art image hashing methods. the open-source code of this project is released at: https://github.com/lzhzwz/hccst.git.",AB_0375
"the majority of image-to-image translation models tend to struggle in varying domain settings. for one varying domain, samples vary significantly in shape and size and have no domain labels. this paper pro-poses an unsupervised class-to-class translation model based on conditional contrastive learning to tackle the domain variations problem. the initial hypothesis is that the latent modalities of two varying domains are categorizable by style differences of different samples and turn the image-to-image translation prob-lem into class-to-class translation. firstly, unsupervised semantic clustering is performed for each domain to divide them into multiple classes and then leverage the classification features of different classes to perform class-to-class translation. two conditional contrastive learning loss functions for each domain are proposed to perform unsupervised semantic clustering and decompose it into multiple classes. then in the class-to-class translation stage, the classification features of different classes are employed to learn the latent modalities. the proposed model outperforms state-of-the-art baseline methods by employing the latent modalities of different classes. the sample code is available at https://github.com/c1a1o1/ucct .(c) 2023 elsevier ltd. all rights reserved.",AB_0375
"the performance of existing underwater object detection methods severely degrades when they face the domain shift caused by complicated underwater environments. due to the limited domain diversity in collected data, deep detectors easily memorize a few seen domains, which leads to low generalization ability. there are two common ideas to improve the domain generalization performance. first, it can be inferred that the detector trained on as many domains as possible is domain-invariant. second, their hidden features should be equivalent because the images with the same semantic content are in different domains. this paper further excavates these two ideas and proposes a domain generalization framework that learns how to generalize across domains from domain mixup and contrastive learning (dmcl). first, based on the formation of underwater images, an image in one kind of underwater environment is the linear transformation of another underwater environment. therefore, a style transfer model, which out-puts a linear transformation matrix instead of the whole image, is proposed to transform images from one source domain to another, enriching the domain diversity of the training data. second, the mixup oper-ation interpolates different domains on the feature level, sampling new domains on the domain manifold. third, a contrastive loss is selectively applied to features from different domains to force the model to learn domain-invariant features but retain the discriminative capacity. with our method, detectors will be robust to domain shift. also, a domain generalization benchmark s-uodac2020 for detection is set up to measure the performance of our method. comprehensive experiments on s-uodac2020 and two object recognition benchmarks (pacs and vlcs) demonstrate that the proposed method is able to learn domain-invariant representations and outperforms other domain generalization methods. the code is available in https://github.com/mousecpn/dmc-domain-generalization-for-underwater-object-detection.gitco 2023 published by elsevier b.v.",AB_0375
"line detection with deep learning is a popular visual task that focuses mostly on lane detection. it requires quicker inference speed and lower consumption, especially for high-speed edge device applications. based on the ufast, we propose the non-residual unrestricted pruned ultra-faster (nrupu) line detection via a novel model compression method including non-interference structural reconstruction (nisr), shallow channel priority reservation (scpr) pruning and non-residual equivalent transformation (nret). nisr is a structure reconstruction scheme allocating residual branches into each layer to solve the cross-layer channel interference in resnet-18. scpr pruning directly uses the factors of bn layers to build channel importance evaluation for backbone and designs channel selection method for head based on data distribution consistency, reducing the parameters of each layer independently. then nret losslessly converts the multi-branch model to a single-branch one containing only convolution, linear, and relu, which reduces implementation complexity on edge devices. these designs follow the theoretical foundations: the data distribution transformation trend and effect of gradient back-propagation on model learning ability. compared with previous pruning methods, our method optimizes not only the parameters of the model but also the structure of the model. we train nrupu in rtx2080ti and deploy tests on edge devices nvidia jetson xavier nx (njxn) and atlas 200 dk (a2dk). extensive experiments are conducted on the dataset tusimple, culane and our belt dataset with 11,894 data. results show that nrupu achieves over 96% speed increase and over 66% parameter reduction on all datasets within 0.7% accuracy loss. the fps can reach 749, 665 and 783 on rtx2080ti, 133, 117 and 143 on njnx, 178, 161 and 183 on a2dk respectively. the code is released at https://anonymous.4open.science/r/nrupu-29b0 . (c) 2023 elsevier ltd. all rights reserved.",AB_0375
"in practical applications, different application fields have various requirements regarding the precision and completeness of building extraction. too low precision or completeness may limit the application and promotion of building extraction. obtaining a good trade-off between the precision and com-pleteness of building extraction is still a challenging issue. to deal with this issue, this paper proposes a context-content collaborative network (c3net) with an encoder-decoder structure. it consists of a context-content aware module (c2am) and an edge residual refinement module (er2m). in the c2am, a context-aware block and a content-aware block complement each other and capture the localization information of buildings and long-range dependencies between the locations of each building, respectively. thanks to the capability of the conventional filter, the er2m can refine the features of decoder output by deploying a residual atrous spatial pyramid pooling with feature edges at the scale of the original image. to explicitly guide the function of the er2m, we introduce a separated deep supervision strategy before and after the er2m, which can consciously refine our c3net towards the precision or completeness of building extraction to a certain extent, and improve the overall detection performance. compared with several classical and state-of-the-art methods, extensive experiments on three open and challenging datasets demonstrate that the proposed c3net not only acquires competitive performance but also achieves a better trade-off between precision and completeness of building extraction. the source code is released at https://github.com/tongfeiliu/ c3net-for-building-extraction.(c) 2023 published by elsevier b.v.",AB_0375
