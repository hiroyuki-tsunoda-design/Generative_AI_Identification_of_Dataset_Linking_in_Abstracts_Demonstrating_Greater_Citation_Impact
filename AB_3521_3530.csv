AB,NO
"deep learning has recently been proven to deliver excellent performance in multi-view stereo (mvs). however, it is difficult for deep learning-based mvs approaches to balance their efficiency and effectiveness. towards this end, we propose the dscmvsnet, a novel coarse-to-fine and end-to-end framework for more efficient and more accurate depth estimation in mvs. in particular, we propose an attention aware 3d unet-shape network, which first uses the depthwise separable convolutions for cost volume regularization. this mechanism enables effective aggregation of information and significantly reduces the model parameters and computation by transforming the ordinary convolution on cost volume as depthwise convolution and pointwise convolution. besides, a 3d-attention module is proposed to alleviate the feature mismatching problem in cost volume regularization and aggregate the important information of cost volume in three dimensions (i.e. channel, space, and depth). moreover, we propose an efficient feature transfer module to upsample the low-resolution (lr) depth map to a high-resolution (hr) depth map to achieve higher accuracy. with extensive experiments on two benchmark datasets, i.e. dtu and tanks & temples, we demonstrate that the parameters of our model are significantly reduced to 25% of the state-of-the-art model mvsnet. besides, our method outperforms or maintains on par accuracy with the state-of-the-art models. our source code is available at https://github.com/zs670980918/dsc-mvsnet.",AB_0353
"recently, deep transfer learning-based intelligent machine diagnosis has been well investigated, and the source and the target domain are commonly assumed to share the same fault categories, which can be called as the closed-set diagnosis transfer (csdt). however, this assumption is hard to cover real engi-neering scenarios because some unknown new fault may occur unexpectedly due to the uncertainty and complexity of machinery components, which is called as the open-set diagnosis transfer (osdt). to solve this challenging but more realistic problem, a theory-guided progressive transfer learning network (tptln) is proposed in this paper. first, the upper bound of transfer learning model under open-set setting is thoroughly analyzed, which provides a theoretical insight to guide the model opti-mization. second, a two-stage module is designed to carry out distracting unknown target samples and attracting known samples through progressive learning, which could effectively promote inter-class separability and intra-class compactness. the performance of proposed tptln is evaluated in two osdt cases, where the diagnosis knowledge is transferred across bearings and gearbox running under different working conditions. comparative results show that the proposed method achieves better robustness and diagnostic performance under different degrees of domain shift and openness variance. the source codes and links to the data can be found in the following github repository: https://github.-com/phoenixdyf/theory-guided-progressive-transfer-learningnetwork.& copy; 2023 elsevier b.v. all rights reserved.",AB_0353
"breast tumor segmentation from ultrasound images is one of the key steps that help us characterize and localize tumor regions. however, variable tumor morphology, blurred boundaries, and similar intensity distributions bring challenges for radiologists to segment breast tumors manually. during clinical diagno-sis, there are higher demands on the segmentation accuracy and efficiency of breast ultrasound images, so there is an urgent need for an automated method to improve the segmentation accuracy as a technical tool to assist diagnosis. inspired by the u-net and its many variations, this paper proposed an unpreten-tious nested u-net (nu-net) for accurate and efficient breast tumor segmentation. the key idea is to uti-lize u-nets with different depths and shared weights to achieve robust characterization of breast tumors. specifically, we first utilize the deeper u-net (fifteen layers) as the backbone network to extract more suf-ficient breast tumor features. then, we developed a multi-output u-net to be taken as the bond between the encoder and the decoder to enhance the network adaptability for breast tumors with different scales. finally, the short-connection based on multi-step down-sampling is used to enhance the correlation of long-range information of encoded features. extensive experimental results with fifteen state-of-the-art segmentation methods on three public breast ultrasound datasets demonstrate that our method has a more competitive segmentation performance on breast tumors. furthermore, the robustness of our ap-proach is further illustrated by the segmentation of renal ultrasound images. the source code is publicly available on https://github.com/cgpxy/nu-net . & copy; 2023 elsevier ltd. all rights reserved.",AB_0353
"recently, 3d vision-and-language tasks have attracted increasing research interest. compared to other vision-and-language tasks, the 3d visual question answering (vqa) task is less exploited and is more susceptible to language priors and co-reference ambiguity. meanwhile, a couple of recently proposed 3d vqa datasets do not well support 3d vqa task due to their limited scale and annotation methods. in this work, we formally define and address a 3d grounded question answering (gqa) task by collecting a new 3d vqa dataset, referred to as flexible and explainable 3d gqa (fe-3dgqa), with diverse and relatively free-form question-answer pairs, as well as dense and completely grounded bounding box annotations. to achieve more explainable answers, we label the objects appeared in the complex qa pairs with different semantic types, including answer-grounded objects (both appeared and not appeared in the questions), and contextual objects for answer-grounded objects. we also propose a new 3d vqa framework to effectively predict the completely visually grounded and explainable answer. extensive experiments verify that our newly collected benchmark datasets can be effectively used to evaluate various 3d vqa methods from different aspects and our newly proposed framework also achieves the state-of-the-art performance on the new benchmark dataset. the datasets and the source code are available via https://github.com/zlccccc/3dvl_codebase.",AB_0353
"anomaly detection in surveillance videos aims to identify frames where abnormal events happen. existing approaches assume that the training and testing videos are from the same scene, exhibiting poor generalization performance when encountering an unseen scene. in this paper, we propose a variational anomaly detection network (vadnet), which is characterized by its high scene-adaptation - it can identify abnormal events in a new scene only via referring to a few normal samples without fine-tuning. our model embodies two major innovations. first, a novel variational normal inference (vni) module is proposed to formulate image reconstruction in a conditional variational auto-encoder (cvae) framework, which learns a probabilistic decision model instead of a traditional deterministic one. secondly, a margin learning embedding (mle) module is leveraged to boost the variational inference and aid in distinguishing normal events. we theoretically demonstrate that minimizing the triplet loss in mle module facilitates maximizing the evidence lower bound (elbo) of cvae, which promotes the convergence of vni. by incorporating variational inference with margin learning, vadnet becomes much more generative that is able to handle the uncertainty caused by the changed scene and limited reference data. extensive experiments on several datasets demonstrate that the proposed vadnet can adapt to a new scene effectively without fine-tuning and achieve remarkable performance, which outperforms other methods significantly and establishes new state-of-the-art in the case of few-shot scene-adaptive anomaly detection. we believe our method is closer to real-world application due to its strong generalization ability. all codes are released in https://github.com/huangxx156/vadnet.",AB_0353
"based on potentially subjective and diverse image quality scores given by a group of subjects, we propose to predict the distribution of image quality scores rather than the mean opinion score (mos) of image quality. therefore, in this paper, we use an alpha stable model to parameterize the image quality score distribution (iqsd), and propose an objective method to predict the alpha-stable-model-based iqsd. first, the live database is re-recorded. specifically, we invite a large group of subjects (187 valid subjects) to evaluate the quality of all 808 images in the live database, with their scores forming reliable iqsds. all images in the live database and their collected subjective quality scores form a new image quality assessment database, named the sjtu iqsd database. we then propose a framework and algorithm to predict the alpha-stable-model-based iqsd, in which quality features are extracted from the structural and natural statistical information of each image, and support vector regressors are trained to predict the alpha stable model parameters. experiments carried out on the sjtu iqsd database verify the feasibility of using the alpha stable model to describe the iqsd, and the experimental results show that the alpha-stable-model-based iqsd can reflect a large amount of subjective information on image quality. we also prove that the objective alpha-stable-model-based iqsd prediction method is effective. the code and the sjtu iqsd database can be downloaded at 'https://github.com/yixuangao98/image-quality-score-distribution-prediction-via-alpha-stable-model.git'.",AB_0353
"video quality assessment is critical in optimizing video coding techniques. however, the state-of-the-art methods have limited performance, which is largely due to the lack of large-scale subjective databases for training. in this work, a semi-automatic labeling method is adopted to build a large-scale compressed video quality database, which allows us to label a large number of compressed videos with manageable human workload. the resulting compressed video quality database with semi-automatic ratings (cvsar), so far the largest of compressed video quality database. we train a no-reference compressed video quality assessment model with a 3d cnn for spatiotemporal feature extraction and evaluation (stfee). experimental results demonstrate that the proposed method outperforms state-of-the-art metrics and achieves promising generalization performance in cross-database tests. the cvsar database has been made publicly available. it can be accessed at https://github.com/rocknroll194/cvsar.",AB_0353
"image rain removal is an essential problem of common concern in the fields of image processing and computer vision. existing methods have resorted to deep learning techniques to separate rain streaks from the background by leveraging some prior knowledge. however, the mismatch between the size of the rain streaks during the training and testing phases, especially when large rain streaks are present, frequently leads to unsatisfactory deraining results. to address this issue, we propose a multi-scale self-calibrated dual attention lightweight residual dense deraining network (mdarnet) for better deraining performance. specifically, the network consists of monogenic wavelet transform-like hierarchy and self-calibrated dual attention mechanism. with the help of scale-space properties of the monogenic wavelet transform, key features at different scales can be extracted at the same location, which makes it easier to match structural features across scales. the self-calibrated double attention mechanism was used as a basic model for enhancing the channel dependence and spatial correlation between each layer component of the monogenic wavelet transform. thus, the network can establish long-range dependencies and take advantage of rich contextual information and multi-scale redundancy to accommodate rain streaks of different shapes and sizes. experiments on synthetic and real image datasets show that the method outperforms many of the latest single-image deraining methods in terms of visual and quantitative metrics. the source code can be obtained from https://github.com/smart-hzw/mdarnet.",AB_0353
"with the development of robust information hiding (rih) approaches, secret messages can be extracted successfully from stego-data after transmission through lossy channels of online social networks (osns). to interrupt illegal covert communications in osns, some methods sanitize the uploaded images by image processing operations to destroy the hidden data that may exist. however, none of the existing methods takes the rih methods that can resist scaling into consideration, while scaling is a common operation in osns. in this paper, we first propose a general framework for image sanitization in osn platforms, which serves as a countermeasure against the rih. by using such a framework, the secret messages embedded in the upload images can be removed and the quality of the sanitized image can be well maintained. our framework contains two deep neural networks: scaling-net and sc-net. the scaling-net is dedicated to the sanitization of oversized images while the sc-net is designed for other images. to achieve a good image quality, we also propose a discriminator for adversarial training of the scaling-net and sc-net. experimental results on different datasets demonstrate that our proposed method outperforms the state-of-the-art methods. the source code and pretrained models are available at our code repository (https://github.com/zyzhu19/image_sanitization).",AB_0353
"recent years witness the tremendous success of generative adversarial networks (gans) in synthesizing photo-realistic images. gan generator learns to compose realistic images and reproduce the real data distribution. through that, a hierarchical visual feature with multi-level semantics spontaneously emerges. in this work we investigate that such a generative feature learned from image synthesis exhibits great potentials in solving a wide range of computer vision tasks, including both generative ones and more importantly discriminative ones. we first train an encoder by considering the pre-trained stylegan generator as a learned loss function. the visual features produced by our encoder, termed as generative hierarchical features (gh-feat), highly align with the layer-wise gan representations, and hence describe the input image adequately from the reconstruction perspective. extensive experiments support the versatile transferability of gh-feat across a range of applications, such as image editing, image processing, image harmonization, face verification, landmark detection, layout prediction, image retrieval, etc. we further show that, through a proper spatial expansion, our developed gh-feat can also facilitate fine-grained semantic segmentation using only a few annotations. both qualitative and quantitative results demonstrate the appealing performance of gh-feat. code and models are available at https://genforce.github.io/ghfeat/.",AB_0353
