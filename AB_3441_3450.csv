AB,NO
"to alleviate the need for large-scale pixel-wise annotations, domain adaptation for semantic segmentation trains segmentation models on synthetic data (source) with computer-generated annotations, which can be then generalized to segment realistic images (target). recently, self-supervised learning (ssl) with a combination of image-to-image translation shows great effectiveness in adaptive segmentation. the most common practice is to perform ssl along with image translation to well align a single domain (source or target). however, in this single-domain paradigm, unavoidable visual inconsistency raised by image translation may affect subsequent learning. in addition, pseudo labels generated by a single segmentation model aligned in either the source or target domain may be not accurate enough for ssl. in this paper, based on the observation that domain adaptation frameworks performed in the source and target domain are almost complementary, we propose a novel adaptive dual path learning (adpl) framework to alleviate visual inconsistency and promote pseudo-labeling by introducing two interactive single-domain adaptation paths aligned in source and target domain respectively. to fully explore the potential of this dual-path design, novel technologies such as dual path image translation (dpit), dual path adaptive segmentation (dpas), dual path pseudo label generation (dpplg) and adaptive classmix are proposed. the inference of adpl is extremely simple, only one segmentation model in the target domain is employed. our adpl outperforms the state-of-the-art methods by large margins on gta5 -> cityscapes, synthia -> cityscapes and gta5 -> bdd100k scenarios. code and models are available at https:// github.com/royee182/dpl.",AB_0345
"measuring perceptual color differences (cds) is of great importance in modern smartphone photography. despite the long history, most cd measures have been constrained by psychophysical data of homogeneous color patches or a limited number of simplistic natural photographic images. it is thus questionable whether existing cd measures generalize in the age of smartphone photography characterized by greater content complexities and learning-based image signal processors. in this article, we put together so far the largest image dataset for perceptual cd assessment, in which the photographic images are 1) captured by six flagship smartphones, 2) altered by photoshop, 3) post-processed by built-in filters of the smartphones, and 4) reproduced with incorrect color profiles. we then conduct a large-scale psychophysical experiment to gather perceptual cds of 30,000 image pairs in a carefully controlled laboratory environment. based on the newly established dataset, we make one of the first attempts to construct an end-to-end learnable cd formula based on a lightweight neural network, as a generalization of several previous metrics. extensive experiments demonstrate that the optimized formula outperforms 33 existing cd measures by a large margin, offers reasonable local cd maps without the use of dense supervision, generalizes well to homogeneous color patch data, and empirically behaves as a proper metric in the mathematical sense. our dataset and code are publicly available at https://github.com/hellooks/cdnet.",AB_0345
"unsupervised person re-identification (re-id) aims at finding the most informative features from unlabeled person datasets. some recent approaches adopted camera-aware strategies for model training and have thereby achieved highly promising results. however, these methods simultaneously address intra-id discrepancies of all cameras and require independent learning under each camera, which increases the complexity of algorithm. to resolve this issue, we present a camera contrast learning framework for unsupervised person re-id. our method first proposes a time-based camera contrastive learning module to facilitate model learning. at each iteration, we follow the time contrast principle to select one camera centroid as proxy of each cluster. by enforcing the samples to converge to positive proxies, the correlation between features and cameras can gradually be reduced. moreover, we design a 3-dimensional attention module to further reduce intra-id discrepancies caused by background shifts. by re-weighting each feature map element in a spatial-channel order, our module can exactly find identity-invariant semantic cues from regions of interest in person images, no matter how the background change. experimental results on several popular datasets prove that our work surpasses existing unsupervised person re-id approaches to a remarkable extent. the source codes can be found in https://github.com/hongweizhang97/ccl.",AB_0345
"numerous studies have shown that in-depth mining of correlations between multi-modal features can help improve the accuracy of cross-modal data analysis tasks. however, the current image description methods based on the encoder-decoder framework only carry out the interaction and fusion of multi-modal features in the encoding stage or the decoding stage, which cannot effectively alleviate the semantic gap. in this paper, we propose a deep fusion transformer (dft) for image captioning to provide a deep multi-feature and multi-modal information fusion strategy throughout the encoding to decoding process. we propose a novel global cross encoder to align different types of visual features, which can effectively compensate for the differences between features and incorporate each other's strengths. in the decoder, a novel cross on cross attention is proposed to realize hierarchical cross-modal data analysis, extending complex cross-modal reasoning capabilities through the multi-level interaction of visual and semantic features. extensive experiments conducted on the mscoco dataset prove that our proposed dft can achieve excellent performance and outperform state-of-the-art methods. the code is available at https://github.com/weimingboya/dft.",AB_0345
"in this paper, we study a challenging but less-touched problem in cross-modal retrieval, i.e., partially mismatched pairs (pmps). specifically, in real-world scenarios, a huge number of multimedia data (e.g., the conceptual captions dataset) are collected from the internet, and thus it is inevitable to wrongly treat some irrelevant cross-modal pairs as matched. undoubtedly, such a pmp problem will remarkably degrade the cross-modal retrieval performance. to tackle this problem, we derive a unified theoretical robust cross-modal learning framework (rcl) with an unbiased estimator of the cross-modal retrieval risk, which aims to endow the cross-modal retrieval methods with robustness against pmps. in detail, our rcl adopts a novel complementary contrastive learning paradigm to address the following two challenges, i.e., the overfitting and underfitting issues. on the one hand, our method only utilizes the negative information which is much less likely false compared with the positive information, thus avoiding the overfitting issue to pmps. however, these robust strategies could induce underfitting issues, thus making training models more difficult. on the other hand, to address the underfitting issue brought by weak supervision, we present to leverage of all available negative pairs to enhance the supervision contained in the negative information. moreover, to further improve the performance, we propose to minimize the upper bounds of the risk to pay more attention to hard samples. to verify the effectiveness and robustness of the proposed method, we carry out comprehensive experiments on five widely-used benchmark datasets compared with nine state-of-the-art approaches w.r.t. the image-text and video-text retrieval tasks. the code is available at https://github.com/penghu-cs/rcl.",AB_0345
"non-rigid 3d registration, which deforms a source 3d shape in a non-rigid way to align with a target 3d shape, is a classical problem in computer vision. such problems can be challenging because of imperfect data (noise, outliers and partial overlap) and high degrees of freedom. existing methods typically adopt the l(p) type robust norm to measure the alignment error and regularize the smoothness of deformation, and use a proximal algorithm to solve the resulting non-smooth optimization problem. however, the slow convergence of such algorithms limits their wide applications. in this paper, we propose a formulation for robust non-rigid registration based on a globally smooth robust norm for alignment and regularization, which can effectively handle outliers and partial overlaps. the problem is solved using the majorization-minimization algorithm, which reduces each iteration to a convex quadratic problem with a closed-form solution. we further apply anderson acceleration to speed up the convergence of the solver, enabling the solver to run efficiently on devices with limited compute capability. extensive experiments demonstrate the effectiveness of our method for non-rigid alignment between two shapes with outliers and partial overlaps, with quantitative evaluation showing that it outperforms state-of-the-art methods in terms of registration accuracy and computational speed. the source code is available at https://github.com/yaoyx689/amm_nrr.",AB_0345
"to enable effective learning of new tasks with only a few examples, meta-learning acquires common knowledge from the existing tasks with a globally shared meta-learner. to further address the problem of task heterogeneity, recent developments balance between customization and generalization by incorporating task clustering to generate task-aware modulation to be applied to the global meta-learner. however, these methods learn task representation mostly from the features ofinput data, while the task-specific optimization process with respect to the base-learner is often neglected. in this work, we propose a clustered task-aware meta-learning (ctml) framework with task representation learned from both features and learning paths. we first conduct rehearsed task learning from the common initialization, and collect a set of geometric quantities that adequately describes this learning path. by inputting this set of values into a meta path learner, we automatically abstract path representation optimized for downstream clustering and modulation. aggregating the path and feature representations results in an improved task representation. to further improve inference efficiency, we devise a shortcut tunnel to bypass the rehearsed learning process at a meta-testing time. extensive experiments on two real-world application domains: few-shot image classification and cold-start recommendation demonstrate the superiority of ctml compared to state-of-the-art methods. we provide our code at https://github.com/didiya0825.",AB_0345
"existing image-based rendering methods usually adopt depth-based image warping operation to synthesize novel views. in this paper, we reason the essential limitations of the traditional warping operation to be the limited neighborhood and only distance-based interpolation weights. to this end, we propose content-aware warping, which adaptively learns the interpolation weights for pixels of a relatively large neighborhood from their contextual information via a lightweight neural network. based on this learnable warping module, we propose a new end-to-end learningbased framework for novel view synthesis from a set of input source views, in which two additional modules, namely confidence-based blending and feature-assistant spatial refinement, are naturally proposed to handle the occlusion issue and capture the spatial correlation among pixels of the synthesized view, respectively. besides, we also propose a weight-smoothness loss term to regularize the network. experimental results on light field datasets with wide baselines and multi-view datasets show that the proposed method significantly outperforms state-of-the-art methods both quantitatively and visually. the source code is publicly available at https://github.com/mantangguo/cw4vs.",AB_0345
"robust few-shot learning (rfsl), which aims to address noisy labels in few-shot learning, has recently gained considerable attention. existing rfsl methods are based on the assumption that the noise comes from known classes (in-domain), which is inconsistent with many real-world scenarios where the noise does not belong to any known classes (out-of-domain). we refer to this more complex scenario as open-world few-shot learning (ofsl), where in-domain and out-of-domain noise simultaneously exists in few-shot datasets. to address the challenging problem, we propose a unified framework to implement comprehensive calibration from instance to metric. specifically, we design a dual-networks structure composed of a contrastive network and a meta network to respectively extract feature-related intra-class information and enlarged inter-class variations. for instance-wise calibration, we present a novel prototype modification strategy to aggregate prototypes with intra-class and inter-class instance reweighting. for metric-wise calibration, we present a novel metric to implicitly scale the per-class prediction by fusing two spatial metrics respectively constructed by the two networks. in this way, the impact of noise in ofsl can be effectively mitigated from both feature space and label space. extensive experiments on various ofsl settings demonstrate the robustness and superiority of our method. our source codes is available at https://github.com/anyuexuan/ideal.",AB_0345
"neural networks often make predictions relying on the spurious correlations from the datasets rather than the intrinsic properties of the task of interest, facing with sharp degradation on out-of-distribution (ood) test data. existing de-bias learning frameworks try to capture specific dataset bias by annotations but they fail to handle complicated ood scenarios. others implicitly identify the dataset bias by special design low capability biased models or losses, but they degrade when the training and testing data are from the same distribution. in this paper, we propose a general greedy de-bias learning framework (ggd), which greedily trains the biased models and base model. the base model is encouraged to focus on examples that are hard to solve with biased models, thus remaining robust against spurious correlations in the test stage. ggd largely improves models' ood generalization ability on various tasks, but sometimes over-estimates the bias level and degrades on the in-distribution test. we further re-analyze the ensemble process of ggd and introduce the curriculum regularization inspired by curriculum learning, which achieves a good trade-off between in-distribution (id) and out-of-distribution performance. extensive experiments on image classification, adversarial question answering, and visual question answering demonstrate the effectiveness of our method. ggd can learn a more robust base model under the settings of both task-specific biased models with prior knowledge and self-ensemble biased model without prior knowledge. codes are available at https://github.com/geraldhan/ggd.",AB_0345
