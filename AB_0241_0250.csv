AB,NO
"within the last few years, great efforts have been made to study few-shot learning. although general object detection is advancing at a rapid pace, few-shot detection remains a very challenging problem. in this work, we propose a novel decoupled metric network (dmnet) for single-stage few-shot object detection. we design a decoupled representation transformation (drt) and an image-level distance metric learning (idml) to solve the few-shot detection problem. the drt can eliminate the adverse effect of handcrafted prior knowledge by predicting objectness and anchor shape. meanwhile, to alleviate the problem of representation disagreement between classification and location (i.e., translational invariance versus translational variance), the drt adopts a decoupled manner to generate adaptive representations so that the model is easier to learn from only a few training data. as for a few-shot classification in the detection task, we design an idml tailored to enhance the generalization ability. this module can perform metric learning for the whole visual feature, so it can be more efficient than traditional dml due to the merit of parallel inference for multiobjects. based on the drt and idml, our dmnet efficiently realizes a novel paradigm for few-shot detection, called single-stage metric detection. experiments are conducted on the pascal voc dataset and the ms coco dataset. as a result, our method achieves state-of-the-art performance in few-shot object detection. the codes are available at https://github.com/yrqs/dmnet.",AB_0025
"channel pruning has been long studied to compress convolutional neural networks (cnns), which significantly reduces the overall computation. prior works implement channel pruning in an unexplainable manner, which tends to reduce the final classification errors while failing to consider the internal influence of each channel. in this article, we conduct channel pruning in a white box. through deep visualization of feature maps activated by different channels, we observe that different channels have a varying contribution to different categories in image classification. inspired by this, we choose to preserve channels contributing to most categories. specifically, to model the contribution of each channel to differentiating categories, we develop a class-wise mask for each channel, implemented in a dynamic training manner with respect to the input image's category. on the basis of the learned class-wise mask, we perform a global voting mechanism to remove channels with less category discrimination. lastly, a fine-tuning process is conducted to recover the performance of the pruned model. to our best knowledge, it is the first time that cnn interpretability theory is considered to guide channel pruning. extensive experiments on representative image classification tasks demonstrate the superiority of our white-box over many state-of-the-arts (sotas). for instance, on cifar-10, it reduces 65.23% floating point operations per seconds (flops) with even 0.62% accuracy improvement for resnet-110. on ilsvrc-2012, white-box achieves a 45.6% flop reduction with only a small loss of 0.83% in the top-1 accuracy for resnet-50. code is available at https://github.com/zyxxmu/white-box.",AB_0025
"land remote-sensing analysis is a crucial research in earth science. in this work, we focus on a challenging task of land analysis, i.e., automatic extraction of traffic roads from remote-sensing data, which has widespread applications in urban development and expansion estimation. nevertheless, conventional methods either only utilized the limited information of aerial images, or simply fused multimodal information (e.g., vehicle trajectories), thus cannot well recognize unconstrained roads. to facilitate this problem, we introduce a novel neural network framework termed cross-modal message propagation network (cmmpnet), which fully benefits the complementary different modal data (i.e., aerial images and crowdsourced trajectories). specifically, cmmpnet is composed of two deep autoencoders for modality-specific representation learning and a tailor-designed dual enhancement module for cross-modal representation refinement. in particular, the complementary information of each modality is comprehensively extracted and dynamically propagated to enhance the representation of another modality. extensive experiments on three real-world benchmarks demonstrate the effectiveness of our cmmpnet for robust road extraction benefiting from blending different modal data, either using image and trajectory data or image and light detection and ranging (lidar) data. from the experimental results, we observe that the proposed approach outperforms current state-of-the-art methods by large margins. our source code is resealed on the project page http://lingboliu.com/multimodal_road_extraction.html.",AB_0025
"discovering novel visual categories from a set of unlabeled images is a crucial and essential capability for intelligent vision systems since it enables them to automatically learn new concepts with no need for human-annotated supervision anymore. to tackle this problem, existing approaches first pretrain a neural network with a set of labeled images and then fine-tune the network to cluster unlabeled images into a few categorical groups. however, their unified feature representation hits a tradeoff bottleneck between feature preservation on labeled data and feature adaptation on unlabeled data. to circumvent this bottleneck, we propose a residual-tuning approach, which estimates a new residual feature from the pretrained network and adds it with a previous basic feature to compute the clustering objective together. our disentangled representation approach facilitates adjusting visual representations for unlabeled images and overcoming forgetting old knowledge acquired from labeled images, with no need of replaying the labeled images again. in addition, residual-tuning is an efficient solution, adding few parameters and consuming modest training time. our results on three common benchmarks show consistent and considerable gains over other state-of-the-art methods, and further reduce the performance gap to the fully supervised learning setup. moreover, we explore two extended scenarios, including using fewer labeled classes and continually discovering more unlabeled sets, where the results further signify the advantages and effectiveness of our residual-tuning approach against previous approaches. our code is available at https://github.com/liuyudut/restune.",AB_0025
"in recent years, video captioning, which uses natural language to describe video content, has achieved encouraging results. however, most of the previous studies in this area have focused on directly decoding video encoding and have thus rarely explored the role of scene semantics in caption generation, especially cross-language and multimodal. obviously, the same video can be described with different languages, which have different forms and are inherently related. meanwhile, despite high evaluation scores, some generated captions cannot represent the video content with many nonentity words. based on the analysis, in this paper, we propose a cross-language scene semantic guidance caption model. it first learns the high-level scene semantics of a video in different languages, from which multilanguage features are extracted. then, the features characterize the video content and guide the generated captions. they make the captions converge toward the video content. in addition, we also apply a leap sampling method for learning entity words in the model so as to better represent the video content. moreover, experiments on the public msr-vtt and vatex datasets show that our model is effective. finally, we establish a multilingual student classroom behavior caption dataset under an education scenario, providing a basis for research into captioning tasks in the education area. we also apply our model to this dataset and achieve certain results. the dataset is available to download online: https://github.com/bnu-wu/student-class-behavior-dataset/tree/master.",AB_0025
"in this article, we present a new pansharpening method, a zero-reference generative adversarial network (zergan), which fuses low spatial resolution multispectral (lr ms) and high spatial resolution panchromatic (pan) images. in the proposed method, zero-reference indicates that it does not require paired reduced-scale images or unpaired full-scale images for training. to obtain accurate fusion results, we establish an adversarial game between a set of multiscale generators and their corresponding discriminators. through multiscale generators, the fused high spatial resolution ms (hr ms) images are progressively produced from lr ms and pan images, while the discriminators aim to distinguish the differences of spatial information between the hr ms images and the pan images. in other words, the hr ms images are generated from lr ms and pan images after the optimization of zergan. furthermore, we construct a nonreference loss function, including an adversarial loss, spatial and spectral reconstruction losses, a spatial enhancement loss, and an average constancy loss. through the minimization of the total loss, the spatial details in the hr ms images can be enhanced efficiently. extensive experiments are implemented on datasets acquired by different satellites. the results demonstrate that the effectiveness of the proposed method compared with the state-of-the-art methods. the source code is publicly available at https://github.com/rsmagneto/zergan.",AB_0025
"creating synthetic lines is the standard mating mode for commercial pig production. traditional mating performance was evaluated through a strictly designed cross -combination test at the 'breed level' to maximize the benefits of production. the duroc-landraceyorkshire (dly) three-way crossbred production system became the most widely used breeding scheme for pigs. here, we proposed an `individual level' genomic mating procedure that can be applied to commercial pig production with efficient algorithms for estimating marker effects and for allocating the appropriate boar-sow pairs, which can be freely accessed to public in our developed hiblup software at https://www.hiblup.com/tutorials#genomic- mating. a total of 875 duroc boars, 350 landrace-yorkshire sows and 3573 dly pigs were used to carry out the genomic mating to assess the production benefits theoretically. the results showed that genomic mating significantly improved the performances of progeny across different traits compared with random mating, such as the feed conversion rate, days from 30 to 120 kg and eye muscle area could be improved by -0.12, -4.64 d and 2.65 cm(2), respectively, which were consistent with the real experimental validations. overall, our findings indicated that genomic mating is an effective strategy to improve the performances of progeny by maximizing their total genetic merit with consideration of both additive and dominant effects. also, a herd of boars from a richer genetic source will increase the effectiveness of genomic mating further.",AB_0025
"recent developments of deep learning methods have demonstrated their feasibility in liver malignancy diagnosis using ultrasound (us) images. however, most of these methods require manual selection and annotation of us images by radiologists, which limit their practical application. on the other hand, us videos provide more comprehensive morphological information about liver masses and their relationships with surrounding structures than us images, potentially leading to a more accurate diagnosis. here, we developed a fully automated artificial intelligence (ai) pipeline to imitate the workflow of radiologists for detecting liver masses and diagnosing liver malignancy. in this pipeline, we designed an automated mass -guided strategy that used segmentation information to direct diagnostic models to focus on liver masses, thus increasing diagnostic accuracy. the diagnostic models based on us videos utilized bi-directional convolutional long short-term memory modules with an attention -boosted module to learn and fuse spatiotemporal information from consecutive video frames. using a large-scale dataset of 50 063 us images and video frames from 11 468 patients, we developed and tested the ai pipeline and investigated its applications. a dataset of annotated us images is available at https://doi. org/10.5281/zenodo.7272660.",AB_0025
"long non -coding rnas (lncrnas) played essential roles in nearly every biological process and disease. many algorithms were developed to distinguish lncrnas from mrnas in transcriptomic data and facilitated discoveries of more than 600 000 of lncrnas. however, only a tiny fraction (<1%) of 1ncrna transcripts (similar to 4000) were further validated by low -throughput experiments (ev1ncrnas). given the cost and labor-intensive nature of experimental validations, it is necessary to develop computational tools to prioritize those potentially functional lncrnas because many lncrnas from high -throughput sequencing (ht1ncrnas) could be resulted from transcriptional noises. here, we employed deep learning algorithms to separate ev1ncrnas from ht1ncrnas and mrnas. for overcoming the challenge of small datasets, we employed a three -layer deep -learning neural network (dnn) with a k-mer feature as the input and a small convolutional neural network (cnn) with one -hot encoding as the input. three separate models were trained for human (h), mouse (m) and plant (p), respectively. the final concatenated models (ev1ncrna-dpred (h), ev1ncrna-dpred (m) and ev1ncrna-dpred (p)) provided substantial improvement over a previous model based on support-vector-machines (ev1ncrna-pred). for example, ev1ncrna-dpred (h) achieved 0.896 for the area under receiver-operating characteristic curve, compared with 0.582 given by sequence -based ev1ncrna-pred model. the models developed here should be useful for screening 1ncrna transcripts for experimental validations. ev1ncrna-dpred is available as a web server at https://www.sdklab-biophysics-dzu.net/ev1nana-dprediindex.html, and the data and source code can be freely available along with the web server.",AB_0025
"covalent inhibitors have received extensive attentions in the past few decades because of their long residence time, high binding efficiency and strong selectivity. therefore, it is valuable to develop computational tools like molecular docking for modeling of covalent protein-ligand interactions or screening of potential covalent drugs. meeting the needs, we have proposed hcovdock, an efficient docking algorithm for covalent protein-ligand interactions by integrating a ligand sampling method of incremental construction and a scoring function with covalent bond -based energy. tested on a benchmark containing 207 diverse protein-ligand complexes, hcovdock exhibits a significantly better performance than seven other state-of-the-art covalent docking programs (autodock, cov_dox, covdock, fitted, gold, icm-pro and moe). with the criterion of ligand root-mean -squared distance < 2.0 angstrom, hcovdock obtains a high success rate of 70.5% and 93.2% in reproducing experimentally observed structures for top 1 and top 10 predictions. in addition, hcovdock is also validated in virtual screening against 10 receptors of three proteins. hcovdock is computationally efficient and the average running time for docking a ligand is only 5 min with as fast as 1 sec for ligands with one rotatable bond and about 18 min for ligands with 23 rotational bonds. hcovdock can be freely assessed at http://huanglab.phys.hust.edu.crilhcovdock/.",AB_0025
