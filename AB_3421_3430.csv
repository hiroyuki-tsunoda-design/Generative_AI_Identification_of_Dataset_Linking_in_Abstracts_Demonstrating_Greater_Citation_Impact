AB,NO
"zero-shot learning (zsl) aims to learn models that can recognize images of semantically related unseen categories, through transferring attribute-based knowledge learned from training data of seen classes to unseen testing data. as visual attributes play a vital role in zsl, recent embedding-based methods usually focus on learning a compatibility function between the visual representation and the class semantic attributes. while in this work, in addition to simply learning the region embedding of different semantic attributes to maintain the generalization capability of the learned model, we further consider to improve the discrimination power of the learned visual features themselves by contrastive embedding. it exploits both the class-wise and instance-wise supervision for gzsl, under the attribute guided weakly supervised representation learning framework. to further improve the robustness of the zsl model, we also propose to train the model under the consistency regularization constraint, through taking full advantages of self-supervised signals of the image under various perturbed augmentation situations, which could make the model robust to some occluded or un-related attribute regions. extensive experimental results demonstrate the effectiveness of the proposed zsl method, achieving superior performances to state-of-the-art methods on three widely-used benchmark datasets, namely cub, sun, and awa2. our source code is released at https://github.com/koriyn/cc-zsl.",AB_0343
"recently, deep learning-based image compression has made significant progresses, and has achieved better rate-distortion (r-d) performance than the latest traditional method, h.266/vvc, in both ms-ssim metric and the more challenging psnr metric. however, a major problem is that the complexities of many leading learned schemes are too high. in this paper, we propose an efficient and effective image coding framework, which achieves similar r-d performance with lower complexity than the state of the art. first, we develop an improved multi-scale residual block (msrb) that can expand the receptive field and capture global information more efficiently, which further reduces the spatial correlation of the latent representations. second, an importance scaling network is introduced to directly scale the latents to achieve content-adaptive bit allocation without sending side information, which is more flexible than previous importance map methods. third, we apply a post-quantization filter (pqf) to reduce the quantization error, motivated by the sample adaptive offset (sao) filter in video coding. moreover, our experiments show that the performance of the system is less sensitive to the complexity of the decoder. therefore, we design an asymmetric paradigm, in which the encoder employs three stages of msrbs to improve the learning capacity, whereas the decoder only uses one stage of msrb, which reduces the decoder complexity and still yields satisfactory performance. experimental results show that compared to the state-of-the-art method, the encoding and decoding time of the proposed method are about 17 times faster, and the r-d performance is only reduced by about 1% on both kodak and tecnick-40 datasets, which is still better than h.266/vvc(4:4:4) and other leading learning-based methods. our source code is publicly available at https://github.com/fengyurenpingsheng.",AB_0343
"as encryption masks the content of the original image and thus statistical characteristics of the original image cannot be used to compress the encrypted version, compressing an encrypted image efficiently remains a significant challenge today. in this study, a novel encryption-then-lossy-compression (etlc) scheme was developed using nonuniform downsampling and a customized deep network. specifically, the nonuniform downsampling method integrates both uniform and random sampling to achieve an arbitrary compression ratio for an encrypted image. lossy reconstruction from the decrypted and decompressed image is described as a constrained optimization problem, and an etlc-oriented customized deep neural network (etcnn) is elaborately designed to solve this problem. etcnn contains three parts: channel-wise non-local attention including residual group and non-local sparse attention, a residual content supplementation (rcs), and a downsampling constraint (dc), where rcs and dc are customized modules exploiting specific features of the downsampling-based etlc system. extensive experimental simulations show that the proposed scheme outperforms the state-of-the-art etlc methods remarkably, indicating the feasibility and effectiveness of the proposed scheme exploiting the nonuniform downsampling and etcnn-based reconstruction. code is available at https://github.com/hujuanzp/etcnn.",AB_0343
"most brands of modern consumer digital cameras nowadays are able to provide raw-rgb image pairs conveniently, even in the automatic mode. raw images store pixel intensities linearly related to the radiance, which could be beneficial for the image reflection removal (irr) task. however, existing irr solutions, usually directly restoring the background in the non-linear rgb domain, severely overlook the valuable information conveyed by readily-available raw images. such a negligence may limit the performance of irr methods on real-scene images. to mitigate this deficiency, we propose a cascaded raw and rgb restoration network (cr3net) by leveraging both the rgb images and their paired raw versions. specifically, we firstly separate background and reflection layers in the linear raw domain, and then restore the two layers in the non-linear rgb format by converting raw features into the rgb domain. a novel raw-to-rgb module (rrm) is devised to upsample these features and mimic pointwise mappings in the camera image signal processor (isp). in addition, we collect the first real-world dataset that contains paired raw and rgb images for irr. compared with state-of-the-art approaches, our method achieves a significant performance gain of about 2.07db in psnr, 0.028 in ssim, and 0.0123 in lpips tested on the captured dataset. the source code and dataset are available at https://github.com/namecantbenull/raw_rgb_rr.",AB_0343
"as a domain-dependent prior knowledge, side information has been introduced into robust principal component analysis (rpca) to alleviate its degenerate or suboptimal performance in some real applications. it has recently realized that the natural structural information can be better retained if the observed data is kept in the original tensor form rather than matricizing it or other order reduction means. hence, studies on rpca of tensor version have attracted more and more attentions. to share the merits from both direct tensor modeling and side information, we propose three models to deal with the problem of tensor rpca with side information based on tensor singular value decomposition (t-svd). to solve these models, we develop an efficient algorithm with convergence guarantee using the well-known alternating direction method of multiplier. extensive experimental studies on both synthetic and real-world tensor data have been carried out to demonstrate the superiority of the proposed models over several other state-of-the-arts. our code is released at https://github.com/zsj9509/tpcpsf.",AB_0343
"this article proposes the scalable cross-modality compression (scmc) paradigm, in which the image compression problem is further cast into a representation task by hierarchically sketching the image with different modalities. herein, we adopt the conceptual organization philosophy to model the overwhelmingly complicated visual patterns, based upon the semantic, structure, and signal level representation accounting for different tasks. the scmc paradigm that incorporates the representation at different granularities supports diverse application scenarios, such as high-level semantic communication and low-level image reconstruction. the decoder, which enables the recovery of the visual information, benefits from the scalable coding based upon the semantic, structure, and signal layers. qualitative and quantitative results demonstrate that the scmc can convey accurate semantic and perceptual information of images, especially at low bitrates, and promising rate-distortion performance has been achieved compared to state-of-the-art methods. the code will be available online https://github.com/ppingzhang/scmc.",AB_0343
"person re-identification (re-id) plays an important role in many areas such as robotics, multimedia and forensics. however, it becomes difficult when considering long-term scenarios, due to changing clothes irregularly for people. therefore, cloth-changing person re-identification (cc-reid) has attracted more attention recently. cc-reid aims to identify the same person but with different clothes. its main challenge is how to disentangle clothes-irrelevant features, such as face, shape, body, etc. most existing methods force the model to learn clothes-irrelevant features by changing the colour of clothes or reconstructing people dressed in different colours. however, due to the lack of the ground truth for supervision, these methods inevitably introduce noises which spoil the discriminativeness of features and lead to uncontrollable disentanglement. in this paper, we propose a novel disentanglement framework, called deep component reconstruction re-id (dcr-reid), which can disentangle the clothes-irrelevant features and the clothes-relevant features in a controllable manner. specifically, we propose a component reconstruction disentanglement (crd) module to disentangle the clothes-irrelevant features and the clothes-relevant features based on the reconstruction of human component regions. in addition, we propose a deep assembled disentanglement (dad) module, which further improves the discriminativeness of these disentangled features. extensive experiments on three real-world benchmark cc-reid datasets, ltcc, prcc, and ccvid, are conducted to demonstrate the effectiveness of the proposed dcr-reid. empirical studies show that our dcr-reid achieves the state-of-the-art performance against the other cc-reid methods. the source code of this paper is available at https://github.com/pku-icst-mipl/dcr-reid_tcsvt2023.",AB_0343
"in federated learning (fl), poisoning attack invades the whole system by manipulating the client data, tampering with the training target, and performing any desired behaviors. until now, numerous poisoning attacks have been carefully studied, however they are still practically challenged in real-world scenarios from two aspects: (i) multiple malicious client selections - poisoning attacks are only successfully launched when the malicious client has been chosen in enough epochs (i.e., more than half of the epochs); (ii) long-term poisoning training - the poisoning training usually needs much more epochs than the normal training (i.e., 3 times longer than normal training), both are unavailable in real cases. to address these overlooked problems, we propose a poisoning enhanced attack (poe) against fls, which is a general poisoning reinforcement framework. it is designed to transfer partially predicted probabilities of the source class to the target one. thus, the inter-class distance between the source class and the target class is narrowed down in feature space for easier attacks. towards this goal, the attack client uses label smoothing to change the model prediction distribution, dragging the global model in the direction that is favorable for poisoning. extensive experiments show that poe can significantly enhance the attack success rate (similar to x 8.4 on average) in practical fls with normal training epochs. it also achieves stateof-the-art adaptive attack performance against defensive fls (i.e., robust aggregations). the code of poe could be downloaded at https://github.com/leon022/poisoning_enhancement.",AB_0343
"in general, videos are powerful at recording physical patterns (e.g., spatial layout) while texts are great at describing abstract symbols (e.g., emotion). when video and text are used in multi-modal tasks, they are claimed to be complementary and their distinct information is crucial. however, when it comes to cross-modal tasks (e.g., retrieval), existing works usually use their common part in the form of common space learning while their distinct information is abandoned. in this paper, we argue that distinct information is also beneficial for cross-modal retrieval. to address this problem, we propose a divide-and-conquer learning approach, namely complementarity-aware space learning (csl), by recasting this challenge into learning of two spaces (i.e., latent and symbolic spaces) to simultaneously explore their common and distinct information by considering multi-modal complementary character. specifically, we first propose to learn a symbolic space from video with a memory-based video encoder and a symbolic generator. in contrast, we also introduce learning a latent space from text with a text encoder and a memory-based latent feature selector. finally, we propose a complementarity-aware loss by integrating two spaces to facilitate video-text retrieval tasks. extensive experiments show that our approach outperforms existing state-of-the-art methods by 5.1%, 2.1% and 0.9% of r@10 for text-to-video retrieval on three benchmarks, respectively. ablation study also verifies that the distinct information from video and text improves the retrieval performance. trained models and source code have been released at https://github.com/novamind-z/csl.",AB_0343
"recent existing methods generally adopt a simple concatenation or addition strategy to integrate features at the fusion layer, failing to adequately consider the intrinsic characteristics of different modal images and feature interaction of different scales, which may produce a limited fusion performance. toward this end, we introduce a cross-scale iterative attentional adversarial fusion network, namely crossfuse. more specifically, in the generator, we design a cross-modal attention integrated module to merge the intrinsic content of different modal images. the parallel spatial-independent and channel-independent pathways are proposed to calculate the attentional weights, which are assigned to measure the activity levels of source images at the same scale. moreover, we construct a cross-scale iterative decoder framework to interact with different modality features at different scales, which can constantly optimize their activity levels. by this means, the generator learns to integrate their modality characteristics via attentional weights in an iterative manner, and the generated result characterizes competitive infrared radiant intensity and distinct visible detail description. extensive experiments on three different benchmarks demonstrate that our crossfuse outperforms other nine state-of-the-art methods in terms of fusion performance, generalization ability and computational efficiency. our codes will be released at https://github.com/zhishe-wang/crossfuse.",AB_0343
