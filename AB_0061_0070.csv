AB,NO
"background: personalised radiotherapy can improve treatment outcomes of patients with head and neck cancer (hnc), where currently a 'one-dose-fits-all' approach is the stan-dard. the aim was to establish individualised outcome prediction based on multi-institutional international 'big-data' to facilitate risk-based stratification of patients with hnc.methods: the data of 4611 hnc radiotherapy patients from three academic cancer centres were split into four cohorts: a training (n = 2241), independent test (n = 786), and external valida-tion cohorts 1 (n = 1087) and 2 (n = 497). tumour-and patient-related clinical variables were considered in a machine learning pipeline to predict overall survival (primary end-point) and local and regional tumour control (secondary end-points); serially, imaging features were consid-ered for optional model improvement. finally, patients were stratified into high-, intermediate-, and low-risk groups.results: performance score, ajcc8th stage, pack-years, and age were identified as predictors for overall survival, demonstrating good performance in both the training cohort (c-index = 0.72 [95% ci, 0.66-0.77]) and in all three validation cohorts (c-indices: 0.76 [0.69-0.83], 0.73 [0.68-0.77], and 0.75 [0.68-0.80]). excellent stratification of patients with hnc into high, interme-diate, and low mortality risk was achieved; with 5-year overall survival rates of 17-46% for the high-risk group compared to 92-98% for the low-risk group. the addition of morphological image feature further improved the performance (c-index = 0.73 [0.64-0.81]). these models are integrated in a clinic-ready interactive web interface: https://uic-evl.github.io/hnc-predictor/conclusions: robust model-based prediction was able to stratify patients with hnc in distinct high, intermediate, and low mortality risk groups. this can effectively be capitalised for perso-nalised radiotherapy, e.g., for tumour radiation dose escalation/de-escalation. 2022 the authors. published by elsevier ltd. this is an open access article under the cc by license ().",AB_0007
"background diagnosing heparin-induced thrombocytopenia (hit) at the bedside remains challenging, exposing a significant number of patients at risk of delayed diagnosis or overtreatment. we hypothesized that machine-learning algorithms could be utilized to develop a more accurate and user-friendly diagnostic tool that integrates diverse clinical and laboratory information and accounts for complex interactions. methods we conducted a prospective cohort study including 1393 patients with suspected hit between 2018 and 2021 from 10 study centers. detailed clinical information and laboratory data were collected, and various immunoassays were conducted. the washed platelet heparin-induced platelet activation assay (hipa) served as the reference standard. findings hipa diagnosed hit in 119 patients (prevalence 8.5%). the feature selection process in the training dataset (75% of patients) yielded the following predictor variables: (1) immunoassay test result, (2) platelet nadir, (3) unfractionated heparin use, (4) crp, (5) timing of thrombocytopenia, and (6) other causes of thrombocytopenia. the best performing models were a support vector machine in case of the chemiluminescent immunoassay (clia) and the elisa, as well as a gradient boosting machine in particle-gel immunoassay (pagia). in the validation dataset (25% of patients), the auroc of all models was 0.99 (95% ci: 0.97, 1.00). compared to the currently recommended diagnostic algorithm (4ts score, immunoassay), the numbers of false-negative patients were reduced from 12 to 6 (-50.0%; elisa), 9 to 3 (-66.7%, pagia) and 14 to 5 (-64.3%; clia). the numbers of false-positive individuals were reduced from 87 to 61 (-29.8%; elisa), 200 to 63 (-68.5%; pagia) and increased from 50 to 63 (+29.0%) for the clia. interpretation our user-friendly machine-learning algorithm for the diagnosis of hit (https://toradi-hit.org) was substantially more accurate than the currently recommended diagnostic algorithm. it has the potential to reduce delayed diagnosis and overtreatment in clinical practice. future studies shall validate this model in wider settings. copyright (c) 2022 the author(s). published by elsevier ltd.",AB_0007
"gencode produces high quality gene and transcript annotation for the human and mouse genomes. all gencode annotation is supported by experimental data and serves as a reference for genome biology and clinical genomics. the gencode consortium generates targeted experimental data, develops bioinformatic tools and carries out analyses that, along with externally produced data and methods, support the identification and annotation of transcript structures and the determination of their function. here, we present an update on the annotation of human and mouse genes, including developments in the tools, data, analyses and major collaborations which underpin this progress. for example, we report the creation of a set of non-canonical orfs identified in gencode transcripts, the lrgasp collaboration to assess the use of long transcriptomic data to build transcript models, the progress in collaborations with refseq and uniprot to increase convergence in the annotation of human and mouse protein-coding genes, the propagation of gencode across the human pan-genome and the development of new tools to support annotation of regulatory features by gencode. our annotation is accessible via ensembl, the ucsc genome browser and https://www.gencodegenes.org.",AB_0007
"the national genomics data center (ngdc), part of the china national center for bioinformation (cncb), provides a family of database resources to support global academic and industrial communities. with the explosive accumulation of multi-omics data generated at an unprecedented rate, cncb-ngdc constantly expands and updates core database resources by big data archive, integrative analysis and value-added curation. in the past year, efforts have been devoted to integrating multiple omics data, synthesizing the growing knowledge, developing new resources and upgrading a set of major resources. particularly, several database resources are newly developed for infectious diseases and microbiology (mpoxvr, kgcov, propan), cancer-trait association (ascancer atlas, twas atlas, brain catalog, ccas) as well as tropical plants (tcod). importantly, given the global health threat caused by monkeypox virus and sars-cov-2, cncb-ngdc has newly constructed the monkeypox virus resource, along with frequent updates of sars-cov-2 genome sequences, variants as well as haplotypes. all the resources and services are publicly accessible at https://ngdc.cncb.ac.cn.",AB_0007
"the aim of the uniprot knowledgebase is to provide users with a comprehensive, high-quality and freely accessible set of protein sequences annotated with functional information. in this publication we describe enhancements made to our data processing pipeline and to our website to adapt to an ever-increasing information content. the number of sequences in uniprotkb has risen to over 227 million and we are working towards including a reference proteome for each taxonomic group. we continue to extract detailed annotations from the literature to update or create reviewed entries, while unreviewed entries are supplemented with annotations provided by automated systems using a variety of machine-learning techniques. in addition, the scientific community continues their contributions of publications and annotations to uniprot entries of their interest. finally, we describe our new website (https://www.uniprot.org/), designed to enhance our users' experience and make our data easily accessible to the research community. this interface includes access to alphafold structures for more than 85% of all entries as well as improved visualisations for subcellular localisation of proteins.",AB_0007
"despite achieving promising results in a breadth of medical image segmentation tasks, deep neural networks (dnns) require large training datasets with pixel-wise annotations. obtaining these curated datasets is a cumbersome process which limits the applicability of dnns in scenarios where annotated images are scarce. mixed supervision is an appealing alternative for mitigating this obstacle. in this setting, only a small fraction of the data contains complete pixel-wise annotations and other images have a weaker form of supervision, e.g., only a handful of pixels are labeled. in this work, we propose a dual-branch architecture, where the upper branch (teacher) receives strong annotations, while the bottom one (student) is driven by limited supervision and guided by the upper branch. combined with a standard cross-entropy loss over the labeled pixels, our novel formulation integrates two important terms: (i) a shannon entropy loss defined over the less-supervised images, which encourages confident student predictions in the bottom branch; and (ii) a kullback-leibler (kl) divergence term, which transfers the knowledge (i.e., predictions) of the strongly supervised branch to the less-supervised branch and guides the entropy (student-confidence) term to avoid trivial solutions. we show that the synergy between the entropy and kl divergence yields substantial improvements in performance. we also discuss an interesting link between shannon-entropy minimization and standard pseudo -mask generation, and argue that the former should be preferred over the latter for leveraging information from unlabeled pixels. we evaluate the effectiveness of the proposed formulation through a series of quantitative and qualitative experiments using two publicly available datasets. results demonstrate that our method significantly outperforms other strategies for semantic segmentation within a mixed-supervision framework, as well as recent semi-supervised approaches. moreover, in line with recent observations in classification, we show that the branch trained with reduced supervision and guided by the top branch largely outperforms the latter. our code is publicly available: https://github.com/by-liu/confkd.",AB_0007
"video instance segmentation is one of the core problems in computer vision. formulating a purely learning-based method, which models the generic track management required to solve the video instance segmentation task, is a highly challenging problem. in this work, we propose a novel learning framework where the entire video instance segmentation problem is modeled jointly. to this end, we design a graph neural network that in each frame jointly processes all detections and a memory of previously seen tracks. past information is considered and processed via a recurrent connection. we demonstrate the effectiveness of the proposed approach in comprehensive experiments. our approach operates online at over 25 fps and obtains 16.3 ap on the challenging ovis benchmark, setting a new state-of-the-art. we further conduct detailed ablative experiments that validate the different aspects of our approach. code is available at https://github.com/emibr948/rgnnvis-plusplus.",AB_0007
"the eggnog (evolutionary gene genealogy non-supervised orthologous groups) database is a bioinformatics resource providing orthology data and comprehensive functional information for organisms from all domains of life. here, we present a major update of the database and website (version 6.0), which increases the number of covered organisms to 12 535 reference species, expands functional annotations, and implements new functionality. in total, eggnog 6.0 provides a hierarchy of over 17m orthologous groups (ogs) computed at 1601 taxonomic levels, spanning 10 756 bacterial, 457 archaeal and 1322 eukaryotic organisms. ogs have been thoroughly annotated using recent knowledge from functional databases, including kegg, gene ontology, uniprotkb, bigg, cazy, card, pfam and smart. eggnog also offers phylogenetic trees for all ogs, maximising utility and versatility for end users while allowing researchers to investigate the evolutionary history of speciation and duplication events as well as the phylogenetic distribution of functional terms within each og. furthermore, the eggnog 6.0 web-site contains new functionality to mine orthology and functional data with ease, including the possibility of generating phylogenetic profiles for multiple ogs across species or identifying single-copy ogs at custom taxonomic levels. eggnog 6.0 is available at http://eggnog6.embl.de.",AB_0007
"with an ever-increasing amount of (meta)genomic data being deposited in sequence databases, (meta)genome mining for natural product biosynthetic pathways occupies a critical role in the discovery of novel pharmaceutical drugs, crop protection agents and biomaterials. the genes that encode these pathways are often organised into biosynthetic gene clusters (bgcs). in 2015, we defined the minimum information about a biosynthetic gene cluster (mibig): a standardised data format that describes the minimally required information to uniquely characterise a bgc. we simultaneously constructed an accompanying online database of bgcs, which has since been widely used by the community as a reference dataset for bgcs and was expanded to 2021 entries in 2019 (mibig 2.0). here, we describe mibig 3.0, a database update comprising large-scale validation and re-annotation of existing entries and 661 new entries. particular attention was paid to the annotation of compound structures and biological activities, as well as protein domain selectivities. together, these new features keep the database upto-date, and will provide new opportunities for the scientific community to use its freely available data, e.g. for the training of new machine learning models to predict sequence-structure-function relationships for diverse natural products. mibig 3.0 is accessible online at https://mibig.secondarymetabolites.org/. [graphics] .",AB_0007
"association rule mining (arm) is defined by its crucial role in finding common pattern in data mining it has different types such as fuzzy, binary, numerical. in this paper, we introduce a multi-objective orthogonal mould algorithm (moosma) with numerical association rule mining (narm) which is a different type of arm. existing algorithms that deal with the narm problem can be classified into three categories: distribution, discretization and optimization. the proposed approach belongs to the optimization category which is considered as a better way to deal with the problem. our main objective is based on four efficiency measures related to each association: support, confidence, comprehensibility, interestingness. to test the performance of our approach, we started by testing our method on widely known generalized dynamic benchmark tests called cec' 09. this benchmark is composed of 20 test functions: 10 functions without constraints and 10 functions with constraints. secondly, we applied our algorithm to solve narm problem using 10 frequently used real-world datasets. experimental analysis shows that our algorithm moosma has better results in terms of average support, average confidence, average lift, average certain factor and average netconf. note that source code of the moosma algorithm is publicly available at https://github.com/gaithmanita/moosma.",AB_0007
