AB,NO
"previous work on plant disease detection demonstrated that object detectors generally suffer from degraded training data, and annotations with noise may cause the training task to fail. well-annotated datasets are therefore crucial to build a robust detector. however, a good label set generally requires much expert knowledge and meticulous work, which is expensive and time-consuming. this paper aims to learn robust feature representations with inaccurate bounding boxes, thereby reducing the model requirements for annotation quality. specifically, we analyze the distribution of noisy annotations in the real world. a teacher-student learning paradigm is proposed to correct inaccurate bounding boxes. the teacher model is used to rectify the degraded bounding boxes, and the student model extracts more robust feature representations from the corrected bounding boxes. furthermore, the method can be easily generalized to semi-supervised learning paradigms and auto-labeling techniques. experimental results show that applying our method to the faster-rcnn detector achieves a 26% performance improvement on the noisy dataset. besides, our method achieves approximately 75% of the performance of a fully supervised object detector when 1% of the labels are available. overall, this work provides a robust solution to real-world location noise. it alleviates the challenges posed by noisy data to precision agriculture, optimizes data labeling technology, and encourages practitioners to further investigate plant disease detection and intelligent agriculture at a lower cost. the code will be released at https://github.com/jiuqingdong/ts_oamil-for-plant-disease-detection.",AB_0081
"facial wrinkles are important indicators of human aging. recently, a method using deep learning and a semi-automatic labeling was proposed to segment facial wrinkles, which showed much better performance than conventional image-processing-based methods. however, the difficulty of wrinkle segmentation remains challenging due to the thinness of wrinkles and their small proportion in the entire image. therefore, performance improvement in wrinkle segmentation is still necessary. to address this issue, we propose a novel loss function that takes into account the thickness of wrinkles based on the semi-automatic labeling approach. first, considering the different spatial dimensions of the decoder in the u-net architecture, we generated weighted wrinkle maps from ground truth. these weighted wrinkle maps were used to calculate the training losses more accurately than the existing deep supervision approach. this new loss computation approach is defined as weighted deep supervision in our study. the proposed method was evaluated using an image dataset obtained from a professional skin analysis device and labeled using semi-automatic labeling. in our experiment, the proposed weighted deep supervision showed higher jaccard similarity index (jsi) performance for wrinkle segmentation compared to conventional deep supervision and traditional image processing methods. additionally, we conducted ex-periments on the labeling using a semi-automatic labeling approach, which had not been explored in previous research, and compared it with human labeling. the semi-automatic labeling technology showed more consistent wrinkle labels than human-made labels. furthermore, to assess the scalability of the proposed method to other domains, we applied it to retinal vessel segmentation. the results demonstrated superior performance of the proposed method compared to existing retinal vessel segmentation approaches. in conclusion, the proposed method offers high performance and can be easily applied to various biomedical domains and u-net-based architectures. therefore, the proposed approach will be beneficial for various biomedical imaging approaches. to facilitate this, we have made the source code of the proposed method publicly available at: https://github. com/resemin/weighteddeepsupervision.",AB_0081
"background the domestic dog, canis lupus familiaris, is a companion animal for humans as well as an animal model in cancer research due to similar spontaneous occurrence of cancers as humans. despite the social and biological importance of dogs, the catalogue of genomic variations and transcripts for dogs is relatively incomplete.results we developed caniso, a new database to hold a large collection of transcriptome profiles and genomic variations for domestic dogs. caniso provides 87,692 novel transcript isoforms and 60,992 known isoforms from whole transcriptome sequencing of canine tumors (n = 157) and their matched normal tissues (n = 64). caniso also provides genomic variation information for 210,444 unique germline single nucleotide polymorphisms (snps) from the whole exome sequencing of 183 dogs, with a query system that searches gene- and transcript-level information as well as covered snps. transcriptome profiles can be compared with corresponding human transcript isoforms at a tissue level, or between sample groups to identify tumor-specific gene expression and alternative splicing patterns.conclusions caniso is expected to increase understanding of the dog genome and transcriptome, as well as its functional associations with humans, such as shared/distinct mechanisms of cancer. caniso is publicly available at https://www.kobic.re.kr/caniso/.",AB_0081
"as a graph data mining task, graph classification has high academic value and wide practical application. among them, the graph neural network-based method is one of the mainstream methods. most graph neural networks (gnns) follow the message passing paradigm and can be called message passing neural networks (mpnns), achieving good results in structural data-related tasks. however, it has also been reported that these methods suffer from over-squashing and limited expressive power. in recent years, many works have proposed different solutions to these problems separately, but none has yet considered these shortcomings in a comprehensive way. after considering these several aspects comprehensively, we identify two specific defects: information loss caused by local information aggregation, and an inability to capture higher-order structures. to solve these issues, we propose a plug-and-play framework based on commute time distance (ctd), in which information is propagated in commute time distance neighborhoods. by considering both local and global graph connections, the commute time distance between two nodes is evaluated with reference to the path length and the number of paths in the whole graph. moreover, the proposed framework ctd-mpnns (commute time distance-based message passing neural networks) can capture higher-order structural information by utilizing commute paths to enhance the expressive power of gnns. thus, our proposed framework can propagate and aggregate messages from defined important neighbors and model more powerful gnns. we conduct extensive experiments using various real-world graph classification benchmarks. the experimental performance demonstrates the effectiveness of our framework. codes are released on https://github.com/haldate-yu/ctd-mpnns.",AB_0081
"contrastive learning has recently achieved remarkable success in many domains including graphs. however contrastive loss, especially for graphs, requires a large number of negative samples which is unscalable and computationally prohibitive with a quadratic time complexity. sub-sampling is not optimal. incorrect negative sampling leads to sampling bias. in this work, we propose a meta-node based approximation technique that is (a) simple, (b) canproxy all negative combinations (c) in quadratic cluster size time complexity, (d) at graph level, not node level, and (e) exploit graph sparsity. by replacing node-pairs with additive cluster-pairs, we compute the negatives in cluster-time at graph level. the resulting proxy approximated meta-node contrastive (pamc) loss, based on simple optimized gpu operations, captures the full set of negatives, yet is efficient with a linear time complexity. by avoiding sampling, we effectively eliminate sample bias. we meet the criterion for larger number of samples, thus achieving block-contrastiveness, which is proven to outperform pair-wise losses. we use learnt soft cluster assignments for the meta-node construction, and avoid possible heterophily and noise added during edge creation. theoretically, we show that real world graphs easily satisfy conditions necessary for our approximation. empirically, we show promising accuracy gains over state-of-the-art graph clustering on 6 benchmarks. importantly, we gain substantially in efficiency; over 2x reduction in training time and over 5x in gpu memory reduction. additionally, our embeddings, combined with a single learnt linear transformation, is sufficient for node classification; we achieve state-of-the-art on citeseer classification benchmark. code:https://github.com/gayanku/pamc",AB_0081
"background. although preregistration can reduce researcher bias and increase transparency in primary research settings, it is less applicable to secondary data analysis. an alternative method that affords additional protection from researcher bias, which cannot be gained from conventional forms of preregistration alone, is an explore and confirm analysis workflow (ecaw). in this workflow, a data management organization initially provides access to only a subset of their dataset to researchers who request it. the researchers then prepare an analysis script based on the subset of data, upload the analysis script to a registry, and then receive access to the full dataset. ecaws aim to achieve similar goals to preregistration, but make access to the full dataset contingent on compliance. the present survey aimed to garner information from the research community where ecaws could be applied-employing the avon longitudinal study of parents and children (alspac) as a case example. methods. we emailed a web-based survey to researchers who had previously applied for access to alspac's transgenerational observational dataset. results. we received 103 responses, for a 9% response rate. the results suggest that-at least among our sample of respondents-ecaws hold the potential to serve their intended purpose and appear relatively acceptable. for example, only 10% of respondents disagreed that alspac should run a study on ecaws (versus 55% who agreed). however, as many as 26% of respondents agreed that they would be less willing to use alspac data if they were required to use an ecaw (versus 45% who disagreed). conclusion. our data and findings provide information for organizations and individuals interested in implementing ecaws and related interventions. preregistration. https://osf.io/g2fw5 deviations from the preregistration are outlined in electronic supplementary material a.",AB_0081
"successful ultrasound-guided supraclavicular block (scb) requires the understanding of sonoanatomy and identification of the optimal view. segmentation using a convolutional neural network (cnn) is limited in clearly determining the optimal view. the present study describes the development of a computer-aided diagnosis (cadx) system using a cnn that can determine the optimal view for complete scb in real time. the aim of this study was the development of computer-aided diagnosis system that aid non-expert to determine the optimal view for complete supraclavicular block in real time. ultrasound videos were retrospectively collected from 881 patients to develop the cadx system (600 to the training and validation set and 281 to the test set). the cadx system included classification and segmentation approaches, with residual neural network (resnet) and u-net, respectively, applied as backbone networks. in the classification approach, an ablation study was performed to determine the optimal architecture and improve the performance of the model. in the segmentation approach, a cascade structure, in which u-net is connected to resnet, was implemented. the performance of the two approaches was evaluated based on a confusion matrix. using the classification approach, resnet34 and gated recurrent units with augmentation showed the highest performance, with average accuracy 0.901, precision 0.613, recall 0.757, f1-score 0.677 and auroc 0.936. using the segmentation approach, u-net combined with resnet34 and augmentation showed poorer performance than the classification approach. the cadx system described in this study showed high performance in determining the optimal view for scb. this system could be expanded to include many anatomical regions and may have potential to aid clinicians in real-time settings. trial registration the protocol was registered with the clinical trial registry of korea (kct0005822, https://cris.nih.go.kr).",AB_0081
"backgroundidentification of pleiotropic variants associated with multiple phenotypic traits has received increasing attention in genetic association studies. overlapping genetic associations from multiple traits help to detect weak genetic associations missed by single-trait analyses. many statistical methods were developed to identify pleiotropic variants with most of them being limited to quantitative traits when pleiotropic effects on both quantitative and qualitative traits have been observed. this is a statistically challenging problem because there does not exist an appropriate multivariate distribution to model both quantitative and qualitative data together. alternatively, meta-analysis methods can be applied, which basically integrate summary statistics of individual variants associated with either a quantitative or a qualitative trait without accounting for correlations among genetic variants.resultswe propose a new statistical selection method based on a unified selection score quantifying how a genetic variant, i.e., a pleiotropic variant associates with both quantitative and qualitative traits. in our extensive simulation studies where various types of pleiotropic effects on both quantitative and qualitative traits were considered, we demonstrated that the proposed method outperforms the existing meta-analysis methods in terms of true positive selection. we also applied the proposed method to a peanut dataset with 6 quantitative and 2 qualitative traits, and a cowpea dataset with 2 quantitative and 6 qualitative traits. we were able to detect some potentially pleiotropic variants missed by the existing methods in both analyses.conclusionsthe proposed method is able to locate pleiotropic variants associated with both quantitative and qualitative traits. it has been implemented into an r package 'uniss', which can be downloaded from http://github.com/statpng/uniss.",AB_0081
"remimazolam, an ultrashort-acting benzodiazepine, allows for rapid and reliable arousal. rapid awakening using remimazolam may be beneficial in transcatheter aortic valve replacement (tavr), as it allows rapid detection of neurologic deficits. the purpose of this study was to compare arousal time and outcomes between monitored anesthesia care (mac) with remimazolam and remifentanil and conventional mac with dexmedetomidine, propofol, and remifentanil. this study was a single center retrospective study. all tavr cases performed under mac (mac-tavr) at our institution between 2019 and 2021 were included. patients were classified by anesthesia method into remimazolam and dexmedetomidine groups. among 258 mac-tavr patients, 253 were enrolled. after propensity score matching, 76 patients were assigned to each group. the time from end of drug-administration to arousal [20.0 (16.0, 24.0) min vs. 38.5 (30.0, 56.3) min, p < 0.0001] and the time from attempted-arousal to arousal [1.0 (1.0, 1.0) min vs. 12.5 (3.0, 26.8) min, p < 0.0001] were significantly shorter in the remimazolam group. there was no significant difference in the length of icu stay [2.0 (2.0, 2.0) days vs. 2.0 (2.0, 2.0) days, p = 0.157] and postoperative hospital stay [ 6.0 (4.0, 9.0) days vs. 5.0 (4.0, 8.0) days, p = 0.262]. trial registration: clinical trial number: r03-123, registry url: https://center6.umin.ac.jp/cgi-openbin/ctr/ctr_view.cgi?recptno=r000051635registration number: umin000045195, principal investigator's name: atsuhiro kitaura,date of registration: 20 august 2021.",AB_0081
"backgrounddeep-intronic variants that alter rna splicing were ineffectively evaluated in the search for the cause of genetic diseases. determination of such pathogenic variants from a vast number of deep-intronic variants (approximately 1,500,000 variants per individual) represents a technical challenge to researchers. thus, we developed a pathogenicity predictor for deep-intronic variants causing aberrant splicing (pdivas) to easily detect pathogenic deep-intronic variants.resultspdivas was trained on an ensemble machine-learning algorithm to classify pathogenic and benign variants in a curated dataset. the dataset consists of manually curated pathogenic splice-altering variants (savs) and commonly observed benign variants within deep introns. splicing features and a splicing constraint metric were used to maximize the predictive sensitivity and specificity, respectively. pdivas showed an average precision of 0.92 and a maximum mcc of 0.88 in classifying these variants, which were the best of the previous predictors. when pdivas was applied to genome sequencing analysis on a threshold with 95% sensitivity for reported pathogenic savs, an average of 27 pathogenic candidates were extracted per individual. furthermore, the causative variants in simulated patient genomes were more efficiently prioritized than the previous predictors.conclusionincorporating pdivas into variant interpretation pipelines will enable efficient detection of disease-causing deep-intronic savs and contribute to improving the diagnostic yield. pdivas is publicly available at https://github.com/shiro-kur/pdivas.",AB_0081
