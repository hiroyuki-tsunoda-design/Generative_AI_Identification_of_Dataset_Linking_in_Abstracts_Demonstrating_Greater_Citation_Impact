AB,NO
"existing supervised quantization methods usually learn the quantizers from pair-wise, triplet, or anchor-based losses, which only capture their relationship locally without aligning them globally. this may cause an inadequate use of the entire space and a severe intersection among different semantics, leading to inferior retrieval performance. furthermore, to enable quantizers to learn in an end-to-end way, current practices usually relax the non-differentiable quantization operation by substituting it with softmax, which unfortunately is biased, leading to an unsatisfying suboptimal solution. to address the above issues, we present spherical centralized quantization (scq), which contains a priori knowledge based feature (pkfa) module for the global alignment of feature vectors, and an annealing regulation semantic quantization (arsq) module for low-biased optimization. specifically, the pkfa module first applies semantic center allocation (sca) to obtain semantic centers based on prior knowledge, and then adopts centralized feature alignment (cfa) to gather feature vectors based on corresponding semantic centers. the sca and cfa globally optimize the inter-class separability and intra-class compactness, respectively. after that, the arsq module performs a partial-soft relaxation to tackle biases, and an annealing regulation quantization loss for further addressing the local optimal solution. experimental results show that our scq outperforms state-of-the-art algorithms by a large margin (2.1%, 3.6%, 5.5% map respectively) on cifar-10, nus-wide, and imagenet with a code length of 8 bits. codes are publicly available:https://github.com/zzb111/spherical-centralized-quantization.",AB_0378
"aggregating neighbor features is essential for point cloud neural network. in the existing work, each point in the cloud may inevitably be selected as the neighbors of multiple aggregation centers, as all centers will gather neighbor features from the whole point cloud independently. thus, each point has to participate in the calculation repeatedly, generating redundant duplicates in the memory, leading to intensive computation costs and memory consumption. meanwhile, to pursue higher accuracy, previous methods often rely on a complex local aggregator to extract fine geometric representation, further slowing down the processing pipeline. to address these issues, we propose a new local aggregator of linear complexity for point cloud analysis, coined as app. specifically, we introduce an auxiliary container as an anchor to exchange features between the source point and the aggregating center. each source point pushes its feature to only one auxiliary container, and each center point pulls features from only one auxiliary container. this avoids the re-computation issue of each source point. to facilitate the learning of the local structure of point cloud, we use an online normal estimation module to provide explainable geometric information to enhance our app modeling capability. our built network is more efficient than all the previous baselines with a clear margin while still consuming a lower memory. experiments on classification and semantic segmentation demonstrate that app-net reaches comparable accuracies to other networks. in the classification task, it can process more than 10,000 samples per second with less than 10gb of memory on a single gpu. we will release the code at https://github.com/mcg-nju/ app-net.",AB_0378
"compared to typical multi-sensor systems, monocular 3d object detection has attracted much attention due to its simple configuration. however, there is still a significant gap between lidar-based and monocular-based methods. in this paper, we find that the ill-posed nature of monocular imagery can lead to depth ambiguity. specifically, objects with different depths can appear with the same bounding boxes and similar visual features in the 2d image. unfortunately, the network cannot accurately distinguish different depths from such non-discriminative visual features, resulting in unstable depth training. to facilitate depth learning, we propose a simple yet effective plug-and-play module, one bounding box multiple objects (obmo). concretely, we add a set of suitable pseudo labels by shifting the 3d bounding box along the viewing frustum. to constrain the pseudo-3d labels to be reasonable, we carefully design two label scoring strategies to represent their quality. in contrast to the original hard depth labels, such soft pseudo labels with quality scores allow the network to learn a reasonable depth range, boosting training stability and thus improving final performance. extensive experiments on kitti and waymo benchmarks show that our method significantly improves state-of-the-art monocular 3d detectors by a significant margin (the improvements under the moderate setting on kitti validation set are 1.82 similar to 10.91% map in bev and 1.18 similar to 9.36% map in 3d). codes have been released at https://github.com/mrsempress/obmo.",AB_0378
"while adversarial training and its variants have shown to be the most effective algorithms to defend against adversarial attacks, their extremely slow training process makes it hard to scale to large datasets like imagenet. the key idea of recent works to accelerate adversarial training is to substitute multi-step attacks (e.g., pgd) with single-step attacks (e.g., fgsm). however, these single-step methods suffer from catastrophic overfitting, where the accuracy against pgd attack suddenly drops to nearly 0% during training, and the network totally loses its robustness. in this work, we study the phenomenon from the perspective of training instances. we show that catastrophic overfitting is instance-dependent, and fitting instances with larger input gradient norm is more likely to cause catastrophic overfitting. based on our findings, we propose a simple but effective method, adversarial training with adaptive step size (atas). atas learns an instance-wise adaptive step size that is inversely proportional to its gradient norm. our theoretical analysis shows that atas converges faster than the commonly adopted non-adaptive counterparts. empirically, atas consistently mitigates catastrophic overfitting and achieves higher robust accuracy on cifar10, cifar100, and imagenet when evaluated on various adversarial budgets. our code is released at https://github.com/huangzhichao95/atas.",AB_0378
"an essential need for accurate visual object tracking is to capture better correlations between the tracking target and the search region. however, the dominant siamese-based trackers are limited to producing dense similarity maps at once via a cross-correlations operation, ignoring to remedy the contamination caused by erroneous or ambiguous matches. in this paper, we propose a novel tracker, termed neighborhood consensus constraint-based siamese tracker (ncsiam), which takes the idea of neighborhood consensus constraint to refine the produced correlation maps. the intuition behind our approach is that we can support the nearby erroneous or ambiguous matches by analyzing a larger context of the scene that contains a unique match. specifically, we devise a 4d convolution-based multi-level similarity refinement (mlsr) strategy. taking the primary similarity maps obtained from a cross-correlation as input, mlsr acquires reliable matches by analyzing neighborhood consensus patterns in 4d space, thus enhancing the discriminability between the tracking target and the distractors. besides, traditional siamese-based trackers directly perform classification and regression on similarity response maps which discard appearance or semantic information. therefore, an appearance affinity decoder (aad) is developed to take full advantage of the semantic information of the search region. to further improve performance, we design a task-specific disentanglement (tsd) module to decouple the learned representations into classification-specific and regression-specific embeddings. extensive experiments are conducted on six challenging benchmarks, including got-10k, trackingnet, lasot, uav123, otb2015, and vot2020. the results demonstrate the effectiveness of our method. the code will be available at https://github.com/laybebe/ncsiam.",AB_0378
"remarkable achievements have been obtained with binary neural networks (bnn) in real-time and energy-efficient single-image super-resolution (sisr) methods. however, existing approaches often adopt the sign function to quantize image features while ignoring the influence of image spatial frequency. we argue that we can minimize the quantization error by considering different spatial frequency components. to achieve this, we propose a frequency-aware binarized network (fabnet) for single image super-resolution. first, we leverage the wavelet transformation to decompose the features into low-frequency and high-frequency components and then employ a divide-and-conquer strategy to separately process them with well-designed binary network structures. additionally, we introduce a dynamic binarization process that incorporates learned-threshold binarization during forward propagation and dynamic approximation during backward propagation, effectively addressing the diverse spatial frequency information. compared to existing methods, our approach is effective in reducing quantization error and recovering image textures. extensive experiments conducted on four benchmark datasets demonstrate that the proposed methods could surpass state-of-the-art approaches in terms of psnr and visual quality with significantly reduced computational costs. our codes are available at https://github.com/xrjiang527/fabnet-pytorch.",AB_0378
"it is challenging to generate temporal action proposals from untrimmed videos. in general, boundary-based temporal action proposal generators are based on detecting temporal action boundaries, where a classifier is usually applied to evaluate the probability of each temporal action location. however, most existing approaches treat boundaries and contents separately, which neglect that the context of actions and the temporal locations complement each other, resulting in incomplete modeling of boundaries and contents. in addition, temporal boundaries are often located by exploiting either local clues or global information, without mining local temporal information and temporal-to-temporal relations sufficiently at different levels. facing these challenges, a novel approach named multi-level content-aware boundary detection (mcbd) is proposed to generate temporal action proposals from videos, which jointly models the boundaries and contents of actions and captures multi-level (i.e., frame level and proposal level) temporal and context information. specifically, the proposed mcbd preliminarily mines rich frame-level features to generate one-dimensional probability sequences, and further exploits temporal-to-temporal proposal-level relations to produce two-dimensional probability maps. the final temporal action proposals are obtained by a fusion of the multi-level boundary and content probabilities, achieving precise boundaries and reliable confidence of proposals. the extensive experiments on the three benchmark datasets of thumos14, activitynet v1.3 and hacs demonstrate the effectiveness of the proposed mcbd compared to state-of-the-art methods. the source code of this work can be found in https://mic.tongji.edu.cn.",AB_0378
"out-of-distribution (ood) detection aims to detect unknown data whose labels have not been seen during the in-distribution (id) training process. recent progress in representation learning gives rise to distance-based ood detection that recognizes inputs as id/ood according to their relative distances to the training data of id classes. previous approaches calculate pairwise distances relying only on global image representations, which can be sub-optimal as the inevitable background clutter and intra-class variation may drive image-level representations from the same id class far apart in a given representation space. in this work, we overcome this challenge by proposing multi-scale ood detection (mode), a first framework leveraging both global visual information and local region details of images to maximally benefit ood detection. specifically, we first find that existing models pretrained by off-the-shelf cross-entropy or contrastive losses are incompetent to capture valuable local representations for mode, due to the scale-discrepancy between the id training and ood detection processes. to mitigate this issue and encourage locally discriminative representations in id training, we propose attention-based local propagation (alpa), a trainable objective that exploits a cross-attention mechanism to align and highlight the local regions of the target objects for pairwise examples. during test-time ood detection, a cross-scale decision (csd) function is further devised on the most discriminative multi-scale representations to distinguish id/ood data more faithfully. we demonstrate the effectiveness and flexibility of mode on several benchmarks - on average, mode outperforms the previous state-of-the-art by up to 19.24% in fpr, 2.77% in auroc. code is available at https://github.com/jimzai/mode-ood.",AB_0378
"the optical flow guidance strategy is ideal for obtaining motion information of objects in the video. it is widely utilized in video segmentation tasks. however, existing optical flow-based methods have a significant dependency on optical flow, which results in poor performance when the optical flow estimation fails for a particular scene. the temporal consistency provided by the optical flow could be effectively supplemented by modeling in a structural form. this paper proposes a new hierarchical graph neural network (gnn) architecture, dubbed hierarchical graph pattern understanding (hgpu), for zero-shot video object segmentation (zs-vos). inspired by the strong ability of gnns in capturing structural relations, hgpu innovatively leverages motion cues (i.e., optical flow) to enhance the high-order representations from the neighbors of target frames. specifically, a hierarchical graph pattern encoder with message aggregation is introduced to acquire different levels of motion and appearance features in a sequential manner. furthermore, a decoder is designed for hierarchically parsing and understanding the transformed multi-modal contexts to achieve more accurate and robust results. hgpu achieves state-of-the-art performance on four publicly available benchmarks (davis-16, youtube-objects, long-videos and davis-17). code and pre-trained model can be found at https://github.com/nust-machine-intelligence-laboratory/hgpu.",AB_0378
"point-based 3d detection approaches usually suffer from the severe point sampling imbalance problem between foreground and background. we observe that prior works have attempted to alleviate this imbalance by emphasizing foreground sampling. however, even adequate foreground sampling may be extremely unbalanced between nearby and distant objects, yielding unsatisfactory performance in detecting distant objects. to tackle this issue, this paper first proposes a novel method named distant object augmented set abstraction and regression (do-sa&r) to enhance distant object detection, which is vital for the timely response of decision-making systems like autonomous driving. technically, our approach first designs do-sa with novel distant object augmented farthest point sampling (do-fps) to emphasize sampling on distant objects by leveraging both object-dependent and depth-dependent information. then, we propose distant object augmented regression to reweight all the instance boxes for strengthening regression training on distant objects. in practice, the proposed do-sa&r can be easily embedded into the existing modules, yielding consistent performance improvements, especially on detecting distant objects. extensive experiments are conducted on the popular kitti, nuscenes and waymo datasets, and do-sa&r demonstrates superior performance, especially for distant object detection. our code is available at https://github.com/mikasa3lili/do-sar.",AB_0378
