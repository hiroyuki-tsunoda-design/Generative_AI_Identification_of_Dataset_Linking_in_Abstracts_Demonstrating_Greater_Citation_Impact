AB,NO
"we present a novel masked image modeling (mim) approach, context autoencoder (cae), for self-supervised representation pretraining. we pretrain an encoder by making predictions in the encoded representation space. the pretraining tasks include two tasks: masked representation prediction-predict the representations for the masked patches, and masked patch reconstruction-reconstruct the masked patches. the network is an encoder-regressor-decoder architecture: the encoder takes the visible patches as input; the regressor predicts the representations of the masked patches, which are expected to be aligned with the representations computed from the encoder, using the representations of visible patches and the positions of visible and masked patches; the decoder reconstructs the masked patches from the predicted encoded representations. the cae design encourages the separation of learning the encoder (representation) from completing the pertaining tasks: masked representation prediction and masked patch reconstruction tasks, and making predictions in the encoded representation space empirically shows the benefit to representation learning. we demonstrate the effectiveness of our cae through superior transfer performance in downstream tasks: semantic segmentation, object detection and instance segmentation, and classification. the code will be available at https://github.com/ atten4vis/cae.",AB_0339
"image super-resolution (sr) has been extensively investigated in recent years. however, due to the absence of trustworthy and precise perceptual quality standards, it is challenging to objectively measure the performance of different sr approaches. in this paper, we propose a novel triple attention dual-scale residual network called tadsrnet for no-reference super-resolution image quality assessment (nr-sriqa). firstly, we simulate the human visual system (hvs) and construct a triple attention mechanism to acquire more significant portions of sr images through cross-dimensionality, making it simpler to identify visually sensitive regions. then a dual-scale convolution module (dscm) is constructed to capture quality-perceived features at different scales. furthermore, in order to collect more informative feature representation, a residual connection is added to the network to compensate for perceptual features. extensive experimental results demonstrate that the proposed tadsrnet can predict visual quality with greater accuracy and better consistency with human perception compared with existing iqa methods. the code will be available at https://github.com/kbzhang0505/tadsrnet.",AB_0339
"in this work, we devote ourselves to the challenging task of unsupervised multi-view representation learning (umrl), which requires learning a unified feature representation from multiple views in an unsupervised manner. existing umrl methods mainly focus on the learning process within the feature space while ignoring the valuable semantic information hidden in different views. to address this issue, we propose a novel approach called semantically consistent multi-view representation learning (scmrl), which aims to excavate underlying multi-view semantic consensus information and utilize it to guide the unified feature representation learning process. specifically, scmrl consists of a within view reconstruction module and a unified feature representation learning module. these modules are elegantly integrated using a contrastive learning strategy, which serves to align the semantic labels of both view-specific feature representations and the learned unified feature representation simultaneously. this integration allows scmrl to effectively leverage consensus information in the semantic space, thereby constraining the learning process of the unified feature representation. compared with several state-of-the-art algorithms, extensive experiments demonstrate its superiority. our code is released on https://github.com/yiyangzhou/scmrl.& copy; 2023 elsevier b.v. all rights reserved.",AB_0339
"the remarkable progress of cross-domain fault diagnosis is based on the balanced distribution of different health conditions in a supervised manner. however, in engineering scenarios, the monitored fault data is scarce and imbalanced; variable working conditions and high labor costs make it luxurious to obtain labels; there is a huge gap between the current domain adaptation methods based on class balance data and real industrial applications. therefore, a dual-view style mixing network (dvsmn) for dealing with unsupervised cross-domain fault diagnosis with imbalanced data is proposed. two parallel graph convolution frameworks are first constructed to extract the fault features. then, the style mixing module together with the domain style loss is proposed for obtaining generalized and domain-invariant representations without augmenting any synthetic samples. an intermediate domain can also be initialized to increase the original cross-domain overlap to facilitate the domain adaptation. finally, a dual-view module that consists of a binary classifier and a multi-class classifier is constructed to realize sample-level dynamic re-weighting and accurate fault classification of imbalanced data. as such, the dvsmn can learn the generalized and domain-invariant features from the imbalanced data without any generative modules for sample re-balancing as well as target labels. cross-domain experiments with different imbalance ratios are carried out via two datasets to validate the performance of the proposed method. comparative studies with state-of-the-art methods and ablation experiments have demonstrated the effectiveness and superiority of the proposed method. the code of dvsmn is available at https://github.com/cqu-zixuchen/dvsmn.(c) 2023 elsevier b.v. all rights reserved.",AB_0339
"the automatic artery/vein (a/v) classification in retinal fundus images plays a significant role in detecting vascular abnormalities and could speed up the diagnosis of various systemic diseases. deep-learning methods have been extensively employed in this task. however, due to the lack of annotated data and the serious data imbalance, the performance of the existing methods is constricted. to address these limitations, we propose a novel multi-channel multi-scale fusion network (mmf-net) that employs the enhancement of vessel structural information to constrain the a/v classification. first, the newly designed multi-channel (mm) module could extract the vessel structure from the original fundus image by the frequency filters, increasing the proportion of blood vessel pixels and reducing the influence caused by the background pixels. second, the mmf-net introduces a multi-scale transformation (mt) module, which could efficiently extract the information from the multi-channel feature representations. third, the mmf-net utilizes a multi-feature fusion (mf) module to improve the robustness of a/v classification by splitting and reorganizing the pixel feature from different scales. we validate our results on several public benchmark datasets. the experimental results show that the proposed method could achieve the best result compared with the existing state-of-the-art methods, which demonstrate the superior performance of the mmf-net. the highly optimized python implementations of our method is released at: https://github.com/chenchouyu/mmf_net.",AB_0339
"map quality is of great importance to location-based-services(lbs) applications such as navigation and route planning. typically, a map can be extracted from either vehicle gps trajectories or aerial images. unfortunately, the quality of the extracted maps is usually unsatisfactory due to the inherent quality issues in the two data sources. compared with extracting maps from a single data source, cross-modal map extraction methods consider both data sources and often achieve better results. however, almost all existing cross-modal methods are based on cnn, which fail to sufficiently model global information. to overcome the above problem, we propose movinet, a novel cross-modal map extraction method that combines vit (vision transformer) and cnn. specifically, instead of partially integrating global information in the fusion scheme as in previous works, movinet introduces a lightweight vit model mobilevit as the encoder to enhance the model's ability to capture global information. meanwhile, we introduce a new lightweight but effective fusion scheme that generates modal-unified fusion features from the features of the two modalities, to enhance the information representation ability of the respective modalities. extensive experiments conducted on the beijing and porto datasets show the superior performance of our proposed method over all baselines. https://github.com/chan6688/movinet (c) 2023 elsevier b.v. all rights reserved.",AB_0339
"sclera segmentation is a key component of sclera recognition, which decides the region-of-interest (roi) for recognition and has a considerable impact on the overall performance. in recent years, a growing interest has been seen in deep learning-based sclera segmentation. despite their promising segmentation performance for diverse eye images, they are still inherently limited in generalizing to unseen target domains. in this paper, we aimto bridge this gap and learn a generalized sclera segmentation model that can handle new unseen domains well. to this end, we introduce meta-learning in the sclera segmentation problem and propose an effective learning framework, named metascleraseg. specifically, we first design a meta-sampling strategy to simulate the source/target domain shift in real-world scenarios. then, a style-invariant unet 3? base model is developed for accurate and robust sclera segmentation. to make the basemodel not only performwell on synthesized source domains but also on synthesized target domains, we employ the bilevel optimization strategy to update the base model, where three useful loss functions are contained. in the experiments, we build a cross-domain sclera segmentation (cdss) dataset with diverse ethnicity and quality as domain labels to supplement the existing dataset. besides, three protocols (cross-dataset, cross-ethnicity, and cross-quality) are designed for comprehensively evaluating the generalization of sclera segmentation models. both quantitative and qualitative experimental results validate the superiority of our method compared to several baselines, which indicates that metascleraseg can learn the transferable knowledge across domains to generalize well on unseen target domains. models and dataset of this paper are publicly available at https://github.com/lhqqq/metascleraseg.",AB_0339
"graph neural networks (gnns) aim to learn well-trained representations in a lower-dimension space for downstream tasks while preserving the topological structures. in recent years, attention mechanism, which is brilliant in the fields of natural language processing and computer vision, is introduced to gnns to adaptively select the discriminative features and automatically filter the noisy information. to the best of our knowledge, due to the fast-paced advances in this domain, a systematic overview of attention-based gnns is still missing. to fill this gap, this paper aims to provide a comprehensive survey on recent advances in attention-based gnns. firstly, we propose a novel two-level taxonomy for attention-based gnns from the perspective of development history and architectural perspectives. specifically, the upper level reveals the three developmental stages of attentionbased gnns, including graph recurrent attention networks, graph attention networks, and graph transformers. the lower level focuses on various typical architectures of each stage. secondly, we review these attention-based methods following the proposed taxonomy in detail and summarize the advantages and disadvantages of various models. a model characteristics table is also provided for a more comprehensive comparison. thirdly, we share our thoughts on some open issues and future directions of attention-based gnns. we hope this survey will provide researchers with an up-to-date reference regarding applications of attention-based gnns. in addition, to cope with the rapid development in this field, we intend to share the relevant latest papers as an open resource at https://github.com/sunxi aobei/awesome-attention-based-gnns.",AB_0339
"recently, deep unfolding networks (duns) have been applied to the fusion of low spatial resolution hyperspectral (lr hs) and high spatial resolution multispectral (hr ms) images and achieved satisfactory high spatial resolution hyperspectral (hr hs) images. however, the low-rank and sparse priors in these networks are not exploited sufficiently. in this paper, we establish an lr hs and hr ms image fusion model based on robust principal component analysis (rpca), which simultaneously captures the low-rank and sparse properties in hs images. then, the fusion model is optimized with the alternating direction method of multipliers (admm). to make full use of the representation capacity of duns, we unfold the derived admm algorithm as a network, named low-rank unfolding network (lru-net). specifically, each iteration in admm is unfolded as one stage in lru-net, in which the low-rank and sparse priors are learned by singular value thresholding (svt) and sparse module, respectively. finally, all features from all stages are integrated to produce the desired hr hs image. three benchmark datasets were chosen for comparison to demonstrate the effectiveness of the proposed lru-net. the experimental results demonstrate that lru-net performs better in terms of both qualitative and quantitative results compared to state-of-the-art fusion methods. the source code is publicly available at https://github.com/rsmagneto/lru-net.",AB_0339
"current methods for few-shot segmentation focus on extracting information from support and query targets, however, most of these methods not only suffer from high model complexity but also fail to capture long-range interactions between query features and support features. to efficiently reduce the information loss due to the appearance inconsistency in spatial between support and query objects and make full use of the limited features extracted from support and query samples, we present a lightweight multi-scale feature enrichment network named lite-fenet that enhances multi-scale feature interactions and ensures the discrimination with the final features under low computational cost. specifically, we design a lightweight and efficient spatial interaction module for the lite-fenet to achieve the contextual information transfer between different spatial scales over long distances. the repeated interactions generate a descriptive feature, and show superior robustness for pixel-level prediction, especially when there is a gap between different instances. experimental results show that the proposed method achieves 63.5% and 64.7% miou on the pascal-5i dataset, and 39.5% and 43.1% miou on the coco-20i datasets under 1-shot and 5-shot settings with only 6.7m learnable parameters, respectively, which significantly exceeds the results of current methods, while ensuring a far smaller model size. code is available at: https://github.com/sunbaoquan/lite-fenet.& copy; 2023 elsevier b.v. all rights reserved.",AB_0339
